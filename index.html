<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Quiz - AIMA Exam Prep</title>
    <style>
        :root {
            --bg-primary: #f0f4f8;
            --bg-secondary: #ffffff;
            --bg-tertiary: #e2e8f0;
            --text-primary: #1e293b;
            --text-secondary: #475569;
            --text-muted: #64748b;
            --accent: #6366f1;
            --accent-hover: #4f46e5;
            --accent-light: rgba(99, 102, 241, 0.1);
            --success: #10b981;
            --success-light: rgba(16, 185, 129, 0.15);
            --error: #ef4444;
            --error-light: rgba(239, 68, 68, 0.15);
            --warning: #f59e0b;
            --border: #e2e8f0;
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            --shadow-lg: 0 10px 25px -5px rgba(0, 0, 0, 0.1);
            --radius: 16px;
        }
        [data-theme="dark"] {
            --bg-primary: #0f172a;
            --bg-secondary: #1e293b;
            --bg-tertiary: #334155;
            --text-primary: #f8fafc;
            --text-secondary: #e2e8f0;
            --text-muted: #94a3b8;
            --border: #475569;
            --accent-light: rgba(99, 102, 241, 0.2);
            --success-light: rgba(16, 185, 129, 0.2);
            --error-light: rgba(239, 68, 68, 0.2);
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.4);
            --shadow-lg: 0 10px 25px -5px rgba(0, 0, 0, 0.5);
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.7;
            min-height: 100vh;
        }
        .container { max-width: 900px; margin: 0 auto; padding: 20px; }
        header { text-align: center; padding: 40px 20px; }
        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            background: linear-gradient(135deg, #6366f1, #a855f7, #ec4899);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 8px;
        }
        .subtitle { color: var(--text-secondary); font-size: 1.1rem; font-weight: 400; }
        .theme-toggle {
            position: fixed; top: 20px; right: 20px;
            background: var(--bg-secondary); border: 2px solid var(--border);
            border-radius: 50%; width: 50px; height: 50px;
            cursor: pointer; font-size: 1.4rem;
            box-shadow: var(--shadow); z-index: 1000;
            transition: all 0.3s ease;
            display: flex; align-items: center; justify-content: center;
        }
        .theme-toggle:hover { transform: scale(1.1); border-color: var(--accent); }
        .home-btn {
            position: fixed; top: 20px; left: 20px;
            background: var(--bg-secondary); border: 2px solid var(--border);
            border-radius: 50%; width: 50px; height: 50px;
            cursor: pointer; font-size: 1.4rem;
            box-shadow: var(--shadow); z-index: 1000;
            transition: all 0.3s ease;
            display: flex; align-items: center; justify-content: center;
        }
        .home-btn:hover { transform: scale(1.1); border-color: var(--accent); }
        .card {
            background: var(--bg-secondary);
            border-radius: var(--radius);
            padding: 28px;
            margin-bottom: 24px;
            box-shadow: var(--shadow);
            border: 1px solid var(--border);
            transition: all 0.3s ease;
        }
        .card:hover { box-shadow: var(--shadow-lg); }
        .card-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 18px;
            color: var(--text-primary);
            display: flex; align-items: center; gap: 10px;
        }
        .btn {
            background: linear-gradient(135deg, var(--accent), #8b5cf6);
            color: white; border: none;
            padding: 14px 28px; border-radius: 12px;
            cursor: pointer; font-size: 1rem; font-weight: 600;
            transition: all 0.3s ease;
            display: inline-flex; align-items: center; gap: 10px;
            text-shadow: 0 1px 2px rgba(0,0,0,0.2);
        }
        .btn:hover { transform: translateY(-2px); box-shadow: 0 8px 20px rgba(99, 102, 241, 0.4); }
        .btn:disabled { opacity: 0.5; cursor: not-allowed; transform: none; }
        .btn-secondary {
            background: var(--bg-tertiary); color: var(--text-primary);
            border: 2px solid var(--border); text-shadow: none;
        }
        .btn-secondary:hover { background: var(--border); box-shadow: var(--shadow); }
        .btn-success { background: linear-gradient(135deg, var(--success), #059669); }
        .checkbox-group {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(220px, 1fr));
            gap: 12px;
        }
        .checkbox-item {
            display: flex; align-items: center; gap: 12px;
            padding: 14px 16px;
            background: var(--bg-primary);
            border-radius: 12px; cursor: pointer;
            border: 2px solid transparent;
            transition: all 0.2s ease;
        }
        .checkbox-item:hover { border-color: var(--accent); background: var(--accent-light); }
        .checkbox-item input[type="checkbox"],
        .checkbox-item input[type="radio"] {
            width: 20px; height: 20px; accent-color: var(--accent); cursor: pointer;
        }
        .checkbox-item span { font-weight: 500; color: var(--text-primary); }
        .radio-group { display: flex; flex-wrap: wrap; gap: 12px; }
        .search-box {
            width: 100%; padding: 14px 18px;
            border: 2px solid var(--border); border-radius: 12px;
            font-size: 1rem;
            background: var(--bg-primary); color: var(--text-primary);
            transition: all 0.2s ease;
        }
        .search-box:focus { outline: none; border-color: var(--accent); box-shadow: 0 0 0 4px var(--accent-light); }
        .search-box::placeholder { color: var(--text-muted); }
        .quiz-section, .results-section { display: none; }
        .progress-container { margin-bottom: 24px; }
        .progress-bar {
            height: 10px; background: var(--bg-tertiary);
            border-radius: 5px; overflow: hidden;
        }
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, var(--accent), #a855f7, #ec4899);
            transition: width 0.4s ease;
            border-radius: 5px;
        }
        .progress-text {
            display: flex; justify-content: space-between;
            margin-top: 10px; font-size: 0.95rem; color: var(--text-secondary); font-weight: 500;
        }
        .timer { font-size: 1.2rem; font-weight: 700; color: var(--accent); }
        .question-card { animation: fadeIn 0.4s ease; }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(15px); } to { opacity: 1; transform: translateY(0); } }
        .question-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 20px; flex-wrap: wrap; gap: 10px; }
        .badge { padding: 6px 14px; border-radius: 20px; font-size: 0.85rem; font-weight: 600; }
        .chapter-badge { background: linear-gradient(135deg, var(--accent), #8b5cf6); color: white; }
        .difficulty-easy { background: var(--success-light); color: var(--success); }
        .difficulty-medium { background: rgba(245, 158, 11, 0.15); color: #d97706; }
        .difficulty-hard { background: var(--error-light); color: var(--error); }
        [data-theme="dark"] .difficulty-medium { color: #fbbf24; }
        .question-text { font-size: 1.25rem; font-weight: 500; margin-bottom: 24px; line-height: 1.8; color: var(--text-primary); }
        .options-list { display: flex; flex-direction: column; gap: 14px; }
        .option-btn {
            width: 100%; padding: 18px 22px; text-align: left;
            background: var(--bg-primary); border: 2px solid var(--border);
            border-radius: 14px; cursor: pointer; font-size: 1.05rem;
            transition: all 0.25s ease;
            display: flex; align-items: center; gap: 16px;
            color: var(--text-primary);
        }
        .option-btn:hover:not(:disabled) { border-color: var(--accent); background: var(--accent-light); transform: translateX(4px); }
        .option-btn:focus { outline: none; border-color: var(--accent); box-shadow: 0 0 0 4px var(--accent-light); }
        .option-btn.selected { border-color: var(--accent); background: var(--accent-light); }
        .option-btn.correct { border-color: var(--success); background: var(--success-light); }
        .option-btn.correct .option-letter { background: var(--success); color: white; }
        .option-btn.incorrect { border-color: var(--error); background: var(--error-light); }
        .option-btn.incorrect .option-letter { background: var(--error); color: white; }
        .option-letter {
            width: 36px; height: 36px;
            display: flex; align-items: center; justify-content: center;
            background: var(--bg-tertiary); border-radius: 10px;
            font-weight: 700; flex-shrink: 0;
            color: var(--text-primary);
            transition: all 0.25s ease;
        }
        .explanation-box {
            margin-top: 24px; padding: 24px;
            background: linear-gradient(135deg, var(--accent-light), rgba(139, 92, 246, 0.1));
            border-left: 5px solid var(--accent);
            border-radius: 0 16px 16px 0;
            display: none;
        }
        .explanation-box.show { display: block; animation: fadeIn 0.4s ease; }
        .explanation-title { font-weight: 700; font-size: 1.1rem; margin-bottom: 12px; color: var(--accent); display: flex; align-items: center; gap: 8px; }
        .explanation-text { color: var(--text-secondary); line-height: 1.8; font-size: 1.05rem; }
        .explanation-detail { margin-top: 16px; padding-top: 16px; border-top: 1px solid var(--border); }
        .explanation-detail strong { color: var(--text-primary); }
        .wrong-options { margin-top: 12px; }
        .wrong-option { padding: 8px 0; color: var(--text-muted); font-size: 0.95rem; display: flex; gap: 8px; }
        .wrong-option::before { content: "‚úó"; color: var(--error); font-weight: bold; }
        .quiz-nav { display: flex; justify-content: space-between; margin-top: 24px; gap: 12px; flex-wrap: wrap; }
        .score-display { text-align: center; padding: 40px 20px; }
        .score-circle {
            width: 180px; height: 180px; border-radius: 50%;
            background: conic-gradient(var(--accent) calc(var(--score-percent) * 3.6deg), var(--bg-tertiary) 0);
            display: flex; align-items: center; justify-content: center;
            margin: 0 auto 24px; box-shadow: var(--shadow-lg);
        }
        .score-inner {
            width: 140px; height: 140px; border-radius: 50%;
            background: var(--bg-secondary);
            display: flex; flex-direction: column;
            align-items: center; justify-content: center;
            box-shadow: inset 0 2px 10px rgba(0,0,0,0.1);
        }
        .score-value { font-size: 2.5rem; font-weight: 800; color: var(--accent); }
        .score-label { font-size: 1rem; color: var(--text-muted); font-weight: 500; }
        .stats-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin-top: 24px; }
        .stat-item { text-align: center; padding: 20px; background: var(--bg-primary); border-radius: 14px; }
        .stat-value { font-size: 2rem; font-weight: 800; }
        .stat-value.correct-stat { color: var(--success); }
        .stat-value.incorrect-stat { color: var(--error); }
        .stat-label { font-size: 0.9rem; color: var(--text-muted); margin-top: 4px; font-weight: 500; }
        .review-item {
            padding: 20px; background: var(--bg-primary);
            border-radius: 14px; margin-bottom: 16px;
            border-left: 5px solid var(--error);
        }
        .review-item.correct { border-left-color: var(--success); }
        .review-question { font-weight: 600; margin-bottom: 12px; font-size: 1.05rem; color: var(--text-primary); }
        .review-answer { font-size: 0.95rem; color: var(--text-secondary); line-height: 1.7; }
        .review-answer strong { color: var(--text-primary); }
        .notes-panel { display: none; margin-top: 20px; }
        .notes-chapter { margin-bottom: 24px; padding: 20px; background: var(--bg-primary); border-radius: 12px; }
        .notes-chapter h4 { color: var(--accent); margin-bottom: 12px; font-size: 1.1rem; }
        .notes-chapter li { margin-bottom: 8px; color: var(--text-secondary); line-height: 1.6; }
        footer { text-align: center; padding: 40px 20px; color: var(--text-muted); font-size: 0.95rem; }
        footer strong { color: var(--accent); }
        .modal { display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0,0,0,0.6); z-index: 2000; align-items: center; justify-content: center; }
        .modal.show { display: flex; }
        .modal-content { background: var(--bg-secondary); padding: 36px; border-radius: 20px; max-width: 450px; width: 90%; text-align: center; box-shadow: var(--shadow-lg); }
        .modal-content h3 { font-size: 1.4rem; margin-bottom: 16px; color: var(--text-primary); }
        .modal-content p { color: var(--text-secondary); margin-bottom: 24px; }
        @media (max-width: 600px) {
            h1 { font-size: 1.8rem; }
            .card { padding: 20px; }
            .checkbox-group { grid-template-columns: 1fr; }
            .stats-grid { grid-template-columns: 1fr; }
            .quiz-nav { flex-direction: column; }
            .quiz-nav .btn { width: 100%; justify-content: center; }
            .question-text { font-size: 1.1rem; }
        }
    </style>
</head>
<body>
    <button class="theme-toggle" onclick="toggleTheme()" aria-label="Toggle dark mode">üåô</button>
    <button class="home-btn" id="homeBtn" onclick="goHome()" aria-label="Return to homepage" style="display:none;">üè†</button>
    <div class="container">
        <header>
            <img src="logo.png" alt="AI Quiz Logo" style="width:120px;height:120px;margin-bottom:20px;border-radius:20px;box-shadow:var(--shadow-lg);">
            <h1>üéì AI Exam Prep Quiz</h1>
            <p class="subtitle">Artificial Intelligence: A Modern Approach ‚Äî Russell & Norvig</p>
        </header>
        <div id="setupSection" class="setup-section">
            <div class="card">
                <h3 class="card-title">üìö Select Chapters</h3>
                <div class="checkbox-group" id="chapterSelect">
                    <label class="checkbox-item"><input type="checkbox" value="1" checked><span>Ch 1: Introduction</span></label>
                    <label class="checkbox-item"><input type="checkbox" value="2" checked><span>Ch 2: Intelligent Agents</span></label>
                    <label class="checkbox-item"><input type="checkbox" value="3" checked><span>Ch 3: Problem Solving</span></label>
                    <label class="checkbox-item"><input type="checkbox" value="19" checked><span>Ch 19: Learning</span></label>
                    <label class="checkbox-item"><input type="checkbox" value="21" checked><span>Ch 21: Reinforcement Learning</span></label>
                    <label class="checkbox-item"><input type="checkbox" value="23" checked><span>Ch 23: NLP</span></label>
                    <label class="checkbox-item"><input type="checkbox" value="25" checked><span>Ch 25: Robotics</span></label>
                    <label class="checkbox-item"><input type="checkbox" value="27" checked><span>Ch 27: Ethics & Safety</span></label>
                    <label class="checkbox-item" style="border:2px solid var(--accent);background:var(--accent-light)"><input type="checkbox" value="100" checked><span>Quiz 1 Intro (Modules 1-4)</span></label>
                    <label class="checkbox-item" style="border:2px solid var(--accent);background:var(--accent-light)"><input type="checkbox" value="101" checked><span>Quiz 2: Machine Learning (Module 5)</span></label>
                    <label class="checkbox-item" style="border:2px solid var(--warning);background:rgba(245,158,11,0.15)"><input type="checkbox" value="102" checked><span>MIDTERM Exam Prep (All Modules)</span></label>
                    <label class="checkbox-item" style="border:2px solid var(--error);background:rgba(239,68,68,0.15)"><input type="checkbox" value="103" checked><span>FINAL EXAM Prep (ML & Logistic Reg)</span></label>
                </div>
            </div>
            <div class="card">
                <h3 class="card-title">‚ö° Difficulty</h3>
                <div class="checkbox-group">
                    <label class="checkbox-item"><input type="checkbox" name="difficulty" value="easy" checked><span>Easy</span></label>
                    <label class="checkbox-item"><input type="checkbox" name="difficulty" value="medium" checked><span>Medium</span></label>
                    <label class="checkbox-item"><input type="checkbox" name="difficulty" value="hard" checked><span>Hard</span></label>
                </div>
            </div>
            <div class="card">
                <h3 class="card-title">üéÆ Mode</h3>
                <div class="radio-group">
                    <label class="checkbox-item"><input type="radio" name="mode" value="practice" checked><span>Practice (Show answers)</span></label>
                    <label class="checkbox-item"><input type="radio" name="mode" value="exam"><span>Exam (Answers at end)</span></label>
                </div>
            </div>
            <div class="card">
                <h3 class="card-title">‚è±Ô∏è Timer</h3>
                <div class="radio-group">
                    <label class="checkbox-item"><input type="radio" name="timer" value="0" checked><span>No Timer</span></label>
                    <label class="checkbox-item"><input type="radio" name="timer" value="30"><span>30 min</span></label>
                    <label class="checkbox-item"><input type="radio" name="timer" value="60"><span>60 min</span></label>
                    <label class="checkbox-item"><input type="radio" name="timer" value="90"><span>90 min</span></label>
                </div>
            </div>
            <div class="card">
                <h3 class="card-title">üîç Search</h3>
                <input type="text" class="search-box" id="searchBox" placeholder="Search questions by keyword...">
                <div id="searchResults" style="margin-top:15px;"></div>
            </div>
            <div class="card">
                <h3 class="card-title">üìù Study Notes</h3>
                <button class="btn btn-secondary" onclick="toggleNotes()">Show Study Notes</button>
                <div id="notesPanel" class="notes-panel"><div id="notesContent"></div></div>
            </div>
            <div style="text-align:center;margin-top:10px;">
                <button class="btn btn-success" onclick="startQuiz()" style="padding:18px 50px;font-size:1.15rem;">üöÄ Start Quiz</button>
            </div>
        </div>
        <div id="quizSection" class="quiz-section">
            <div class="progress-container">
                <div class="progress-bar"><div class="progress-fill" id="progressFill"></div></div>
                <div class="progress-text"><span id="progressText">Question 1 of 10</span><span class="timer" id="timerDisplay"></span></div>
            </div>
            <div class="card question-card" id="questionCard">
                <div class="question-header">
                    <span class="badge chapter-badge" id="chapterBadge">Chapter 1</span>
                    <span class="badge" id="difficultyBadge">Medium</span>
                </div>
                <p class="question-text" id="questionText"></p>
                <div class="options-list" id="optionsList"></div>
                <div class="explanation-box" id="explanationBox"></div>
            </div>
            <div class="quiz-nav">
                <button class="btn btn-secondary" id="prevBtn" onclick="prevQuestion()">‚Üê Previous</button>
                <button class="btn" id="nextBtn" onclick="nextQuestion()">Next ‚Üí</button>
                <button class="btn btn-success" id="finishBtn" onclick="finishQuiz()" style="display:none;">‚úì Finish Quiz</button>
            </div>
        </div>
        <div id="resultsSection" class="results-section">
            <div class="card">
                <div class="score-display">
                    <div class="score-circle" id="scoreCircle" style="--score-percent:0;"><div class="score-inner"><span class="score-value" id="scoreValue">0%</span><span class="score-label">Score</span></div></div>
                    <h2 style="margin-bottom:8px;">Quiz Complete!</h2>
                    <p style="color:var(--text-muted);">Review your answers below</p>
                    <div class="stats-grid">
                        <div class="stat-item"><div class="stat-value correct-stat" id="correctCount">0</div><div class="stat-label">Correct</div></div>
                        <div class="stat-item"><div class="stat-value incorrect-stat" id="incorrectCount">0</div><div class="stat-label">Incorrect</div></div>
                        <div class="stat-item"><div class="stat-value" id="totalQuestions" style="color:var(--accent);">0</div><div class="stat-label">Total</div></div>
                    </div>
                </div>
            </div>
            <div class="card"><h3 class="card-title">üìã Review Answers</h3><div id="reviewList"></div></div>
            <div style="display:flex;gap:12px;flex-wrap:wrap;justify-content:center;">
                <button class="btn" onclick="downloadResults()">üì• Download Results</button>
                <button class="btn btn-secondary" onclick="restartQuiz()">üîÑ New Quiz</button>
            </div>
        </div>
        <footer><p>Designed with ‚ù§Ô∏è by <strong>Aboud Elzubair</strong></p><p style="margin-top:8px;font-size:0.85rem;">Based on "Artificial Intelligence: A Modern Approach" by Russell & Norvig</p></footer>
    </div>
    <div class="modal" id="confirmModal"><div class="modal-content"><h3>‚ö†Ô∏è Finish Quiz?</h3><p>You have unanswered questions. Finish anyway?</p><div style="display:flex;gap:12px;justify-content:center;"><button class="btn btn-secondary" onclick="closeModal()">Cancel</button><button class="btn" onclick="confirmFinish()">Finish</button></div></div></div>
<script>
const questions = [
// CHAPTER 1: INTRODUCTION (40 questions)
{chapter:1,difficulty:"easy",question:"What is Artificial Intelligence primarily concerned with?",options:["Building faster computers","Creating systems that act rationally","Replacing all human jobs","Making robots look human"],answerIndex:1,explanation:"AI focuses on creating systems that act rationally‚Äîmaking optimal decisions given available information. This is the 'rational agent' approach emphasized in AIMA.",wrongExplanations:["Speed is a hardware concern, not AI's primary goal","AI augments human capabilities rather than replacing all jobs","Humanoid appearance is robotics aesthetics, not AI's core concern"]},
{chapter:1,difficulty:"easy",question:"Who coined the term 'Artificial Intelligence'?",options:["Alan Turing","John McCarthy","Marvin Minsky","Claude Shannon"],answerIndex:1,explanation:"John McCarthy coined 'Artificial Intelligence' in 1956 at the Dartmouth Conference, which is considered the birth of AI as a field.",wrongExplanations:["Turing proposed the Turing Test but didn't coin the term AI","Minsky was a pioneer but McCarthy named the field","Shannon founded information theory, not AI terminology"]},
{chapter:1,difficulty:"easy",question:"The Turing Test evaluates a machine's ability to:",options:["Perform calculations faster than humans","Exhibit intelligent behavior indistinguishable from a human","Pass through physical obstacles","Store more data than the human brain"],answerIndex:1,explanation:"The Turing Test (1950) evaluates if a machine can exhibit intelligent behavior equivalent to or indistinguishable from a human through conversation.",wrongExplanations:["Calculation speed is not what the test measures","Physical navigation is unrelated to the Turing Test","Storage capacity doesn't indicate intelligence"]},
{chapter:1,difficulty:"medium",question:"Which approach to AI focuses on building systems that think like humans?",options:["Acting rationally","Thinking rationally","Cognitive modeling","Laws of thought approach"],answerIndex:2,explanation:"Cognitive modeling aims to build systems that think like humans by modeling actual human cognitive processes, validated against psychological experiments.",wrongExplanations:["Acting rationally focuses on optimal behavior, not human-like thinking","Thinking rationally uses logic, not necessarily human-like processes","Laws of thought is about logical reasoning, not cognitive simulation"]},
{chapter:1,difficulty:"medium",question:"The 'AI Winter' refers to:",options:["A period of rapid AI advancement","A period of reduced funding and interest in AI","The use of AI in cold climates","AI's inability to handle seasonal data"],answerIndex:1,explanation:"AI Winters were periods (notably 1974-1980 and 1987-1993) when funding and interest in AI declined significantly due to unmet expectations.",wrongExplanations:["AI Winter is the opposite‚Äîa slowdown, not advancement","This has nothing to do with physical climate","Seasonal data is unrelated to this historical term"]},
{chapter:1,difficulty:"medium",question:"What is the Chinese Room argument designed to challenge?",options:["Machine learning efficiency","Strong AI‚Äîthat machines can truly understand","Neural network architectures","Computational speed limits"],answerIndex:1,explanation:"John Searle's Chinese Room (1980) argues that symbol manipulation alone doesn't constitute genuine understanding, challenging Strong AI claims.",wrongExplanations:["The argument is philosophical, not about ML efficiency","It's not specifically about neural network design","Computational speed is irrelevant to this philosophical argument"]},
{chapter:1,difficulty:"hard",question:"Which statement best describes the difference between weak AI and strong AI?",options:["Weak AI is slower than strong AI","Weak AI simulates intelligence while strong AI possesses genuine understanding","Strong AI uses more computational resources","Weak AI only works on simple problems"],answerIndex:1,explanation:"Weak AI simulates intelligent behavior without genuine understanding (like chess programs). Strong AI would possess actual consciousness and understanding.",wrongExplanations:["The distinction is about understanding, not speed","Resource usage doesn't define the difference","Problem complexity doesn't define weak vs strong AI"]},
{chapter:1,difficulty:"easy",question:"The Dartmouth Conference of 1956 is significant because:",options:["The first computer was invented there","The term 'Artificial Intelligence' was coined there","Deep learning was discovered there","The internet was created there"],answerIndex:1,explanation:"The 1956 Dartmouth Summer Research Project, organized by McCarthy, is where AI became an official field and got its name.",wrongExplanations:["Computers existed before 1956 (ENIAC was 1945)","Deep learning emerged much later (2000s-2010s)","The internet was developed in the 1960s-70s"]},
{chapter:1,difficulty:"medium",question:"Which is NOT one of the four approaches to AI in AIMA?",options:["Acting humanly","Thinking humanly","Acting rationally","Computing efficiently"],answerIndex:3,explanation:"The four approaches are: (1) Thinking Humanly, (2) Acting Humanly, (3) Thinking Rationally, (4) Acting Rationally. Computing efficiently is not an approach.",wrongExplanations:["Acting humanly (Turing Test) is one of the four","Thinking humanly (cognitive modeling) is one of the four","Acting rationally (rational agent) is one of the four"]},
{chapter:1,difficulty:"hard",question:"The rationality of an agent depends on all EXCEPT:",options:["The performance measure defining success","Prior knowledge of the environment","The physical appearance of the agent","Actions available to the agent"],answerIndex:2,explanation:"Rationality depends on: performance measure, prior knowledge, possible percepts/actions, and percept sequence. Physical appearance is irrelevant.",wrongExplanations:["Performance measure is essential‚Äîit defines what 'success' means","Prior knowledge affects what actions are rational","Available actions constrain what the agent can do"]},
{chapter:1,difficulty:"easy",question:"ELIZA was an early AI program that:",options:["Played chess at grandmaster level","Simulated conversation using pattern matching","Solved complex mathematical theorems","Controlled industrial robots"],answerIndex:1,explanation:"ELIZA (1966, Joseph Weizenbaum) simulated a Rogerian psychotherapist using simple pattern matching, demonstrating how easily humans attribute intelligence.",wrongExplanations:["Deep Blue played chess, not ELIZA","Theorem proving was done by programs like Logic Theorist","ELIZA was purely conversational, not robotics"]},
{chapter:1,difficulty:"medium",question:"The Physical Symbol System Hypothesis states that:",options:["Physical symbols are unnecessary for AI","A physical symbol system has the necessary and sufficient means for intelligent action","Only biological systems can be intelligent","Symbols must be visible to be processed"],answerIndex:1,explanation:"Newell and Simon's hypothesis (1976) claims that a physical symbol system (like a computer) has necessary AND sufficient means for general intelligent action.",wrongExplanations:["The hypothesis says symbols ARE necessary","It claims non-biological systems CAN be intelligent","Visibility is irrelevant‚Äîinternal processing counts"]},
{chapter:1,difficulty:"hard",question:"Which philosopher argued that machines cannot think because they lack intentionality?",options:["Alan Turing","John Searle","Ren√© Descartes","Gottfried Leibniz"],answerIndex:1,explanation:"John Searle argued machines lack intentionality‚Äîthe ability to have mental states that are 'about' something‚Äîthrough his Chinese Room argument.",wrongExplanations:["Turing argued FOR the possibility of machine thinking","Descartes lived before computers existed","Leibniz envisioned calculating machines favorably"]},
{chapter:1,difficulty:"easy",question:"IBM's Deep Blue is famous for:",options:["Winning at Go against a world champion","Defeating world chess champion Garry Kasparov","Translating between all human languages","Driving a car autonomously"],answerIndex:1,explanation:"Deep Blue defeated Garry Kasparov in 1997, becoming the first computer to beat a reigning world chess champion in a match.",wrongExplanations:["AlphaGo (2016) defeated Go champions, not Deep Blue","Deep Blue was specialized for chess, not translation","Autonomous driving came much later"]},
{chapter:1,difficulty:"medium",question:"The Total Turing Test adds which capabilities beyond the original test?",options:["Mathematical proof verification","Computer vision and robotics","Database query optimization","Network communication protocols"],answerIndex:1,explanation:"The Total Turing Test extends the original by requiring computer vision (to perceive) and robotics (to manipulate objects), testing embodied intelligence.",wrongExplanations:["Math proofs aren't part of the Total Turing Test","Database operations aren't tested","Network protocols are infrastructure, not intelligence tests"]},
{chapter:1,difficulty:"easy",question:"What does GOFAI stand for?",options:["General Optimization For AI","Good Old-Fashioned Artificial Intelligence","Graphical Output For AI","Generic Objective Function AI"],answerIndex:1,explanation:"GOFAI refers to classical symbolic AI approaches using explicit rules and logic, as opposed to modern statistical/neural approaches.",wrongExplanations:["This is not a real acronym expansion","GOFAI refers to methodology, not graphics","Objective functions are from optimization, not this term"]},
{chapter:1,difficulty:"medium",question:"The knowledge representation hypothesis states that:",options:["Knowledge is unnecessary for intelligence","Intelligent behavior requires explicit knowledge structures","Only implicit knowledge matters for AI","Knowledge cannot be represented digitally"],answerIndex:1,explanation:"This hypothesis claims any intelligent system must have declarative, explicit symbolic representations of its knowledge to reason about the world.",wrongExplanations:["The hypothesis says knowledge IS necessary","It emphasizes explicit over implicit knowledge","Digital representation of knowledge is central to AI"]},
{chapter:1,difficulty:"hard",question:"Functionalism in philosophy of mind suggests that:",options:["Mental states are defined by their functional/causal roles","Only biological brains can have mental states","Functions have no relation to consciousness","Mental states are purely physical brain states"],answerIndex:0,explanation:"Functionalism holds that mental states are defined by their functional roles‚Äîinputs, outputs, and relations to other mental states‚Äîsupporting AI possibility.",wrongExplanations:["Functionalism allows non-biological minds","It connects function directly to mental states","It's distinct from pure physicalism/identity theory"]},
{chapter:1,difficulty:"easy",question:"Which of the following is a subfield of AI?",options:["Quantum mechanics","Machine learning","Organic chemistry","Civil engineering"],answerIndex:1,explanation:"Machine learning is a major AI subfield focused on systems that learn from data, including deep learning, reinforcement learning, etc.",wrongExplanations:["Quantum mechanics is physics, though quantum computing relates to AI","Organic chemistry is unrelated to AI","Civil engineering is a separate discipline"]},
{chapter:1,difficulty:"medium",question:"The frame problem in AI refers to:",options:["Issues with picture frames in computer vision","The challenge of representing what doesn't change when an action occurs","Problems with window frames in GUIs","Display resolution limitations"],answerIndex:1,explanation:"The frame problem is the challenge of efficiently representing and reasoning about what aspects of the world remain unchanged when actions occur.",wrongExplanations:["This is a logical/representational problem, not visual","GUI frames are unrelated","Display resolution is a hardware issue"]},
{chapter:1,difficulty:"easy",question:"What year was the Dartmouth Conference held?",options:["1943","1950","1956","1969"],answerIndex:2,explanation:"The Dartmouth Summer Research Project on Artificial Intelligence was held in 1956, marking the official birth of AI as a field.",wrongExplanations:["1943 was when McCulloch-Pitts neurons were proposed","1950 was when Turing published his famous paper","1969 was when Minsky & Papert published 'Perceptrons'"]},
{chapter:1,difficulty:"medium",question:"The 'acting humanly' approach to AI is exemplified by:",options:["Formal logic systems","The Turing Test","Bayesian networks","Expert systems"],answerIndex:1,explanation:"The Turing Test exemplifies 'acting humanly' because it evaluates whether a machine's behavior is indistinguishable from human behavior.",wrongExplanations:["Logic systems represent 'thinking rationally'","Bayesian networks are about rational reasoning under uncertainty","Expert systems emphasize knowledge, not human-like behavior"]},
{chapter:1,difficulty:"hard",question:"Which AI researcher proposed the 'Society of Mind' theory?",options:["John McCarthy","Marvin Minsky","Herbert Simon","Allen Newell"],answerIndex:1,explanation:"Marvin Minsky proposed that intelligence emerges from the interaction of many simple agents (a 'society' of mind) in his 1986 book.",wrongExplanations:["McCarthy focused on logic and Lisp","Simon worked on bounded rationality and problem solving","Newell co-developed production systems with Simon"]},
{chapter:1,difficulty:"easy",question:"The Logic Theorist program was designed to:",options:["Play chess","Prove mathematical theorems","Understand natural language","Recognize images"],answerIndex:1,explanation:"Logic Theorist (1956, Newell & Simon) was the first AI program, designed to prove theorems from Principia Mathematica using heuristic search.",wrongExplanations:["Chess programs came later","NLU was a different research direction","Image recognition developed much later"]},
{chapter:1,difficulty:"medium",question:"Bounded rationality, proposed by Herbert Simon, acknowledges that:",options:["Humans have unlimited cognitive resources","Real agents have computational limitations affecting their rationality","Rationality has no bounds","Only unbounded agents can be intelligent"],answerIndex:1,explanation:"Bounded rationality recognizes that real agents have limited time, memory, and computational resources, so they 'satisfice' rather than optimize.",wrongExplanations:["Simon argued humans have LIMITED resources","Bounded rationality sets practical bounds","Limited agents can still be intelligent within constraints"]},
{chapter:1,difficulty:"hard",question:"The Singularity hypothesis in AI predicts:",options:["AI will never surpass human intelligence","A point where AI improvement becomes uncontrollably rapid","AI development will gradually slow down","Machines will become singular entities"],answerIndex:1,explanation:"The Technological Singularity predicts a point where AI becomes capable of recursive self-improvement, leading to rapid, potentially uncontrollable advancement.",wrongExplanations:["The Singularity assumes AI WILL surpass humans","It predicts acceleration, not slowdown","'Singular' refers to the event, not individual machines"]},
{chapter:1,difficulty:"easy",question:"Which programming language was created specifically for AI research?",options:["FORTRAN","COBOL","LISP","Pascal"],answerIndex:2,explanation:"LISP (1958, John McCarthy) was created specifically for AI research, featuring symbolic computation and recursive functions ideal for AI applications.",wrongExplanations:["FORTRAN was for scientific computing","COBOL was for business applications","Pascal was for teaching structured programming"]},
{chapter:1,difficulty:"medium",question:"The Lighthill Report (1973) contributed to:",options:["Increased AI funding in the UK","The first AI winter by criticizing AI progress","The development of expert systems","The creation of the World Wide Web"],answerIndex:1,explanation:"The Lighthill Report criticized AI's lack of progress and led to severe funding cuts in the UK, contributing to the first AI Winter.",wrongExplanations:["It caused funding CUTS, not increases","Expert systems came as a response to the criticisms","The Web was invented much later (1989)"]},
{chapter:1,difficulty:"hard",question:"Connectionism differs from symbolic AI because it:",options:["Uses explicit symbol manipulation","Relies on distributed representations in neural networks","Requires formal logic rules","Cannot learn from data"],answerIndex:1,explanation:"Connectionism uses neural networks with distributed representations, where knowledge is encoded in connection weights rather than explicit symbols.",wrongExplanations:["Symbolic AI uses symbol manipulation, not connectionism","Connectionism avoids explicit logic rules","Neural networks excel at learning from data"]},
{chapter:1,difficulty:"easy",question:"Alan Turing's famous 1950 paper was titled:",options:["Artificial Intelligence: A Modern Approach","Computing Machinery and Intelligence","The Organization of Behavior","Perceptrons"],answerIndex:1,explanation:"Turing's 1950 paper 'Computing Machinery and Intelligence' introduced the Turing Test and addressed 'Can machines think?'",wrongExplanations:["AIMA is Russell & Norvig's textbook (1995)","Hebb wrote 'Organization of Behavior' (1949)","Minsky & Papert wrote 'Perceptrons' (1969)"]},
{chapter:1,difficulty:"medium",question:"Expert systems in the 1980s primarily used:",options:["Deep neural networks","Knowledge bases with if-then rules","Reinforcement learning","Genetic algorithms"],answerIndex:1,explanation:"Expert systems like MYCIN and DENDRAL used knowledge bases of if-then rules elicited from human experts to solve domain-specific problems.",wrongExplanations:["Deep learning emerged much later (2010s)","RL was not the primary approach for expert systems","Genetic algorithms were a separate AI approach"]},
{chapter:1,difficulty:"hard",question:"The symbol grounding problem asks:",options:["How to store symbols efficiently","How symbols in AI systems acquire meaning","How to compress symbolic data","How symbols are transmitted over networks"],answerIndex:1,explanation:"The symbol grounding problem (Harnad, 1990) asks how symbols in an AI system become connected to their real-world referents‚Äîhow they get meaning.",wrongExplanations:["Storage efficiency is an engineering concern","Compression is unrelated to grounding","Transmission doesn't address meaning"]},
{chapter:1,difficulty:"easy",question:"Which company developed Watson, the Jeopardy-winning AI?",options:["Google","Microsoft","IBM","Apple"],answerIndex:2,explanation:"IBM's Watson won Jeopardy! in 2011 against human champions, demonstrating natural language understanding and question answering capabilities.",wrongExplanations:["Google developed different AI systems","Microsoft wasn't behind Watson","Apple focuses on consumer products"]},
{chapter:1,difficulty:"medium",question:"The Loebner Prize is awarded for:",options:["Best robotics design","Most human-like chatbot (Turing Test)","Fastest algorithm","Best computer vision system"],answerIndex:1,explanation:"The Loebner Prize (since 1991) is an annual Turing Test competition awarding the most human-like conversational AI.",wrongExplanations:["Robotics competitions are separate","Speed isn't the criterion","Computer vision has different benchmarks"]},
{chapter:1,difficulty:"hard",question:"Hubert Dreyfus criticized AI primarily on the grounds that:",options:["Computers are too slow","Human intelligence relies on embodied, intuitive know-how that can't be formalized","AI is too expensive","Computers lack sufficient memory"],answerIndex:1,explanation:"Dreyfus argued in 'What Computers Can't Do' (1972) that human expertise relies on intuitive, embodied knowledge that resists formalization.",wrongExplanations:["His critique was philosophical, not about speed","Cost wasn't his concern","Memory limitations weren't his argument"]},
{chapter:1,difficulty:"easy",question:"AlphaGo, developed by DeepMind, defeated a world champion in:",options:["Chess","Checkers","Go","Poker"],answerIndex:2,explanation:"AlphaGo defeated Go world champion Lee Sedol in 2016, a landmark achievement given Go's enormous search space.",wrongExplanations:["Deep Blue conquered chess in 1997","Checkers was solved earlier","Poker AI (Libratus) came later"]},
{chapter:1,difficulty:"medium",question:"The 'thinking rationally' approach to AI is based on:",options:["Psychology experiments","Formal logic and laws of thought","Biological neural networks","Economic utility theory"],answerIndex:1,explanation:"The 'thinking rationally' approach uses formal logic‚Äîsyllogisms, propositional logic, first-order logic‚Äîto derive correct conclusions.",wrongExplanations:["Psychology underlies 'thinking humanly'","Neural networks are computational tools","Utility theory underlies 'acting rationally'"]},
{chapter:1,difficulty:"hard",question:"The qualification problem in AI concerns:",options:["How to qualify AI researchers","The difficulty of listing all preconditions for an action to succeed","Qualifying statements in natural language","Quality control in AI systems"],answerIndex:1,explanation:"The qualification problem is the difficulty of explicitly stating ALL preconditions that must hold for an action to achieve its intended effect.",wrongExplanations:["It's not about researcher qualifications","It's about action preconditions, not language","Quality control is an engineering concern"]},

// CHAPTER 2: INTELLIGENT AGENTS (40 questions)
{chapter:2,difficulty:"easy",question:"An agent is defined as anything that:",options:["Has a physical body","Perceives its environment through sensors and acts through actuators","Only performs mathematical calculations","Requires constant human supervision"],answerIndex:1,explanation:"An agent perceives its environment through sensors and acts upon that environment through actuators. This is the fundamental agent definition in AIMA.",wrongExplanations:["Agents can be software without physical bodies","Agents can do much more than just calculations","Many agents operate autonomously without supervision"]},
{chapter:2,difficulty:"easy",question:"What is a percept?",options:["A type of actuator","The agent's perceptual input at any given instant","A performance measure","An action sequence"],answerIndex:1,explanation:"A percept is the content that an agent's sensors perceive at a single instant. The percept sequence is the complete history of everything perceived.",wrongExplanations:["Actuators produce actions, they don't perceive","Performance measures evaluate agent behavior","Action sequences are outputs, not inputs"]},
{chapter:2,difficulty:"medium",question:"A rational agent is one that:",options:["Always achieves perfect outcomes","Selects actions expected to maximize its performance measure","Never makes any mistakes","Has unlimited computational resources"],answerIndex:1,explanation:"A rational agent selects actions expected to maximize its performance measure, based on the percept sequence and built-in knowledge. It may still fail.",wrongExplanations:["Rationality doesn't guarantee success, only good choices","Rational agents can make mistakes in uncertain environments","Rationality is about decision quality, not resources"]},
{chapter:2,difficulty:"easy",question:"PEAS stands for:",options:["Performance, Environment, Actuators, Sensors","Perception, Execution, Action, State","Plan, Execute, Adapt, Succeed","Process, Evaluate, Act, Store"],answerIndex:0,explanation:"PEAS specifies a task environment: Performance measure, Environment, Actuators, and Sensors. It's the framework for describing any agent's setting.",wrongExplanations:["This is not the correct expansion","PEAS is about task specification, not processes","These terms don't match the PEAS framework"]},
{chapter:2,difficulty:"medium",question:"A fully observable environment means:",options:["The agent can see everything physically","Sensors detect all aspects of the environment relevant to choosing an action","Nothing in the environment ever changes","The environment is completely predictable"],answerIndex:1,explanation:"Full observability means the agent's sensors give access to the complete state relevant to action selection at each point in time.",wrongExplanations:["It's about relevant information, not literal visibility","Observability is about sensing, not dynamics","Predictability is a separate property (determinism)"]},
{chapter:2,difficulty:"medium",question:"An episodic environment is one where:",options:["The agent's experience is divided into independent atomic episodes","TV episodes are shown to the agent","Time doesn't exist in the environment","Actions have permanent long-term effects"],answerIndex:0,explanation:"In episodic environments, each episode's outcome depends only on actions in that episode‚Äîno long-term consequences carry over.",wrongExplanations:["This is unrelated to television","Time still exists, episodes are time-bounded","Sequential environments have long-term effects, not episodic ones"]},
{chapter:2,difficulty:"hard",question:"A stochastic environment differs from a deterministic one because:",options:["It uses more sophisticated statistics","The next state is not completely determined by current state and action","It processes data in random order","It requires more computing power"],answerIndex:1,explanation:"In stochastic environments, the same action in the same state may lead to different outcomes due to inherent randomness or uncertainty.",wrongExplanations:["Stochasticity is about outcome uncertainty, not statistics usage","Processing order is unrelated","Computational needs vary independently of stochasticity"]},
{chapter:2,difficulty:"easy",question:"Which is an example of a simple reflex agent?",options:["A chess-playing AI","A thermostat","A self-driving car","A medical diagnosis system"],answerIndex:1,explanation:"A thermostat is a simple reflex agent: it perceives temperature and acts (heat on/off) based purely on the current percept using simple rules.",wrongExplanations:["Chess requires lookahead and planning","Self-driving cars need complex state and planning","Medical diagnosis requires extensive knowledge and reasoning"]},
{chapter:2,difficulty:"medium",question:"A model-based reflex agent differs from a simple reflex agent by:",options:["Being more expensive to build","Maintaining internal state about unobservable aspects of the world","Using better quality sensors","Processing information faster"],answerIndex:1,explanation:"Model-based agents maintain internal state to track aspects of the world not directly observable, using a model of how the world evolves.",wrongExplanations:["Cost isn't the defining difference","Sensor quality is independent of agent architecture","Processing speed isn't the distinguishing feature"]},
{chapter:2,difficulty:"hard",question:"Goal-based agents are advantageous over reflex agents when:",options:["The environment is completely static","Flexible behavior is needed to achieve different objectives","Speed is the only concern","The action space is very small"],answerIndex:1,explanation:"Goal-based agents can consider future outcomes and adapt behavior to achieve goals, unlike reflex agents that just react to current percepts.",wrongExplanations:["Reflex agents work fine in static environments","Speed often favors simpler reflex agents","Small action spaces don't require goal reasoning"]},
{chapter:2,difficulty:"easy",question:"A utility function in agent design:",options:["Measures code usefulness","Maps states to a numerical measure of desirability","Counts utility bills","Determines computational efficiency"],answerIndex:1,explanation:"A utility function assigns real numbers to states representing how desirable/happy that state is for the agent, enabling tradeoff reasoning.",wrongExplanations:["It's about state desirability, not code quality","Unrelated to financial utilities","Efficiency is measured differently"]},
{chapter:2,difficulty:"medium",question:"A multi-agent environment is characterized by:",options:["An agent with multiple goals simultaneously","Multiple agents whose actions affect each other","An agent using multiple sensors","Multiple instances of the same agent program"],answerIndex:1,explanation:"Multi-agent environments contain multiple agents whose actions can affect each other's performance, potentially cooperatively or competitively.",wrongExplanations:["Multiple goals is a single-agent property","Multiple sensors is about the agent, not environment","Multiple instances might still not interact"]},
{chapter:2,difficulty:"easy",question:"The agent function:",options:["Maps percept sequences to actions","Calculates mathematical functions only","Measures agent size","Determines sensor quality"],answerIndex:0,explanation:"The agent function is an abstract mathematical description that maps every possible percept sequence to an action the agent should take.",wrongExplanations:["It's broader than just math calculations","Agent size is irrelevant","Sensor quality is separate from the function"]},
{chapter:2,difficulty:"medium",question:"In a learning agent, which component suggests exploratory actions?",options:["The critic","The problem generator","The performance element","The learning element"],answerIndex:1,explanation:"The problem generator suggests exploratory actions that may lead to new, informative experiences, even if suboptimal short-term.",wrongExplanations:["The critic evaluates performance","The performance element selects actions for the task","The learning element modifies the agent based on feedback"]},
{chapter:2,difficulty:"hard",question:"In a partially observable environment, an agent must:",options:["Give up trying to act rationally","Maintain beliefs about the current state based on percept history","Only use simple reflex actions","Avoid taking any actions under uncertainty"],answerIndex:1,explanation:"Under partial observability, agents maintain a belief state‚Äîa probability distribution over possible states‚Äîupdated using percepts and a model.",wrongExplanations:["Rational action is still possible with beliefs","Complex state tracking is often needed","Actions under uncertainty are often necessary"]},
{chapter:2,difficulty:"easy",question:"The vacuum cleaner world with two rooms is an example of:",options:["A complex multi-agent system","A simple task environment for illustrating agent concepts","A natural language processing domain","A computer vision application"],answerIndex:1,explanation:"The 2-room vacuum world is a pedagogical example used to illustrate basic agent concepts like states, actions, and agent architectures.",wrongExplanations:["It's deliberately simple, not complex","It's not about language","It's not about vision"]},
{chapter:2,difficulty:"medium",question:"A competitive multi-agent environment is also called:",options:["Cooperative","Adversarial","Episodic","Static"],answerIndex:1,explanation:"Adversarial environments have agents with opposing goals‚Äîone agent's gain is another's loss (e.g., games, competitive markets).",wrongExplanations:["Cooperative is the opposite‚Äîagents help each other","Episodic describes time structure, not competition","Static describes change rate, not agent relationships"]},
{chapter:2,difficulty:"hard",question:"The performance element in a learning agent:",options:["Evaluates how well the agent is performing","Selects external actions based on percepts","Modifies knowledge based on feedback","Generates new learning problems"],answerIndex:1,explanation:"The performance element is what we previously called 'the agent'‚Äîit takes percepts and decides on actions using current knowledge.",wrongExplanations:["The critic evaluates performance","The learning element modifies knowledge","The problem generator creates learning opportunities"]},
{chapter:2,difficulty:"easy",question:"A discrete environment has:",options:["Continuous states that flow smoothly","A finite number of distinct states, percepts, and actions","No states at all","Infinite possible configurations"],answerIndex:1,explanation:"Discrete environments have countably many distinct states and actions (like chess), unlike continuous ones (like driving with real-valued controls).",wrongExplanations:["Continuous is the opposite of discrete","Environments always have states","Discrete means finite/countable, not infinite"]},
{chapter:2,difficulty:"medium",question:"The critic component in a learning agent:",options:["Criticizes the agent's programming style","Provides feedback on how well the agent is doing relative to the performance standard","Only finds faults in the agent","Selects actions for the agent"],answerIndex:1,explanation:"The critic observes the world and tells the learning element how well the agent is doing, using a fixed performance standard for evaluation.",wrongExplanations:["It evaluates behavior, not code style","It provides feedback, not just negative criticism","Action selection is done by the performance element"]},
{chapter:2,difficulty:"easy",question:"Sensors in an agent are used to:",options:["Move the agent through the environment","Perceive information from the environment","Store the agent's memory","Compute the agent's decisions"],answerIndex:1,explanation:"Sensors are the agent's means of perceiving its environment‚Äîcameras, microphones, keyboards, or any input device providing percepts.",wrongExplanations:["Movement is done by actuators","Memory is internal storage, not sensors","Decision computation is done by the agent program"]},
{chapter:2,difficulty:"medium",question:"A static environment:",options:["Never changes at all","Does not change while the agent is deliberating","Is always fully observable","Contains no other agents"],answerIndex:1,explanation:"A static environment doesn't change while the agent deliberates about its action. Dynamic environments may change during deliberation.",wrongExplanations:["It may change between decisions, just not during","Static is about change rate, not observability","It may contain other passive elements"]},
{chapter:2,difficulty:"hard",question:"The percept sequence for an agent is:",options:["The most recent percept only","The complete history of everything the agent has perceived","The predicted future percepts","The filtered important percepts only"],answerIndex:1,explanation:"The percept sequence is the complete history of all percepts the agent has received from the start. The agent function maps this to actions.",wrongExplanations:["That's just the current percept","Future percepts aren't known yet","All percepts matter, not just filtered ones"]},
{chapter:2,difficulty:"easy",question:"Actuators in an agent are used to:",options:["Sense the environment","Perform actions that affect the environment","Store percept history","Calculate utility values"],answerIndex:1,explanation:"Actuators are the agent's means of acting on its environment‚Äîwheels, robot arms, display screens, or any output mechanism.",wrongExplanations:["Sensing is done by sensors","Storage is internal memory","Calculation is done by the agent program"]},
{chapter:2,difficulty:"medium",question:"A semi-dynamic environment:",options:["Changes at half the normal speed","The environment doesn't change but the agent's performance score does","Only partially changes","Is both static and dynamic simultaneously"],answerIndex:1,explanation:"In semi-dynamic environments, the environment itself doesn't change during deliberation, but the agent's performance score may (e.g., time penalties).",wrongExplanations:["It's not about speed","This describes semi-dynamic correctly","It's specifically about score changing while state doesn't"]},
{chapter:2,difficulty:"easy",question:"An agent's performance measure:",options:["Measures the agent's physical dimensions","Evaluates the desirability of environment states","Counts the number of actions taken","Measures computational speed"],answerIndex:1,explanation:"The performance measure is an objective criterion for evaluating the success of agent behavior‚Äîit's how we judge if the agent is doing well.",wrongExplanations:["Physical size is irrelevant","Action count alone doesn't measure success","Speed is just one possible factor"]},
{chapter:2,difficulty:"hard",question:"Table-driven agents are impractical for complex environments because:",options:["Tables are outdated technology","The lookup table would be astronomically large","Tables are too slow to access","Tables can only store numbers"],answerIndex:1,explanation:"For any reasonably complex environment, the table mapping all percept sequences to actions would be impossibly large (exponential in sequence length).",wrongExplanations:["Tables are fundamental data structures, still used","Lookup is actually fast‚Äîsize is the issue","Tables can store any data type"]},
{chapter:2,difficulty:"medium",question:"A known environment means the agent:",options:["Has a map of the physical layout","Has complete knowledge of the 'physics' governing the environment","Can see everything in the environment","Has been there before"],answerIndex:1,explanation:"A known environment means the agent understands the rules‚Äîtransition probabilities, effects of actions‚Äîeven if current state is unknown.",wrongExplanations:["Physical layout is different from environment laws","That's observability, not knowledge","Prior experience isn't the same as knowing the rules"]},
{chapter:2,difficulty:"easy",question:"Which agent type is most suitable for a fully observable, deterministic environment?",options:["Learning agent with complex neural network","Simple reflex agent with condition-action rules","Utility-based agent with lookahead","Goal-based planning agent"],answerIndex:1,explanation:"Simple reflex agents work well in fully observable, deterministic settings because the correct action depends only on the current percept.",wrongExplanations:["Complex learning is unnecessary here","Utility and lookahead aren't needed","Planning is overkill for this simple case"]},
{chapter:2,difficulty:"medium",question:"A single-agent environment:",options:["Contains exactly one entity of any kind","Has only one agent affecting the environment state","Is always fully observable","Cannot change over time"],answerIndex:1,explanation:"Single-agent environments have only one agent whose actions affect the state. Other entities may exist but aren't modeled as agents.",wrongExplanations:["Other entities may exist, just not as agents","Observability is a separate property","Dynamic single-agent environments exist"]},
{chapter:2,difficulty:"hard",question:"The difference between agent function and agent program is:",options:["There is no difference‚Äîthey are synonyms","The function is an abstract specification; the program is a concrete implementation","The program is theoretical; the function is practical","Functions are for math, programs for computers"],answerIndex:1,explanation:"The agent function is the abstract mathematical mapping (infinite table); the program is the concrete implementation running on an architecture.",wrongExplanations:["They're related but different concepts","It's the opposite‚Äîfunction is abstract","Both can be mathematical and computational"]},
{chapter:2,difficulty:"easy",question:"In the reflex vacuum agent example, what is the environment?",options:["The agent's code","The two-room grid with possible dirt locations","The user's expectations","The agent's memory"],answerIndex:1,explanation:"The environment is the external world the agent interacts with‚Äîthe rooms, their cleanliness states, and how they change.",wrongExplanations:["Code is the agent program, not environment","Expectations aren't part of the physical environment","Memory is internal to the agent"]},
{chapter:2,difficulty:"medium",question:"A continuous action space means:",options:["Actions happen continuously without stopping","The set of possible actions is infinite/real-valued","Actions are always connected to each other","The agent can't stop acting"],answerIndex:1,explanation:"Continuous action spaces have infinitely many possible actions (like steering angle 0¬∞-360¬∞), unlike discrete spaces with finite choices.",wrongExplanations:["Continuous refers to the space, not timing","Connectedness isn't the definition","Agents can pause even with continuous actions"]},
{chapter:2,difficulty:"hard",question:"An online learning agent differs from offline learning by:",options:["Being connected to the internet","Learning while interacting with the real environment","Only learning during office hours","Learning from online courses"],answerIndex:1,explanation:"Online learning agents learn incrementally during actual task performance, updating with each new experience rather than training on a fixed dataset.",wrongExplanations:["Online/offline here refers to timing, not internet","Office hours are irrelevant","It's not about educational platforms"]},
{chapter:2,difficulty:"easy",question:"What is the agent's 'architecture' in AIMA?",options:["The building where the agent is located","The computing platform (hardware/software) on which the agent runs","The agent's belief structure","The agent's goal hierarchy"],answerIndex:1,explanation:"The architecture is the computing device with sensors and actuators on which the agent program runs‚Äîthe physical/virtual platform.",wrongExplanations:["It's about computing platforms, not buildings","Beliefs are part of internal state, not architecture","Goals are part of the agent program"]},
{chapter:2,difficulty:"medium",question:"Omniscience differs from rationality because:",options:["Omniscience is a type of rationality","A rational agent acts on available information, while omniscience requires knowing actual outcomes","Rationality is more powerful than omniscience","They are identical concepts"],answerIndex:1,explanation:"Omniscience means knowing actual outcomes (impossible). Rationality means making best decisions given available information at decision time.",wrongExplanations:["They're distinct concepts, not subsets","Omniscience would be 'better' but is impossible","They are distinctly different"]},
{chapter:2,difficulty:"hard",question:"An autonomous agent is one that:",options:["Runs without electricity","Operates successfully without relying entirely on built-in knowledge","Never interacts with humans","Has no programmed instructions at all"],answerIndex:1,explanation:"Autonomous agents can adapt and learn from experience rather than relying entirely on the designer's prior knowledge about the environment.",wrongExplanations:["Autonomy is about independence, not power source","Autonomous agents may interact with humans","They have initial programming but can adapt beyond it"]},

// CHAPTER 3: PROBLEM SOLVING BY SEARCHING (50 questions)
{chapter:3,difficulty:"easy",question:"A problem in search is formally defined by:",options:["Its code length and complexity","Initial state, actions, transition model, goal test, and path cost","Processing speed requirements","The number of variables involved"],answerIndex:1,explanation:"A search problem is defined by: initial state, actions available, transition model (successor function), goal test, and path cost function.",wrongExplanations:["Code length is an implementation detail","Speed is a performance metric, not problem definition","Variables are part of state representation"]},
{chapter:3,difficulty:"easy",question:"Breadth-first search explores nodes:",options:["Deepest nodes first","In random order","Level by level, shallowest first","Based on heuristic estimates"],answerIndex:2,explanation:"BFS uses a FIFO queue, expanding all nodes at depth d before any at depth d+1, guaranteeing the shallowest solution is found first.",wrongExplanations:["That's depth-first search","Random would be random search","Heuristics are used in informed search"]},
{chapter:3,difficulty:"medium",question:"Depth-first search may not be complete because:",options:["It's too computationally slow","It can get stuck in infinite paths without finding existing solutions","It uses too much memory","It doesn't explore deep enough"],answerIndex:1,explanation:"DFS can follow an infinite branch forever, never backtracking to find a solution that exists at shallower depths in another branch.",wrongExplanations:["DFS is actually fast per node","DFS uses minimal memory (linear)","DFS explores very deep, possibly infinitely"]},
{chapter:3,difficulty:"easy",question:"The time complexity of BFS is:",options:["O(d)","O(b^d)","O(log n)","O(1)"],answerIndex:1,explanation:"BFS time is O(b^d) where b is branching factor and d is depth of shallowest solution‚Äîit may generate all nodes to that depth.",wrongExplanations:["O(d) ignores branching factor","O(log n) is for divide-and-conquer","O(1) is constant time, not realistic for search"]},
{chapter:3,difficulty:"medium",question:"A* search uses which evaluation function?",options:["f(n) = g(n)","f(n) = h(n)","f(n) = g(n) + h(n)","f(n) = g(n) √ó h(n)"],answerIndex:2,explanation:"A* uses f(n) = g(n) + h(n): actual path cost so far (g) plus estimated cost to goal (h), combining both for optimal informed search.",wrongExplanations:["g(n) alone is uniform cost search","h(n) alone is greedy best-first","Multiplication isn't the A* formula"]},
{chapter:3,difficulty:"hard",question:"An admissible heuristic:",options:["Always overestimates the cost to reach the goal","Never overestimates the actual cost to reach the goal","Exactly equals the true cost","Ignores path cost completely"],answerIndex:1,explanation:"Admissibility means h(n) ‚â§ h*(n) for all n, where h*(n) is the true cost. This guarantees A* finds optimal solutions.",wrongExplanations:["Overestimating breaks optimality","Exact equality is a stronger condition (perfect heuristic)","Path cost is essential to the definition"]},
{chapter:3,difficulty:"easy",question:"Uniform cost search expands nodes based on:",options:["Depth in the search tree","Path cost g(n) from start","Heuristic estimate h(n)","Alphabetical order of states"],answerIndex:1,explanation:"UCS uses a priority queue ordered by path cost g(n), always expanding the node with lowest total path cost first.",wrongExplanations:["Depth-based is BFS/DFS","Heuristic-based is greedy best-first","Alphabetical ordering isn't a standard strategy"]},
{chapter:3,difficulty:"medium",question:"Which search strategy uses a LIFO (stack) data structure?",options:["Breadth-first search","Depth-first search","Uniform cost search","A* search"],answerIndex:1,explanation:"DFS uses a stack (LIFO), always expanding the most recently generated node, going deep before wide.",wrongExplanations:["BFS uses a FIFO queue","UCS uses a priority queue by cost","A* uses a priority queue by f(n)"]},
{chapter:3,difficulty:"hard",question:"Iterative deepening combines the advantages of:",options:["Neural networks and symbolic AI","DFS's space efficiency and BFS's completeness/optimality","Random search and systematic search","Forward and backward search"],answerIndex:1,explanation:"ID-DFS does repeated depth-limited searches with increasing limits, achieving O(bd) space like DFS and completeness/optimality like BFS.",wrongExplanations:["It's not about AI paradigms","It's systematic, not random","It's not bidirectional"]},
{chapter:3,difficulty:"easy",question:"A consistent (monotonic) heuristic satisfies:",options:["h(n) ‚â§ c(n,a,n') + h(n') for every successor n'","h(n) > h(n') always","h(n) = 0 everywhere","h(n) = g(n) always"],answerIndex:0,explanation:"Consistency is a triangle inequality: the heuristic estimate from n is ‚â§ step cost to n' plus estimate from n'. This implies admissibility.",wrongExplanations:["h can increase or decrease depending on the path","h=0 is trivially consistent but uninformative","h and g are different functions"]},
{chapter:3,difficulty:"medium",question:"The 8-puzzle has how many reachable states?",options:["8!","9!/2","2^8","9^9"],answerIndex:1,explanation:"The 8-puzzle has 9!/2 = 181,440 reachable states. Half of all permutations are unreachable due to parity constraints.",wrongExplanations:["8! ignores the blank tile","2^8 doesn't account for permutations","9^9 overcounts‚Äîtiles can't repeat positions"]},
{chapter:3,difficulty:"easy",question:"Which is an informed search strategy?",options:["Breadth-first search","Depth-first search","Greedy best-first search","Depth-limited search"],answerIndex:2,explanation:"Greedy best-first search uses a heuristic h(n) to estimate distance to goal‚Äîit has domain-specific knowledge, making it 'informed'.",wrongExplanations:["BFS uses no heuristic‚Äîit's uninformed","DFS uses no heuristic‚Äîit's uninformed","Depth-limited is uninformed, just bounded"]},
{chapter:3,difficulty:"hard",question:"Graph search differs from tree search by:",options:["Using graphs instead of trees as input","Keeping track of explored states to avoid repetition","Being slower in all cases","Using different goal tests"],answerIndex:1,explanation:"Graph search maintains a closed/explored set of already-expanded states, preventing re-expansion and avoiding infinite loops on graphs.",wrongExplanations:["Both can work on any state space structure","Graph search can be faster by avoiding redundancy","Goal tests are the same"]},
{chapter:3,difficulty:"medium",question:"The Manhattan distance heuristic for the 8-puzzle:",options:["Counts misplaced tiles only","Sums horizontal and vertical distances of each tile from its goal position","Measures actual shortest path length","Counts total moves made so far"],answerIndex:1,explanation:"Manhattan distance sums |Œîx| + |Œîy| for each tile‚Äîhow many moves if tiles could pass through each other. It's admissible and dominates misplaced tiles.",wrongExplanations:["That's the misplaced tiles heuristic","Actual path length isn't known in advance","That's g(n), not h(n)"]},
{chapter:3,difficulty:"easy",question:"A state space is:",options:["Physical room for the computer","The set of all states reachable from initial state by actions","A data structure for storing states","The goal state only"],answerIndex:1,explanation:"The state space is the full set of states reachable through any sequence of actions from the initial state‚Äîthe search landscape.",wrongExplanations:["It's abstract, not physical","It's the conceptual space, not just storage","It includes all reachable states, not just the goal"]},
{chapter:3,difficulty:"medium",question:"Bidirectional search works by:",options:["Searching in two random directions","Searching forward from start and backward from goal simultaneously","Using two different heuristics","Exploring twice as many nodes"],answerIndex:1,explanation:"Bidirectional search runs two simultaneous searches‚Äîforward from start, backward from goal‚Äîmeeting in the middle for potential speedup.",wrongExplanations:["The directions are specific, not random","It uses one strategy from each end","It can explore fewer nodes if they meet early"]},
{chapter:3,difficulty:"hard",question:"IDA* (Iterative Deepening A*) uses:",options:["Depth limits like regular iterative deepening","f-cost limits instead of depth limits","Random cutoff values","No limits at all"],answerIndex:1,explanation:"IDA* uses the f(n) = g(n) + h(n) value as cutoff, increasing it each iteration to the minimum f-value that exceeded the previous limit.",wrongExplanations:["IDA* uses f-cost, not pure depth","Cutoffs are systematic, not random","It definitely uses cutoff limits"]},
{chapter:3,difficulty:"easy",question:"A goal test determines:",options:["The optimal path cost","Whether a given state is a goal state","The branching factor","How to generate successors"],answerIndex:1,explanation:"The goal test is a function that takes a state and returns true if it satisfies the goal condition, false otherwise.",wrongExplanations:["Path cost is separate from goal testing","Branching factor comes from the successor function","Successor generation is the transition model"]},
{chapter:3,difficulty:"medium",question:"The effective branching factor b* is:",options:["The actual average number of children per node","The branching factor of a uniform tree with same nodes and depth as the search","The maximum branching in the tree","The minimum branching in the tree"],answerIndex:1,explanation:"b* is computed from N = 1 + b* + (b*)¬≤ + ... + (b*)^d where N is nodes generated and d is solution depth. It measures heuristic quality.",wrongExplanations:["That's the actual branching factor, not effective","Maximum doesn't capture average behavior","Minimum doesn't either"]},
{chapter:3,difficulty:"hard",question:"If h2(n) ‚â• h1(n) for all n, and both are admissible, then:",options:["h1 dominates h2","h2 dominates h1 and will never expand more nodes","They will always expand the same nodes","Neither dominates the other"],answerIndex:1,explanation:"If h2 ‚â• h1 everywhere and both admissible, h2 dominates h1. A* with h2 expands every node h1 expands, but not vice versa‚Äîh2 is more informed.",wrongExplanations:["h2 dominates, not h1","They generally expand different node sets","Dominance is defined when this inequality holds"]},
{chapter:3,difficulty:"easy",question:"What data structure does BFS use?",options:["Stack (LIFO)","Queue (FIFO)","Priority queue","Hash table"],answerIndex:1,explanation:"BFS uses a FIFO queue: first generated, first expanded. This ensures level-by-level exploration.",wrongExplanations:["Stack is for DFS","Priority queue is for UCS/A*","Hash tables are for the explored set, not frontier"]},
{chapter:3,difficulty:"medium",question:"Greedy best-first search is:",options:["Always optimal","Complete in finite spaces with graph search","Guaranteed fastest","Always expands minimal nodes"],answerIndex:1,explanation:"Greedy best-first is complete with graph search in finite spaces (avoids loops) but NOT optimal‚Äîit ignores path cost g(n).",wrongExplanations:["It's not optimal‚Äîignores actual cost","Speed isn't guaranteed","It can expand many unnecessary nodes"]},
{chapter:3,difficulty:"easy",question:"The path cost function:",options:["Assigns a cost to each path (sequence of actions)","Only considers the final state reached","Ignores all action costs","Is always exactly 1 per action"],answerIndex:0,explanation:"Path cost sums the costs of actions in a path. It's typically additive: g(n) = g(parent) + step_cost(parent, action, n).",wrongExplanations:["It considers the entire path, not just destination","Action costs are central","Step costs can vary by action"]},
{chapter:3,difficulty:"hard",question:"RBFS (Recursive Best-First Search):",options:["Uses O(bd) memory like A*","Keeps track of the best alternative path's f-value and backtracks when current exceeds it","Is not optimal even with admissible heuristics","Never regenerates nodes"],answerIndex:1,explanation:"RBFS stores the f-value of the best alternative at each ancestor. It uses linear space but may regenerate nodes, trading time for space.",wrongExplanations:["A* uses O(bd); RBFS uses O(bd) for memory‚Äîlinear","It IS optimal with admissible heuristics","It does regenerate nodes when backtracking"]},
{chapter:3,difficulty:"medium",question:"A solution in search is:",options:["Any state in the state space","A path from initial state to a goal state","The goal state alone","The fastest algorithm"],answerIndex:1,explanation:"A solution is an action sequence (path) leading from the initial state to a state that satisfies the goal test.",wrongExplanations:["Not every state is a solution","The path matters, not just the destination state","Solutions are paths, not algorithms"]},
{chapter:3,difficulty:"easy",question:"The branching factor b is:",options:["The depth of the tree","The maximum number of successors of any node","The number of goal states","The cost of the optimal solution"],answerIndex:1,explanation:"The branching factor is the number of successors (children) each node has. For complexity analysis, we use the maximum or average.",wrongExplanations:["Depth is d, not b","Goal count is unrelated to branching","Optimal cost is a path property"]},
{chapter:3,difficulty:"medium",question:"Depth-limited search:",options:["Is complete for all problems","Avoids infinite paths by cutting off at depth limit l","Always finds optimal solutions","Uses the same memory as BFS"],answerIndex:1,explanation:"Depth-limited search is DFS with a depth cutoff l, preventing infinite paths. Complete if solution is within l, not optimal unless l = d.",wrongExplanations:["It's incomplete if solution is beyond limit l","Not optimal‚Äîmay find deep solutions first","It uses O(bl) memory, like DFS"]},
{chapter:3,difficulty:"hard",question:"The space complexity of A* is:",options:["O(d) - linear in depth","O(b^d) - exponential in depth","O(log n)","O(1) - constant"],answerIndex:1,explanation:"A* stores all generated nodes in memory (frontier + explored), which can grow exponentially: O(b^d) in the worst case.",wrongExplanations:["Linear space is IDA* or RBFS, not A*","O(log n) isn't achievable for general search","Storing the frontier requires significant memory"]},
{chapter:3,difficulty:"easy",question:"An uninformed search strategy:",options:["Uses domain-specific heuristics","Has no information beyond the problem definition","Is always slower than informed search","Cannot find solutions"],answerIndex:1,explanation:"Uninformed (blind) search uses only information available in the problem definition‚Äîno heuristics about how close states are to the goal.",wrongExplanations:["Heuristics define informed search","Uninformed can sometimes be competitive","Uninformed strategies can find solutions"]},
{chapter:3,difficulty:"medium",question:"The space complexity of DFS is:",options:["O(b^d) - exponential","O(bm) - linear in maximum depth m","O(d) - linear in solution depth","O(1) - constant"],answerIndex:1,explanation:"DFS stores only the current path plus unexplored siblings at each level: O(bm) where m is maximum depth. Very space-efficient.",wrongExplanations:["Exponential is BFS/A* memory","O(d) ignores siblings at each level","Search always needs some memory"]},
{chapter:3,difficulty:"hard",question:"A* is optimally efficient, meaning:",options:["It runs in minimum time","No algorithm with the same heuristic expands fewer nodes than A*","It uses minimum memory","It finds solutions faster than any other algorithm"],answerIndex:1,explanation:"A* with a given heuristic expands no more nodes than any other optimal algorithm using the same heuristic (or less information).",wrongExplanations:["Optimally efficient refers to nodes, not absolute time","Memory efficiency isn't the claim","Speed depends on many factors beyond optimality"]},
{chapter:3,difficulty:"easy",question:"What is the purpose of the transition model?",options:["To test if a state is the goal","To determine what states result from actions","To calculate path costs","To generate random states"],answerIndex:1,explanation:"The transition model (successor function) specifies what state results from taking an action in a given state: RESULT(s, a) ‚Üí s'.",wrongExplanations:["Goal testing is a separate function","Path costs come from the cost function","Transitions are deterministic, not random"]},
{chapter:3,difficulty:"medium",question:"The heuristic function h(n) estimates:",options:["The cost from start to node n","The cost from node n to the nearest goal","The total path cost through n","The number of nodes to expand"],answerIndex:1,explanation:"h(n) estimates the cost of the cheapest path from n to any goal state. It provides forward-looking guidance for informed search.",wrongExplanations:["Start to n is g(n)","Total through n is f(n) = g(n) + h(n)","Node count isn't what h(n) estimates"]},
{chapter:3,difficulty:"hard",question:"The complexity of iterative deepening compared to BFS:",options:["Is much worse because nodes are regenerated","Is asymptotically the same: O(b^d)","Is always better","Cannot be compared"],answerIndex:1,explanation:"Despite regeneration, ID's overhead is small: O(b^d) vs BFS's O(b^d). The factor is b/(b-1), approaching 1 for large b.",wrongExplanations:["Regeneration adds only a constant factor","It's the same asymptotically, not always better","They're comparable and nearly equal"]},
{chapter:3,difficulty:"easy",question:"A node in a search tree contains:",options:["Only the state","State, parent, action, path cost, and depth","Just the goal test result","Only the heuristic value"],answerIndex:1,explanation:"A search node contains: state, parent pointer (for path reconstruction), action that led here, path cost g(n), and depth.",wrongExplanations:["Nodes contain more than just state","Goal test is applied to nodes, not stored in them","h(n) is computed, may be stored, but isn't the only content"]},
{chapter:3,difficulty:"medium",question:"When is DFS preferred over BFS?",options:["When optimal solutions are required","When memory is limited and solutions are deep","When solutions are near the root","When all solutions have the same depth"],answerIndex:1,explanation:"DFS is preferred when memory is scarce and solutions are expected to be deep, since it uses O(bm) space vs BFS's O(b^d).",wrongExplanations:["BFS is preferred for optimality (unit costs)","BFS is better when solutions are shallow","Same depth doesn't specifically favor DFS"]},
{chapter:3,difficulty:"hard",question:"A* never reopens nodes in the closed set when:",options:["The heuristic is admissible","The heuristic is consistent (monotonic)","The heuristic is always zero","The graph has no cycles"],answerIndex:1,explanation:"With a consistent heuristic, A* always finds optimal paths to nodes on first expansion‚Äîf values never decrease along paths‚Äîso reopening is unnecessary.",wrongExplanations:["Admissibility alone doesn't prevent reopening","h=0 is consistent but uninformative","Cycles don't determine reopening behavior"]},
{chapter:3,difficulty:"easy",question:"The misplaced tiles heuristic for 8-puzzle:",options:["Counts how many tiles are not in their goal position","Calculates Manhattan distance for each tile","Estimates the exact solution length","Counts empty spaces"],answerIndex:0,explanation:"Misplaced tiles h(n) = count of tiles not in goal position. It's admissible because each misplaced tile needs at least one move.",wrongExplanations:["Manhattan distance is a different, stronger heuristic","It's an estimate, not exact","There's only one empty space"]},
{chapter:3,difficulty:"medium",question:"In uniform cost search, the frontier is ordered by:",options:["Heuristic value h(n)","Path cost g(n)","Depth in the tree","Evaluation f(n) = g(n) + h(n)"],answerIndex:1,explanation:"UCS uses a priority queue ordered by g(n), the actual path cost from start. It's A* with h(n) = 0.",wrongExplanations:["h(n) ordering is greedy best-first","Depth ordering is BFS","f(n) ordering is A*"]},
{chapter:3,difficulty:"hard",question:"A heuristic derived from a relaxed problem is guaranteed to be:",options:["Optimal","Admissible","Consistent","All of the above"],answerIndex:3,explanation:"Relaxed problem heuristics remove constraints, making solutions easier. The exact solution to the relaxed problem is admissible and typically consistent.",wrongExplanations:["All three properties typically hold","All three properties typically hold","All three properties typically hold for relaxed heuristics"]},
{chapter:3,difficulty:"easy",question:"What makes A* complete?",options:["Using any heuristic function","Having a finite branching factor and positive step costs","Being faster than BFS","Using depth limits"],answerIndex:1,explanation:"A* is complete if the branching factor is finite, step costs are positive (no zero-cost loops), and the heuristic is admissible.",wrongExplanations:["The heuristic must be admissible specifically","Speed doesn't determine completeness","A* doesn't use depth limits"]},
{chapter:3,difficulty:"medium",question:"The optimality of BFS depends on:",options:["Using a consistent heuristic","All step costs being equal (typically 1)","Having a finite state space","Using graph search instead of tree search"],answerIndex:1,explanation:"BFS finds the shallowest solution‚Äîoptimal only if all actions have the same cost. Otherwise, use UCS or A*.",wrongExplanations:["BFS doesn't use heuristics","Finite space affects completeness, not optimality","Graph search affects completeness, not BFS optimality"]},
{chapter:3,difficulty:"hard",question:"SMA* (Simplified Memory-Bounded A*) handles memory limits by:",options:["Stopping search when memory fills","Dropping the worst-leaf node when memory is full","Using only O(1) memory","Switching to DFS when memory runs out"],answerIndex:1,explanation:"SMA* drops the node with highest f-value (worst) when memory fills, remembering its f-value in the parent for potential regeneration.",wrongExplanations:["It continues by dropping nodes","It uses as much memory as available","It stays an A*-variant, not switching to DFS"]},

// CHAPTER 19: LEARNING FROM EXAMPLES (30 questions)
{chapter:19,difficulty:"easy",question:"Supervised learning involves:",options:["Learning without any feedback or labels","Learning from labeled input-output pairs","Learning only from rewards and penalties","Learning without any data"],answerIndex:1,explanation:"Supervised learning uses labeled training examples (input-output pairs) provided by a 'teacher' to learn a function mapping inputs to outputs.",wrongExplanations:["Feedback via labels IS provided","Reward-based is reinforcement learning","All ML requires data"]},
{chapter:19,difficulty:"easy",question:"In classification, the output is:",options:["A continuous real number","A discrete category or class label","Always a binary 0 or 1","An undefined value"],answerIndex:1,explanation:"Classification predicts discrete categorical outputs (class labels) like spam/not-spam, cat/dog, or digit 0-9.",wrongExplanations:["Continuous output is regression","Binary is a special case of classification","Outputs are well-defined categories"]},
{chapter:19,difficulty:"medium",question:"Overfitting occurs when:",options:["The model is too simple to capture patterns","The model fits training data noise and fails to generalize","The model underfits the data","There is too much training data"],answerIndex:1,explanation:"Overfitting: model learns training noise/specifics, performing well on training data but poorly on unseen test data. Too much complexity.",wrongExplanations:["Too simple causes underfitting","Underfitting is the opposite problem","More data typically reduces overfitting"]},
{chapter:19,difficulty:"easy",question:"A decision tree classifies by:",options:["Computing distances to all training examples","Following attribute tests from root to leaf nodes","Using neural network layers","Random selection from classes"],answerIndex:1,explanation:"Decision trees test attributes at internal nodes, following branches based on values, until reaching a leaf with the class prediction.",wrongExplanations:["Distance-based is k-NN","Neural networks are a different model","Decision trees are deterministic, not random"]},
{chapter:19,difficulty:"medium",question:"Information gain in decision trees measures:",options:["Financial profit from predictions","The reduction in entropy from splitting on an attribute","The depth of the tree","How many leaves are created"],answerIndex:1,explanation:"Information gain = entropy(parent) - weighted_entropy(children). Higher gain means the attribute better separates classes.",wrongExplanations:["It's about information/entropy, not money","Depth is structural, not a splitting criterion","Leaf count doesn't measure information"]},
{chapter:19,difficulty:"hard",question:"The VC dimension measures:",options:["The number of features in the model","The capacity/expressiveness of a hypothesis class","Validation accuracy on cross-validation","Variance in predictions"],answerIndex:1,explanation:"VC dimension is the largest set of points the hypothesis class can shatter (classify all 2^n ways). It quantifies model capacity/complexity.",wrongExplanations:["Feature count is input dimensionality","VC is about capacity, not validation accuracy","Variance is a statistical measure, not VC dimension"]},
{chapter:19,difficulty:"easy",question:"k-Nearest Neighbors classifies based on:",options:["A learned decision tree","The majority class among k closest training examples","A trained neural network","Maximum likelihood estimation"],answerIndex:1,explanation:"k-NN finds the k closest training examples (by distance) and predicts the majority class. It's instance-based, non-parametric.",wrongExplanations:["k-NN doesn't build decision trees","k-NN doesn't use neural networks","k-NN isn't probability-based in the ML sense"]},
{chapter:19,difficulty:"medium",question:"Cross-validation is used to:",options:["Validate data against domain experts","Estimate how well a model will generalize to unseen data","Cross-reference multiple databases","Validate user credentials"],answerIndex:1,explanation:"Cross-validation (e.g., k-fold) trains on subsets and tests on held-out folds, estimating generalization performance without a separate test set.",wrongExplanations:["It's a statistical technique, not expert validation","Database cross-referencing is different","It's about model evaluation, not authentication"]},
{chapter:19,difficulty:"easy",question:"Linear regression predicts:",options:["Discrete class labels","Continuous numerical values","Binary categories only","Text strings"],answerIndex:1,explanation:"Linear regression predicts continuous outputs as a linear function of inputs: y = w¬∑x + b, minimizing squared error.",wrongExplanations:["Discrete labels are classification","Binary is logistic regression (classification)","Text prediction is NLP/generation"]},
{chapter:19,difficulty:"medium",question:"Regularization helps prevent overfitting by:",options:["Adding more training data automatically","Adding a penalty for model complexity to the loss function","Removing features randomly","Increasing model capacity"],answerIndex:1,explanation:"Regularization adds terms like L1 (|w|) or L2 (w¬≤) to the loss, penalizing large weights and encouraging simpler, more generalizable models.",wrongExplanations:["Data augmentation is separate","Feature removal is feature selection","Increasing capacity increases overfitting risk"]},
{chapter:19,difficulty:"hard",question:"The bias-variance tradeoff describes:",options:["Political considerations in AI development","The tradeoff between underfitting (high bias) and overfitting (high variance)","Biased versus unbiased estimators only","Data collection trade-offs"],answerIndex:1,explanation:"High bias = underfitting (model too simple). High variance = overfitting (model too sensitive to training data). Must balance both for generalization.",wrongExplanations:["It's a statistical/ML concept, not political","It includes model behavior, not just estimator theory","It's about model complexity, not data collection"]},
{chapter:19,difficulty:"easy",question:"A training set is used to:",options:["Evaluate final model performance","Train/fit the model parameters","Choose hyperparameters","Deploy the model to production"],answerIndex:1,explanation:"Training data is used to learn model parameters (weights, tree structure, etc.) by minimizing training error.",wrongExplanations:["Test set evaluates final performance","Validation set chooses hyperparameters","Deployment is a separate phase"]},
{chapter:19,difficulty:"medium",question:"Ensemble methods combine:",options:["Data from multiple sources only","Multiple models to improve prediction accuracy","Multiple datasets into one","Multiple loss functions"],answerIndex:1,explanation:"Ensembles like Random Forests, Boosting, and Bagging combine predictions from multiple models for better accuracy and robustness.",wrongExplanations:["Data fusion is different from model ensembling","Ensembles combine models, not just datasets","Loss function combination is less common"]},
{chapter:19,difficulty:"easy",question:"Unsupervised learning differs from supervised because:",options:["It's faster to train","There are no labeled outputs in the training data","It uses more sophisticated algorithms","It requires human supervision"],answerIndex:1,explanation:"Unsupervised learning finds patterns in unlabeled data‚Äîno target outputs provided. Examples: clustering, dimensionality reduction.",wrongExplanations:["Speed varies by algorithm, not supervision type","Algorithm sophistication varies","'Unsupervised' means no labels, not no human involvement"]},
{chapter:19,difficulty:"hard",question:"PAC learning (Probably Approximately Correct) provides:",options:["Guaranteed perfect accuracy","Bounds on sample complexity for learning with high probability","Only approximate, unreliable results","Parallel computing acceleration"],answerIndex:1,explanation:"PAC theory gives bounds on training examples needed to learn a concept within error Œµ with probability 1-Œ¥. Foundational for learning theory.",wrongExplanations:["PAC gives probabilistic guarantees, not perfection","'Approximately correct' is well-defined mathematically","PAC is theory, not about parallel computing"]},
{chapter:19,difficulty:"medium",question:"Feature selection helps by:",options:["Creating entirely new features","Choosing the most relevant features, reducing dimensionality","Selecting the best training examples","Choosing the learning algorithm"],answerIndex:1,explanation:"Feature selection identifies which input features are most predictive, removing irrelevant ones to improve efficiency and reduce overfitting.",wrongExplanations:["Creating new features is feature engineering","Example selection is instance selection","Algorithm choice is model selection"]},
{chapter:19,difficulty:"easy",question:"A hypothesis in machine learning is:",options:["A scientific conjecture to be tested","A function mapping inputs to predicted outputs","A random guess","The learning algorithm itself"],answerIndex:1,explanation:"A hypothesis h is a specific function from inputs to outputs, selected by the learning algorithm from the hypothesis space.",wrongExplanations:["More specific than a general conjecture","It's learned, not random","The algorithm selects hypotheses, it isn't one"]},
{chapter:19,difficulty:"medium",question:"Pruning in decision trees:",options:["Adds more branches for accuracy","Removes branches to prevent overfitting","Plants new decision nodes","Increases tree depth"],answerIndex:1,explanation:"Pruning removes subtrees that don't improve generalization, reducing overfitting. Can be pre-pruning (stop early) or post-pruning (remove after building).",wrongExplanations:["Adding branches increases overfitting risk","Pruning removes, doesn't add","Pruning decreases, not increases, tree size"]},
{chapter:19,difficulty:"hard",question:"The No Free Lunch theorem states:",options:["All learning algorithms have costs","No single algorithm is best across all possible problems","Lunch is never free in the cafeteria","Simple algorithms always work best"],answerIndex:1,explanation:"NFL: averaged over all possible problems, no algorithm outperforms any other. Algorithm choice depends on problem structure/assumptions.",wrongExplanations:["It's about algorithm universality, not literal costs","Nothing to do with actual lunch","Simple isn't always best‚Äîdepends on the problem"]},
{chapter:19,difficulty:"easy",question:"A test set is used to:",options:["Train the model parameters","Tune hyperparameters","Evaluate final model performance on unseen data","Debug the code"],answerIndex:2,explanation:"The test set provides an unbiased estimate of final model performance‚Äîdata never seen during training or validation.",wrongExplanations:["Training data trains parameters","Validation data tunes hyperparameters","Debugging is a development activity"]},
{chapter:19,difficulty:"medium",question:"Bagging (Bootstrap Aggregating) reduces:",options:["Training time significantly","Variance by training on bootstrap samples","Bias in the model","The need for validation"],answerIndex:1,explanation:"Bagging trains multiple models on bootstrap samples (random samples with replacement), averaging predictions to reduce variance.",wrongExplanations:["Bagging increases training time (multiple models)","Boosting addresses bias more directly","Validation is still needed"]},
{chapter:19,difficulty:"hard",question:"Support Vector Machines find:",options:["The average separating boundary","The maximum margin hyperplane between classes","Support vectors after training only","The minimum complexity boundary"],answerIndex:1,explanation:"SVMs find the hyperplane that maximizes the margin (distance to nearest points of each class). Support vectors are the critical boundary points.",wrongExplanations:["Maximum margin, not average","Support vectors define the solution, found during training","Maximum margin is the objective, not minimum complexity"]},
{chapter:19,difficulty:"easy",question:"Entropy in machine learning measures:",options:["Heat in the computer","Impurity or uncertainty in a dataset","Algorithm efficiency","Memory usage"],answerIndex:1,explanation:"Entropy H = -Œ£ p(x) log p(x) measures uncertainty/impurity. Higher entropy = more uniform distribution, maximum uncertainty.",wrongExplanations:["Thermodynamic entropy is different","Entropy measures data uncertainty, not speed","Memory usage is separate"]},
{chapter:19,difficulty:"medium",question:"Gradient descent updates parameters by:",options:["Random changes until improvement","Moving in the direction that decreases the loss function","Trying all possible values","Copying from other models"],answerIndex:1,explanation:"Gradient descent computes the gradient ‚àáL and updates: w ‚Üê w - Œ±‚àáL, moving toward lower loss. Œ± is the learning rate.",wrongExplanations:["Updates are computed, not random","Brute force isn't feasible for continuous parameters","Parameters are learned, not copied"]},
{chapter:19,difficulty:"hard",question:"The kernel trick allows SVMs to:",options:["Make training faster on CPUs","Implicitly compute in high-dimensional space without explicit transformation","Use multiple kernels simultaneously","Skip the optimization step"],answerIndex:1,explanation:"The kernel trick computes inner products in high-dimensional feature space without explicitly computing the transformation, enabling nonlinear separation.",wrongExplanations:["Kernels don't primarily improve speed","One kernel is typically used","Optimization is still required"]},

// CHAPTER 21: REINFORCEMENT LEARNING (40 questions)
{chapter:21,difficulty:"easy",question:"In reinforcement learning, an agent learns by:",options:["Studying labeled examples provided by a teacher","Receiving rewards or penalties for its actions in an environment","Clustering similar data points together","Copying expert behavior exactly"],answerIndex:1,explanation:"RL agents learn through trial-and-error interaction with an environment, receiving scalar rewards that guide learning toward better behavior.",wrongExplanations:["Labeled examples define supervised learning","Clustering is unsupervised learning","Exact imitation is imitation learning, related but distinct"]},
{chapter:21,difficulty:"easy",question:"A policy in reinforcement learning is:",options:["A set of rules for the environment","A mapping from states to actions defining agent behavior","A type of reward function","The learning algorithm used"],answerIndex:1,explanation:"Policy œÄ(s) specifies which action to take in each state. The goal of RL is often to find the optimal policy œÄ*.",wrongExplanations:["Environment rules aren't the policy","Reward functions evaluate, they don't select actions","Policy is the result, not the algorithm"]},
{chapter:21,difficulty:"medium",question:"The discount factor Œ≥ (gamma) determines:",options:["How much to reduce the learning rate","How much future rewards are valued relative to immediate rewards","The probability of exploration","The number of training episodes"],answerIndex:1,explanation:"Œ≥ ‚àà [0,1] discounts future rewards: total = r‚ÇÄ + Œ≥r‚ÇÅ + Œ≥¬≤r‚ÇÇ + ... Lower Œ≥ = more short-sighted; higher Œ≥ = more far-sighted.",wrongExplanations:["Learning rate is Œ±, not Œ≥","Exploration rate is Œµ in Œµ-greedy","Episode count is a training parameter, not Œ≥"]},
{chapter:21,difficulty:"easy",question:"Q-learning is:",options:["A quality assurance technique","A model-free RL algorithm learning action-value function Q(s,a)","A questioning methodology","A quantum computing approach"],answerIndex:1,explanation:"Q-learning learns Q(s,a)‚Äîexpected discounted return from taking action a in state s and following optimal policy thereafter. Model-free and off-policy.",wrongExplanations:["Q stands for 'quality' of action in state","Not a questioning technique","Unrelated to quantum computing"]},
{chapter:21,difficulty:"medium",question:"The exploration vs exploitation tradeoff involves:",options:["Exploring new code vs using tested code","Trying new actions vs using currently best-known actions","Mining resources vs using them","Learning vs applying knowledge"],answerIndex:1,explanation:"Exploration tries new actions to discover potentially better rewards; exploitation uses current best knowledge. Must balance both for optimal learning.",wrongExplanations:["It's about action selection, not code","Resource management is different","It's specifically about action choice in RL"]},
{chapter:21,difficulty:"hard",question:"Temporal difference (TD) learning:",options:["Measures time intervals between actions","Updates estimates using differences between consecutive predictions","Only works with discrete time","Requires complete episodes to learn"],answerIndex:1,explanation:"TD updates V(s) using the TD error: [r + Œ≥V(s') - V(s)]. It learns online from incomplete episodes, bootstrapping from estimates.",wrongExplanations:["It's about value estimate differences, not time measurement","TD works with discrete and continuous time","TD doesn't need complete episodes‚Äîthat's Monte Carlo"]},
{chapter:21,difficulty:"easy",question:"The reward function in RL:",options:["Rewards the programmer for good code","Maps states or state-action pairs to numerical reward values","Always provides positive values","Is learned by the agent"],answerIndex:1,explanation:"R(s) or R(s,a) gives immediate numerical feedback. The agent's goal is to maximize cumulative discounted reward over time.",wrongExplanations:["It rewards the agent, not programmer","Rewards can be negative (penalties)","The reward function is typically given, not learned (though inverse RL learns it)"]},
{chapter:21,difficulty:"medium",question:"Model-based RL differs from model-free because:",options:["It uses 3D models of the environment","It learns/uses a model of environment dynamics (transitions and rewards)","It requires a modeling degree","It's less flexible"],answerIndex:1,explanation:"Model-based RL learns T(s'|s,a) and R(s,a), then uses planning. Model-free learns policy/value directly from experience without a model.",wrongExplanations:["Not about 3D graphics","No academic degree required","Model-based can be more flexible with planning"]},
{chapter:21,difficulty:"hard",question:"The Bellman optimality equation for Q states:",options:["Q*(s,a) = R(s)","Q*(s,a) = R(s,a) + Œ≥ Œ£ P(s'|s,a) max_a' Q*(s',a')","Q*(s,a) = 0","Q*(s,a) = V*(s)"],answerIndex:1,explanation:"Bellman optimality: Q*(s,a) equals immediate reward plus discounted expected value of the best action in the next state.",wrongExplanations:["This ignores future rewards and max operation","Optimal Q is generally non-zero","Q and V are related but not equal"]},
{chapter:21,difficulty:"easy",question:"Œµ-greedy exploration:",options:["Always selects random actions","Selects random action with probability Œµ, best action with 1-Œµ","Never explores new actions","Uses epsilon for learning rate"],answerIndex:1,explanation:"Œµ-greedy: with probability Œµ explore (random action), with 1-Œµ exploit (action with highest Q-value). Simple exploration strategy.",wrongExplanations:["It's not purely random","It does explore when random action chosen","Œµ is for exploration, not learning rate (that's Œ±)"]},
{chapter:21,difficulty:"medium",question:"The value function V(s) represents:",options:["The immediate reward at state s","Expected cumulative discounted reward starting from state s","The count of visits to state s","The cost to reach state s"],answerIndex:1,explanation:"VœÄ(s) = E[Œ£ Œ≥·µór‚Çú | s‚ÇÄ=s, œÄ]: expected sum of discounted rewards when starting at s and following policy œÄ.",wrongExplanations:["Immediate reward is R(s), not V(s)","Visit count is different","Cost to reach is backward-looking; V is forward-looking"]},
{chapter:21,difficulty:"hard",question:"Policy gradient methods:",options:["Compute gradients of state values","Directly optimize policy parameters using gradient ascent on expected return","Only work for discrete action spaces","Don't use neural networks"],answerIndex:1,explanation:"Policy gradients parameterize œÄ(a|s;Œ∏) and compute ‚àáŒ∏ J(Œ∏) to directly optimize the policy, useful for continuous actions and stochastic policies.",wrongExplanations:["They optimize policy, not just values","Work well with continuous actions too","Often combined with neural networks (actor-critic, PPO)"]},
{chapter:21,difficulty:"easy",question:"A Markov Decision Process (MDP) assumes:",options:["All past history is relevant for decisions","The future depends only on current state, not history (Markov property)","States must be discrete","Rewards are always positive"],answerIndex:1,explanation:"Markov property: P(s‚Çú‚Çä‚ÇÅ|s‚Çú,a‚Çú,...,s‚ÇÄ,a‚ÇÄ) = P(s‚Çú‚Çä‚ÇÅ|s‚Çú,a‚Çú). The current state contains all relevant information.",wrongExplanations:["Only current state matters‚Äîkey MDP property","MDPs can have continuous states","Rewards can be negative"]},
{chapter:21,difficulty:"medium",question:"SARSA differs from Q-learning because:",options:["SARSA is off-policy, Q-learning is on-policy","SARSA uses the actual next action taken (on-policy), Q-learning uses max action (off-policy)","SARSA is faster","They are identical algorithms"],answerIndex:1,explanation:"SARSA: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥Q(s',a') - Q(s,a)] using actual a'. Q-learning uses max_a' Q(s',a'). SARSA is on-policy, Q-learning off-policy.",wrongExplanations:["It's the opposite‚ÄîSARSA on-policy, Q-learning off-policy","Speed depends on problem, not the algorithm","They differ in their update rules"]},
{chapter:21,difficulty:"easy",question:"An episode in RL is:",options:["A single action taken","A sequence of states and actions from start to terminal state","An error in the algorithm","One update to the Q-table"],answerIndex:1,explanation:"An episode is a complete trajectory: s‚ÇÄ,a‚ÇÄ,r‚ÇÅ,s‚ÇÅ,a‚ÇÅ,r‚ÇÇ,...,s‚Çú (terminal). Games, maze navigation, etc. have natural episodes.",wrongExplanations:["An episode contains many actions","Not an error term","Multiple updates occur per episode"]},
{chapter:21,difficulty:"medium",question:"Function approximation in RL is needed when:",options:["Functions need approximate values","The state/action space is too large for tabular methods","Approximations are faster","Exact solutions are preferred"],answerIndex:1,explanation:"Tabular methods (Q-tables) fail for large/continuous spaces. Function approximation (neural nets, linear functions) generalizes across similar states.",wrongExplanations:["It's about representing Q/V with functions","Can be slower due to training","Approximation trades exactness for scalability"]},
{chapter:21,difficulty:"hard",question:"The credit assignment problem refers to:",options:["Assigning code authorship","Determining which past actions contributed to current reward","Assigning credits in games","Credit card processing in AI"],answerIndex:1,explanation:"Credit assignment: when reward is delayed, which of many past actions deserve credit? TD methods and eligibility traces help address this.",wrongExplanations:["Not about authorship","Not about game credits in the common sense","Unrelated to financial credit"]},
{chapter:21,difficulty:"easy",question:"A terminal state is:",options:["A state displayed on a computer terminal","A state where the episode ends, no further actions possible","The starting state","An error state"],answerIndex:1,explanation:"Terminal states end episodes: game over, goal reached, time expired. V(terminal) = 0 typically (no future rewards).",wrongExplanations:["Nothing to do with computer terminals","Starting state is s‚ÇÄ, not terminal","Terminal is normal episode ending, not error"]},
{chapter:21,difficulty:"medium",question:"Off-policy learning means:",options:["The policy is off (disabled)","Learning about one policy while following a different behavior policy","Learning offline from stored data only","The opposite of on-policy"],answerIndex:1,explanation:"Off-policy learns target policy œÄ while following behavior policy b (possibly different). Q-learning is off-policy‚Äîlearns optimal Q while exploring.",wrongExplanations:["The policy isn't disabled","Off-policy can learn online too","True, but this describes what it means"]},
{chapter:21,difficulty:"hard",question:"Eligibility traces in TD(Œª) help with:",options:["Tracing bugs in code","Combining TD and Monte Carlo methods, bridging credit assignment over time","Making agents eligible for rewards","Tracking exploration paths"],answerIndex:1,explanation:"Eligibility traces maintain decaying memory of visited states. Œª=0 is pure TD, Œª=1 is Monte Carlo. Helps faster credit assignment.",wrongExplanations:["Not debugging","'Eligible' is about contribution to learning","They do track visits but for learning purposes"]},
{chapter:21,difficulty:"easy",question:"The action-value function Q(s,a) gives:",options:["Only the immediate reward for action a","Expected cumulative reward for taking a in s and following the policy","The quality of code","The cost of action a"],answerIndex:1,explanation:"Q(s,a) = E[Œ£ Œ≥·µór‚Çú | s‚ÇÄ=s, a‚ÇÄ=a, œÄ]: expected return starting with action a in state s, then following policy.",wrongExplanations:["Q includes all future rewards, not just immediate","It's about value, not code quality","Q is reward-based, not cost-based"]},
{chapter:21,difficulty:"medium",question:"Monte Carlo RL methods require:",options:["Random sampling only","Complete episodes to update value estimates","No exploration at all","Continuous state spaces only"],answerIndex:1,explanation:"Monte Carlo averages actual returns from complete episodes. Unlike TD, it doesn't bootstrap‚Äîwaits for episode end.",wrongExplanations:["Random sampling is part of it, but episodes are key","Exploration is still needed","Works with any state space type"]},
{chapter:21,difficulty:"hard",question:"The deadly triad in deep RL refers to combining:",options:["Three deadly algorithms","Function approximation, bootstrapping, and off-policy learning","Three terminal states","Memory, computation, and time limits"],answerIndex:1,explanation:"The deadly triad can cause instability: function approximation + bootstrapping + off-policy. Deep Q-networks use techniques to mitigate this.",wrongExplanations:["It's a specific technical concept","Not about terminal states","Not about resource limits"]},
{chapter:21,difficulty:"easy",question:"In a gridworld, states typically represent:",options:["Grid positions the agent can occupy","The agent's internal beliefs","Action sequences","Reward values"],answerIndex:0,explanation:"Gridworld states are usually (row, column) positions. Actions move between positions; rewards may be at specific cells or for reaching goals.",wrongExplanations:["Beliefs are part of the agent, not environment state","Actions lead to states, aren't states","Rewards are values at states, not states themselves"]},
{chapter:21,difficulty:"medium",question:"Value iteration computes:",options:["The number of iterations needed","Optimal value function through repeated Bellman updates until convergence","The value of the algorithm","Iteration count for termination"],answerIndex:1,explanation:"Value iteration: V(s) ‚Üê max_a [R(s,a) + Œ≥ Œ£ P(s'|s,a)V(s')] repeatedly until values converge. Dynamic programming method.",wrongExplanations:["It computes values, not iteration counts","'Value' refers to state values, not algorithm worth","Termination is when values converge"]},
{chapter:21,difficulty:"hard",question:"Actor-critic methods combine:",options:["Different acting styles","Policy gradient (actor) and value function estimation (critic)","Multiple actors in multi-agent RL","Critical analysis of actions"],answerIndex:1,explanation:"Actor = policy œÄŒ∏(a|s) for action selection. Critic = value function VœÜ(s) for evaluating states. Critic reduces variance in policy gradient.",wrongExplanations:["Not about acting styles","Multi-agent is different","'Critic' is a technical term for value estimator"]},

// CHAPTER 23: NLP (35 questions)
{chapter:23,difficulty:"easy",question:"Natural Language Processing (NLP) deals with:",options:["Processing only spoken language","Computational understanding and generation of human language","Natural number computations","Processing chemical language of nature"],answerIndex:1,explanation:"NLP is the field of AI/CS focused on enabling computers to understand, interpret, and generate human language in useful ways.",wrongExplanations:["NLP includes both text and speech","Natural numbers are a different field","NLP is about human language"]},
{chapter:23,difficulty:"easy",question:"Tokenization is the process of:",options:["Creating security tokens","Splitting text into individual tokens (words, subwords, characters)","Encrypting text data","Removing tokens from text"],answerIndex:1,explanation:"Tokenization breaks text into units: words, subwords (BPE, WordPiece), or characters. It's the first step in most NLP pipelines.",wrongExplanations:["Not about security tokens","Not encryption","Not removal‚Äîit's segmentation"]},
{chapter:23,difficulty:"medium",question:"Part-of-speech (POS) tagging assigns:",options:["Position tags for geocoding","Grammatical categories (noun, verb, adjective, etc.) to words","Priority tags for importance","Post-office location tags"],answerIndex:1,explanation:"POS tagging labels each word with its grammatical role: NN (noun), VB (verb), JJ (adjective), etc. Essential for syntax analysis.",wrongExplanations:["Not about geographic positions","Not about importance ranking","Unrelated to postal systems"]},
{chapter:23,difficulty:"easy",question:"Parsing in NLP refers to:",options:["Parsing numbers from strings","Analyzing the grammatical structure of sentences","Parsing command-line arguments","Partial analysis only"],answerIndex:1,explanation:"Parsing analyzes sentence structure, producing parse trees showing grammatical relationships (subject, predicate, modifiers, etc.).",wrongExplanations:["That's numeric parsing, different domain","Not about command lines","Parsing aims to be complete, not partial"]},
{chapter:23,difficulty:"medium",question:"Semantic analysis determines:",options:["The number of words in a sentence","The meaning of words and sentences in context","The speed of text processing","The font of the text"],answerIndex:1,explanation:"Semantics concerns meaning: word senses, entity references, relationships, propositional content‚Äîwhat language actually means.",wrongExplanations:["Word count is superficial, not semantic","Processing speed is computational","Font is presentation, not meaning"]},
{chapter:23,difficulty:"hard",question:"Word embeddings (like Word2Vec) represent words as:",options:["One-hot sparse vectors","Dense real-valued vectors capturing semantic relationships","Binary codes","Hash table keys"],answerIndex:1,explanation:"Word embeddings map words to dense vectors (e.g., 300 dimensions) where similar words have similar vectors. Enable semantic arithmetic.",wrongExplanations:["One-hot is the old approach‚Äîsparse, no semantics","Not binary‚Äîcontinuous values","Hashing doesn't capture semantics"]},
{chapter:23,difficulty:"easy",question:"A language model predicts:",options:["Which languages are most popular","The probability of word sequences or next words","The language of a document","Future language trends"],answerIndex:1,explanation:"Language models compute P(w‚ÇÅ,w‚ÇÇ,...,w‚Çô) or P(w‚Çô|w‚ÇÅ,...,w‚Çô‚Çã‚ÇÅ). They power text generation, completion, and scoring.",wrongExplanations:["Not about language popularity","Language identification is different","Not about trend prediction"]},
{chapter:23,difficulty:"medium",question:"Named Entity Recognition (NER) identifies:",options:["File names in directories","Named entities like persons, places, organizations in text","Variable names in code","DNS entries"],answerIndex:1,explanation:"NER finds and classifies proper nouns: PERSON (John), ORG (Google), LOC (Paris), DATE, MONEY, etc. Key for information extraction.",wrongExplanations:["Not about file systems","Not about programming variables","Not about network names"]},
{chapter:23,difficulty:"hard",question:"The attention mechanism in NLP allows models to:",options:["Pay attention to user engagement","Dynamically focus on relevant parts of input when generating output","Measure attention span of readers","Filter out unimportant words permanently"],answerIndex:1,explanation:"Attention computes weighted combinations of input representations, allowing variable focus. Self-attention in Transformers enables parallel processing of sequences.",wrongExplanations:["Not about user metrics","Not about reader psychology","Attention is dynamic, not permanent filtering"]},
{chapter:23,difficulty:"easy",question:"Machine translation converts:",options:["Code to machine language","Text from one human language to another","Speech to text only","Numbers to text"],answerIndex:1,explanation:"MT translates text between human languages (English‚ÜîFrench, etc.). Neural MT uses encoder-decoder architectures with attention.",wrongExplanations:["That's compilation, not MT","Speech-to-text is speech recognition","Number-to-text is number formatting"]},
{chapter:23,difficulty:"medium",question:"A context-free grammar (CFG) consists of:",options:["Grammar rules that ignore all context","Production rules with single non-terminal on the left-hand side","Rules that work in any context","Grammar without any structure"],answerIndex:1,explanation:"CFG: rules of form A ‚Üí Œ≤ where A is a single non-terminal. 'Context-free' means A always rewrites the same way regardless of surrounding symbols.",wrongExplanations:["It has specific structural rules","Single non-terminal on left is the defining property","CFGs define structure, they have plenty of it"]},
{chapter:23,difficulty:"easy",question:"Sentiment analysis determines:",options:["The sentence count in a document","The emotional tone or opinion (positive/negative/neutral) expressed","The sensory content of text","The sentimental value of items"],answerIndex:1,explanation:"Sentiment analysis classifies text by opinion polarity: positive, negative, neutral. Used for reviews, social media monitoring, etc.",wrongExplanations:["Sentence counting is basic statistics","Not about sensory descriptions","Not about monetary/emotional value of objects"]},
{chapter:23,difficulty:"medium",question:"N-grams are:",options:["Units of weight measurement","Contiguous sequences of n items (words, characters) from text","Graphical elements in NLP","N grammatical rules"],answerIndex:1,explanation:"N-grams: unigrams (single words), bigrams (pairs), trigrams (triples), etc. Used in language modeling and feature extraction.",wrongExplanations:["Nothing to do with weight","Not graphical elements","Not a count of grammar rules"]},
{chapter:23,difficulty:"hard",question:"Pragmatics in NLP concerns:",options:["Practical programming","How context affects meaning beyond literal semantics","Pragmatic file formats","Practical NLP applications"],answerIndex:1,explanation:"Pragmatics studies how context, speaker intent, and shared knowledge influence meaning. 'Can you pass the salt?' is a request, not yes/no question.",wrongExplanations:["Not about programming practices","It's about contextual meaning","Not about application domains"]},
{chapter:23,difficulty:"easy",question:"Stop words are:",options:["Words that halt processing","Common words (the, is, a) often filtered in some NLP tasks","Error indicators","Final words in sentences"],answerIndex:1,explanation:"Stop words are high-frequency, low-content words often removed in tasks like IR. Modern deep learning often keeps them for context.",wrongExplanations:["They don't stop processing","Not error indicators","Not about sentence position"]},
{chapter:23,difficulty:"medium",question:"Coreference resolution identifies:",options:["Core references in bibliography","When different expressions refer to the same entity","Reference errors in code","Primary sources in research"],answerIndex:1,explanation:"Coreference: 'John said he was tired'‚Äî'he' and 'John' corefer. Essential for understanding documents with pronouns and noun phrases.",wrongExplanations:["Not about citations","Not about code references","Not about source primacy"]},
{chapter:23,difficulty:"hard",question:"Transformers revolutionized NLP by:",options:["Converting voltage levels","Using self-attention to process all positions in parallel","Transforming data formats","Translating between programming languages"],answerIndex:1,explanation:"Transformers (Vaswani 2017) replace RNNs with self-attention, enabling parallel training on sequences. Foundation for BERT, GPT, etc.",wrongExplanations:["Not electrical transformers","Not about data format conversion","That's transpilation, different domain"]},
{chapter:23,difficulty:"easy",question:"Text classification assigns:",options:["Numbers to each character","Predefined categories to text documents","Authors to anonymous texts","Grades to student essays"],answerIndex:1,explanation:"Text classification labels documents with categories: spam/ham, topics, sentiment, language, etc. Fundamental NLP task.",wrongExplanations:["Character numbering is encoding","Authorship attribution is a specialized task","Essay grading is a specific application"]},
{chapter:23,difficulty:"medium",question:"BLEU score is used to evaluate:",options:["The quality of machine translation output","The blueness of images","The speed of NLP algorithms","The beauty of language"],answerIndex:1,explanation:"BLEU (Bilingual Evaluation Understudy) measures MT quality by comparing n-gram precision against reference translations.",wrongExplanations:["Not related to color","Not about computational speed","Not about aesthetic quality"]},
{chapter:23,difficulty:"hard",question:"Beam search in NLP decoding:",options:["Searches for beams of light","Keeps top-k hypotheses at each step during sequence generation","Uses laser beam alignment","Searches beam-structured data"],answerIndex:1,explanation:"Beam search maintains k best partial sequences during decoding, balancing quality and computational cost. Used in MT, captioning, etc.",wrongExplanations:["Not about physical beams","Not about physical alignment","Not about data structure shape"]},

// CHAPTER 25: ROBOTICS (35 questions)
{chapter:25,difficulty:"easy",question:"A robot is defined as:",options:["Any computer system","A mechanical agent that perceives and acts in the physical world","Only humanoid machines","Virtual assistants only"],answerIndex:1,explanation:"Robots are physical agents with sensors to perceive their environment and actuators to take physical actions. They inhabit the real world.",wrongExplanations:["Computers aren't necessarily physical agents","Robots come in many forms, not just humanoid","Virtual assistants aren't physically embodied"]},
{chapter:25,difficulty:"easy",question:"Robot sensors include:",options:["Only cameras","Cameras, LIDAR, touch sensors, encoders, IMUs, and more","Only touch sensors","Only GPS receivers"],answerIndex:1,explanation:"Robots use diverse sensors: cameras (vision), LIDAR (laser ranging), sonar, touch, wheel encoders, IMUs (inertial measurement), GPS, etc.",wrongExplanations:["Robots typically use multiple sensor types","Touch alone is insufficient for most tasks","GPS alone doesn't provide local information"]},
{chapter:25,difficulty:"medium",question:"LIDAR technology measures:",options:["Light intensity only","Distances using time-of-flight of laser pulses","Sound wave reflections","Temperature gradients"],answerIndex:1,explanation:"LIDAR (Light Detection and Ranging) emits laser pulses and times reflections to build 3D distance maps. Essential for autonomous vehicles.",wrongExplanations:["It measures distance, not just intensity","Sound-based is sonar, not LIDAR","Temperature sensing is different technology"]},
{chapter:25,difficulty:"easy",question:"Actuators in robotics:",options:["Sense the environment","Convert energy into physical motion","Process sensory data","Store robot programs"],answerIndex:1,explanation:"Actuators produce motion: electric motors, hydraulic/pneumatic cylinders, servos. They enable robots to move and manipulate objects.",wrongExplanations:["Sensors do sensing, not actuators","Processing is done by computers","Storage is done by memory systems"]},
{chapter:25,difficulty:"medium",question:"Degrees of freedom (DOF) refers to:",options:["Freedom from physical constraints","The number of independent parameters defining robot configuration","Free movement in space","Degrees of rotation only"],answerIndex:1,explanation:"DOF = number of independent configuration variables. A 6-DOF arm can position and orient its end-effector fully in 3D space.",wrongExplanations:["DOF is about parametrization, not freedom from constraints","DOF includes translation and rotation","Includes all motion types, not just rotation"]},
{chapter:25,difficulty:"hard",question:"Configuration space (C-space) represents:",options:["Memory configuration settings","The space of all possible robot configurations","Workspace volume only","Configuration files for robots"],answerIndex:1,explanation:"C-space has one dimension per DOF. Each point is a complete robot configuration. Path planning operates in C-space, where obstacles become C-space obstacles.",wrongExplanations:["Not about software configuration","C-space is more abstract than physical workspace","Not about configuration files"]},
{chapter:25,difficulty:"easy",question:"Path planning finds:",options:["The shortest internet route","A collision-free path from start to goal configuration","Network paths only","The path of least resistance"],answerIndex:1,explanation:"Path planning computes a trajectory through C-space avoiding obstacles, connecting start configuration to goal. Core robotics problem.",wrongExplanations:["Network routing is different","Path planning is about physical motion","Not metaphorical‚Äîactual geometric paths"]},
{chapter:25,difficulty:"medium",question:"SLAM stands for:",options:["Systematic Learning and Mapping","Simultaneous Localization and Mapping","Slow Linear Actuator Motion","Spatial Location and Measurement"],answerIndex:1,explanation:"SLAM builds a map of unknown environment while simultaneously tracking the robot's location within that map. Chicken-and-egg problem solved iteratively.",wrongExplanations:["It's about localization and mapping","Not about actuators","Not just spatial measurement"]},
{chapter:25,difficulty:"hard",question:"RRT (Rapidly-exploring Random Tree) algorithm:",options:["Grows natural trees rapidly","Efficiently explores high-dimensional C-spaces using random sampling","Removes redundant tree structures","Rapidly rotates robot tools"],answerIndex:1,explanation:"RRT grows a tree by sampling random configurations and extending toward them, efficiently exploring C-space for path planning without discretization.",wrongExplanations:["It's a planning algorithm, not about nature","It builds trees, not removes them","Not about physical rotation"]},
{chapter:25,difficulty:"easy",question:"Localization determines:",options:["The location of files on disk","The robot's position within its environment/map","Local network addresses","Nearby objects only"],answerIndex:1,explanation:"Localization answers 'where am I?' using sensor data matched against a map. Essential for navigation‚Äîrobot must know its location to plan paths.",wrongExplanations:["Not about file systems","Nearby objects are perception, not localization","Not about networking"]},
{chapter:25,difficulty:"medium",question:"Potential field navigation uses:",options:["Electrical potential measurements","Goals as attractive forces, obstacles as repulsive forces","Potential energy calculations only","Political potential of regions"],answerIndex:1,explanation:"Potential fields: goal attracts (negative potential), obstacles repel (positive potential). Robot follows gradient descent. Simple but can have local minima.",wrongExplanations:["Uses virtual potentials, not electrical","Includes both attraction and repulsion in an analogy","Not about actual energy or politics"]},
{chapter:25,difficulty:"easy",question:"Forward kinematics computes:",options:["Forward motion speed","End-effector position given all joint angles","Future trajectories","Forward-looking predictions"],answerIndex:1,explanation:"Forward kinematics: given Œ∏‚ÇÅ,Œ∏‚ÇÇ,...,Œ∏‚Çô (joint angles), compute (x,y,z,rotation) of end-effector. Straightforward geometric calculation.",wrongExplanations:["Not about speed","It's a geometric calculation from joints to position","Not about predictions"]},
{chapter:25,difficulty:"medium",question:"Inverse kinematics computes:",options:["Backward motion","Joint angles needed to achieve a desired end-effector pose","Inverse of all motions","Reversed joint order"],answerIndex:1,explanation:"IK: given desired end-effector pose, find joint angles that achieve it. Harder than FK‚Äîmay have multiple solutions or none. Often iterative.",wrongExplanations:["Not about reversing motion","Computes joint angles, specific problem","Not about joint ordering"]},
{chapter:25,difficulty:"hard",question:"Probabilistic roadmaps (PRM) work by:",options:["Calculating probability distributions","Sampling random valid configurations and connecting collision-free neighbors","Creating road infrastructure maps","Probabilistic road travel times"],answerIndex:1,explanation:"PRM: sample random configurations in free C-space, connect nearby pairs with collision-free paths, then search graph for planning queries.",wrongExplanations:["Not probability distributions themselves","Not about physical road infrastructure","Not about travel time estimation"]},
{chapter:25,difficulty:"easy",question:"Odometry estimates position using:",options:["Odor sensors","Wheel rotation measurements integrated over time","Optical distance sensors only","GPS coordinates exclusively"],answerIndex:1,explanation:"Odometry integrates wheel encoder readings: Œîposition = wheel circumference √ó rotations. Simple but accumulates drift error over time.",wrongExplanations:["'Odo-' relates to distance/travel, not odor","Uses encoders, may include other sensors","GPS is separate from odometry"]},
{chapter:25,difficulty:"medium",question:"Sensor fusion combines:",options:["Multiple sensors physically","Data from multiple sensors for better state estimation","Fusion reactors with sensors","Sensor manufacturing"],answerIndex:1,explanation:"Sensor fusion integrates data from multiple sensors (camera, LIDAR, IMU) using filters like Kalman or particle filters for robust state estimation.",wrongExplanations:["It's about data fusion, not physical combination","Not about fusion reactors","Not about manufacturing"]},
{chapter:25,difficulty:"easy",question:"Mobile robots move using:",options:["Only legs","Wheels, legs, tracks, propellers, or other locomotion methods","Telepathy","Manual carrying only"],answerIndex:1,explanation:"Mobile robots use various locomotion: wheeled (differential, omnidirectional), legged (biped, quadruped), tracked, flying, swimming, etc.",wrongExplanations:["Many locomotion types exist","Obviously not telepathy","Robots move autonomously, not just carried"]},
{chapter:25,difficulty:"hard",question:"The kidnapped robot problem tests:",options:["Robot theft prevention","Ability to recover localization after sudden unknown displacement","Robot kidnapping simulation","Security protocols"],answerIndex:1,explanation:"Kidnapped robot: suddenly move the robot to unknown location. Tests if the localizer can recover, re-localize from scratch. Challenging for many algorithms.",wrongExplanations:["It's about localization recovery","Not a kidnapping simulation","Not about security systems"]},
{chapter:25,difficulty:"medium",question:"A holonomic robot can:",options:["Take holy paths only","Move instantaneously in any direction without reorienting","Only move forward and backward","Hover in place"],answerIndex:1,explanation:"Holonomic robots have controllable velocity in all DOFs. Omnidirectional wheels enable this. Non-holonomic (cars) can't move sideways directly.",wrongExplanations:["Nothing to do with holiness","Non-holonomic robots have directional constraints","Hovering isn't related to holonomy"]},
{chapter:25,difficulty:"easy",question:"An end-effector is:",options:["The effect at the end of computation","The tool or device at the end of a robot arm","Final output of sensors","End-of-program marker"],answerIndex:1,explanation:"End-effector: gripper, welding torch, camera, or tool mounted at the robot arm's end. What the robot uses to interact with objects.",wrongExplanations:["It's physical hardware, not computational effect","Sensors are different from end-effectors","Not a programming concept"]},

// CHAPTER 27: PHILOSOPHY, ETHICS & SAFETY (30 questions)
{chapter:27,difficulty:"easy",question:"AI ethics is concerned with:",options:["Making AI systems run faster","Moral implications and responsible development of AI","AI programming standards only","Ethical treatment of AI by humans"],answerIndex:1,explanation:"AI ethics addresses fairness, accountability, transparency, privacy, safety, and broader societal impacts of AI development and deployment.",wrongExplanations:["Speed is a performance concern","Ethics goes beyond coding standards","It's about AI's effects on humans, and related issues"]},
{chapter:27,difficulty:"easy",question:"Algorithmic bias refers to:",options:["Political preferences of programmers","Systematic unfair outcomes from biased training data or design","Algorithms that work faster for some users","Preferred algorithms in a codebase"],answerIndex:1,explanation:"Algorithmic bias: AI systems producing systematically unfair outcomes, often reflecting biases in training data, features, or design choices.",wrongExplanations:["Programmer politics is one possible source, but bias is in outcomes","Not about performance speed","Not about code preferences"]},
{chapter:27,difficulty:"medium",question:"The alignment problem concerns:",options:["Aligning robot components physically","Ensuring AI systems' objectives align with human values and intentions","Text alignment in documents","Political alignment of AI companies"],answerIndex:1,explanation:"Alignment: making AI systems pursue what we actually want. Misaligned superintelligent AI could be catastrophic. Active research area.",wrongExplanations:["Not about physical alignment","Not about document formatting","Not about company politics"]},
{chapter:27,difficulty:"easy",question:"Explainable AI (XAI) aims to:",options:["Explain AI concepts to beginners","Make AI decision-making understandable to humans","Expand AI capabilities","Export AI models"],answerIndex:1,explanation:"XAI develops techniques to make AI systems' reasoning transparent and interpretable, enabling trust, debugging, and accountability.",wrongExplanations:["Not just education‚Äîit's about system transparency","Explainability, not expansion","Not about export functions"]},
{chapter:27,difficulty:"medium",question:"The trolley problem in AI ethics illustrates:",options:["Problems with train scheduling algorithms","Difficult moral decisions autonomous systems may need to make","How to design trolley controllers","Trolley manufacturing issues"],answerIndex:1,explanation:"The trolley problem (divert and save five, killing one, or do nothing?) illustrates ethical dilemmas facing autonomous vehicles and other AI systems.",wrongExplanations:["It's a thought experiment, not actual scheduling","Not about trolley design","Not about manufacturing"]},
{chapter:27,difficulty:"hard",question:"Superintelligence refers to:",options:["Very intelligent humans","AI that vastly exceeds human cognitive abilities across virtually all domains","Superintendents in schools","Supervised intelligence"],answerIndex:1,explanation:"Superintelligence: hypothetical AI surpassing best human minds in science, creativity, social skills‚Äîall cognitive domains. Raises control concerns.",wrongExplanations:["It's about AI, not humans","Nothing to do with school administration","Not about supervised learning"]},
{chapter:27,difficulty:"easy",question:"Privacy concerns in AI include:",options:["Private AI company operations","Collection, use, and potential misuse of personal data by AI systems","Privacy of AI's own thoughts","Private network protocols"],answerIndex:1,explanation:"AI privacy concerns: mass surveillance, facial recognition, data harvesting, inference of sensitive attributes, consent, data security, etc.",wrongExplanations:["It's about individual privacy, not company privacy","AI doesn't have personal thoughts to protect","Not about network protocols"]},
{chapter:27,difficulty:"medium",question:"Autonomous weapons raise concerns about:",options:["Weapon weight and size","Accountability, escalation risks, and ethics of machine-made lethal decisions","Autonomy in weapon design","Weapons that work alone"],answerIndex:1,explanation:"Autonomous weapons (killer robots) raise issues: who's responsible if they kill wrongly? Could they escalate conflicts? Should machines decide to kill?",wrongExplanations:["Not about physical dimensions","Design autonomy is different","The concern is about lethal decision-making"]},
{chapter:27,difficulty:"hard",question:"The control problem in AI safety asks:",options:["How to control robot movements precisely","How to maintain meaningful human control over increasingly powerful AI systems","Quality control in AI manufacturing","Remote control of drones"],answerIndex:1,explanation:"The control problem: as AI becomes more capable, how do we ensure it remains beneficial and controllable? Especially relevant for potential superintelligence.",wrongExplanations:["Not just about motor control","Not about manufacturing QC","Not about drone operation"]},
{chapter:27,difficulty:"easy",question:"Transparency in AI means:",options:["See-through robot casings","Openness about how AI systems work, are trained, and make decisions","Transparent data formats","Clear display screens"],answerIndex:1,explanation:"AI transparency: disclosing training data, model architecture, decision processes, limitations. Enables accountability and informed use.",wrongExplanations:["Not about physical transparency","Not about file formats","Not about display technology"]},
{chapter:27,difficulty:"medium",question:"AI's impact on employment concerns:",options:["AI job interviews","Potential job displacement and workforce transformation due to automation","AI being employed at companies","Employment of AI researchers"],answerIndex:1,explanation:"AI automation may displace workers in various sectors while creating new roles. Debates over universal basic income, retraining, economic disruption.",wrongExplanations:["Not about how AI conducts interviews","Not about AI 'being employed' as if a person","Researcher employment is a different issue"]},
{chapter:27,difficulty:"easy",question:"Fairness in AI requires:",options:["Equal processing speed for all","Avoiding discrimination and ensuring equitable treatment across groups","Fair weather operation conditions","Fair pricing of AI services"],answerIndex:1,explanation:"AI fairness: systems should not discriminate unfairly based on race, gender, age, etc. Requires careful attention to data, features, and outcomes.",wrongExplanations:["Not about computational equality","Not about weather","Not about pricing"]},
{chapter:27,difficulty:"medium",question:"The Turing Test has been criticized because:",options:["Turing was not a real person","It focuses on deception/imitation rather than genuine intelligence","It is too easy to pass","It was never actually proposed"],answerIndex:1,explanation:"Critics argue the test measures imitation of humans, not true understanding or intelligence. Passing it may require deception, not comprehension.",wrongExplanations:["Turing was very real","It's actually quite hard to truly pass","It was proposed in 1950"]},
{chapter:27,difficulty:"hard",question:"Existential risk from AI refers to:",options:["AI's existential philosophy","Potential threats to humanity's long-term survival from advanced AI","Risk that AI might not exist","Existential dread in AI systems"],answerIndex:1,explanation:"Existential risk: scenarios where superintelligent AI could pose catastrophic or existential threats to humanity if not properly aligned and controlled.",wrongExplanations:["Not about AI doing philosophy","It's about risk TO humanity, not AI's existence","AI doesn't experience dread"]},
{chapter:27,difficulty:"easy",question:"Accountability in AI requires:",options:["Accounting software for AI budgets","Clear responsibility and answerability for AI system outcomes","Making AI count accurately","Account management in AI companies"],answerIndex:1,explanation:"Accountability: someone must be responsible when AI causes harm. Questions of liability, oversight, audit trails, and governance mechanisms.",wrongExplanations:["Not about financial accounting","Not about mathematical accuracy","Not about business accounts"]},
{chapter:27,difficulty:"medium",question:"Beneficial AI research focuses on:",options:["AI that provides financial benefits","Ensuring AI development benefits humanity overall","Beneficial bacteria research","Benefits packages for AI researchers"],answerIndex:1,explanation:"Beneficial AI: research into ensuring AI systems are safe, aligned with human values, and broadly beneficial to humanity rather than harmful.",wrongExplanations:["Not specifically about profit","Not about microbiology","Not about researcher benefits"]},
{chapter:27,difficulty:"hard",question:"Value alignment involves:",options:["Aligning numerical values in spreadsheets","Ensuring AI systems understand and act according to human values","Aligning political values","Value propositions in business"],answerIndex:1,explanation:"Value alignment: making AI systems that correctly understand, learn, and optimize for what humans truly value. Extremely challenging research problem.",wrongExplanations:["Not about spreadsheet formatting","Not about political alignment","Not about business value propositions"]},
{chapter:27,difficulty:"easy",question:"AI regulation involves:",options:["Regulating voltage for AI hardware","Laws and policies governing AI development, deployment, and use","Regulating AI's access to data automatically","Self-regulation by AI systems"],answerIndex:1,explanation:"AI regulation: government laws, industry standards, international agreements governing how AI can be developed and deployed. Rapidly evolving area.",wrongExplanations:["Not about electrical regulation","Not about automatic data access","Not about AI self-regulation"]},
{chapter:27,difficulty:"medium",question:"Dual-use concerns in AI refer to:",options:["AI that can use two tools","Technologies with both beneficial and harmful applications","AI used by two different users","Dual-core processors for AI"],answerIndex:1,explanation:"Dual-use: AI tech can be used beneficially (medical diagnosis) or harmfully (autonomous weapons, surveillance). Creates ethical dilemmas for researchers.",wrongExplanations:["Not about physical tool use","Not about number of users","Not about processor architecture"]},
{chapter:27,difficulty:"hard",question:"An AI safety 'corrigibility' means:",options:["Correcting AI spelling mistakes","An AI's willingness to be corrected, modified, or shut down by humans","Correctness of AI predictions","Curriculum for AI education"],answerIndex:1,explanation:"Corrigibility: AI that allows modification and shutdown without resisting or manipulating to prevent it. Important for maintaining human control.",wrongExplanations:["Not about spelling","Not about prediction accuracy","Not about education curricula"]},
// QUIZ 1 INTRO: MODULES 1-4 (47 questions)
{chapter:100,difficulty:"easy",question:"Which of the following is NOT one of the four AI definition categories?",options:["Think humanly","Act humanly","Learn humanly","Think rationally"],answerIndex:2,explanation:"The four AI categories are: Think Humanly, Act Humanly, Think Rationally, Act Rationally. 'Learn humanly' is not one of them ‚Äî learning falls under machine learning, not a definition category.",wrongExplanations:["Think humanly is one of the four categories (cognitive modeling approach)","Act humanly is one of the four categories (Turing Test approach)","Think rationally is one of the four categories (laws of thought approach)"]},
{chapter:100,difficulty:"easy",question:"What are the two dimensions on which all AI definitions revolve?",options:["Speed and efficiency","Human versus Rational and Thought versus Behavior","Hardware and Software","Agents and systems"],answerIndex:1,explanation:"AI definitions are organized along two dimensions: (1) Human vs. Rational ‚Äî is success measured against human performance or an ideal standard? (2) Thought vs. Behavior ‚Äî do we care about internal reasoning or external actions?",wrongExplanations:["Speed and efficiency are performance metrics, not definitional dimensions","Hardware/software is an implementation distinction, not a definition framework","Agents and systems are AI components, not the definitional dimensions"]},
{chapter:100,difficulty:"medium",question:"Which definition focuses on 'creating machines that perform functions that require intelligence when performed by people'?",options:["Think humanly","Act humanly","Think rationally","Act rationally"],answerIndex:1,explanation:"'Act humanly' focuses on machines performing tasks that would require intelligence if done by humans. This is the Turing Test approach ‚Äî if it acts smart, it is smart.",wrongExplanations:["Think humanly focuses on modeling cognitive processes, not performing functions","Think rationally focuses on logical reasoning systems","Act rationally focuses on doing the mathematically optimal thing, not mimicking humans"]},
{chapter:100,difficulty:"medium",question:"Who proposed the concept that AI should be defined as 'doing the right thing'?",options:["Haugeland","Bellman","Nilsson","Winston"],answerIndex:2,explanation:"Nilsson's 'Acting Rationally' definition sees AI as doing the right thing ‚Äî choosing the action that maximizes goal achievement given available information. This is the rational agent approach.",wrongExplanations:["Haugeland focused on 'thinking humanly' and GOFAI concepts","Bellman defined AI in terms of automation of activities associated with human thinking","Winston focused on models that make computers do things that currently require human intelligence"]},
{chapter:100,difficulty:"easy",question:"An Agent in AI is defined as:",options:["A human who programs computers","Something that acts and can operate autonomously","Only a robotic device","A decision-making algorithm only"],answerIndex:1,explanation:"An agent is anything that perceives its environment through sensors and acts upon it through actuators. Agents can be software (chatbots) or hardware (robots) and operate autonomously.",wrongExplanations:["A human programmer is not an AI agent ‚Äî agents are the systems themselves","Agents can be software programs, not just robots","Agents include perception + action, not just decision-making algorithms"]},
{chapter:100,difficulty:"easy",question:"In 1950, who proposed the Turing Test as a measure of machine intelligence?",options:["John McCarthy","Alan Turing","Marvin Minsky","Claude Shannon"],answerIndex:1,explanation:"Alan Turing proposed the 'Imitation Game' in his 1950 paper 'Computing Machinery and Intelligence.' If a machine can fool a human interrogator, it demonstrates intelligent behavior.",wrongExplanations:["John McCarthy coined 'Artificial Intelligence' in 1956, not the Turing Test","Marvin Minsky co-founded the MIT AI Lab but didn't create the Turing Test","Claude Shannon founded information theory but didn't propose this test"]},
{chapter:100,difficulty:"easy",question:"What is the main question the Turing Test tries to answer?",options:["Can machines be faster than humans?","Can machines think?","Can machines see?","Can machines move?"],answerIndex:1,explanation:"The Turing Test addresses the fundamental question: 'Can machines think?' Turing reframed this as whether a machine can imitate human conversation convincingly enough to fool a judge.",wrongExplanations:["Speed comparison isn't the purpose ‚Äî it's about intelligence, not performance","Vision is tested in the Total Turing Test, not the original","Movement relates to robotics, not the conversational Turing Test"]},
{chapter:100,difficulty:"medium",question:"In the Turing Test, the interrogator must:",options:["See both respondents","Hear both respondents","Guess if a conversation is with a computer or human via typed messages","Physically interact with both respondents"],answerIndex:2,explanation:"The interrogator communicates only through typed messages and must determine which respondent is human and which is machine. No visual or physical contact is allowed to avoid bias.",wrongExplanations:["The interrogator cannot see the respondents ‚Äî that would defeat the purpose","Communication is text-based only, not audio","Physical interaction is part of the Total Turing Test, not the standard one"]},
{chapter:100,difficulty:"medium",question:"For a machine to pass the Turing Test, it needs how many of the specified capabilities?",options:["1-2 capabilities","All 4 required capabilities","At least 3 capabilities","Just one major capability"],answerIndex:1,explanation:"A machine needs all 4 capabilities: (1) Natural Language Processing, (2) Knowledge Representation, (3) Automated Reasoning, and (4) Machine Learning. Missing any one would expose the machine as non-human.",wrongExplanations:["1-2 capabilities would not be sufficient to maintain a convincing conversation","All 4 are required, not just 3 ‚Äî each covers a critical aspect of intelligent behavior","No single capability alone can simulate full human-like conversation"]},
{chapter:100,difficulty:"easy",question:"What does it mean if a machine 'passes' the Turing Test?",options:["It can solve all problems","It fools the interrogator 30% of the time or more","It is smarter than humans","It can learn new languages instantly"],answerIndex:1,explanation:"A machine passes the Turing Test if it fools the interrogator at least 30% of the time ‚Äî this was Turing's original threshold. It doesn't mean the machine is truly intelligent, just convincingly human-like.",wrongExplanations:["Passing doesn't mean solving all problems ‚Äî it means imitating humans well enough","It doesn't prove superior intelligence, just indistinguishable behavior","Language learning ability isn't directly measured in the test"]},
{chapter:100,difficulty:"easy",question:"Which of these is an example of a chatbot that has fooled humans in the Turing Test context?",options:["Deep Blue","ELIZA","Watson","Siri"],answerIndex:1,explanation:"ELIZA (1966) used simple pattern matching to simulate a therapist and famously fooled some users into believing they were talking to a real person, despite having no real understanding.",wrongExplanations:["Deep Blue was a chess program, not a conversational chatbot","Watson was designed for Jeopardy question answering, not Turing Test conversations","Siri is a voice assistant ‚Äî functional but not designed to pass the Turing Test"]},
{chapter:100,difficulty:"medium",question:"The Turing Total Test differs from the standard Turing Test by requiring:",options:["More time for conversation","Interaction with physical objects (video signal and passing objects)","Multiple interrogators","Higher accuracy levels"],answerIndex:1,explanation:"The Total Turing Test extends the original by adding video signals and physical object interaction. This tests computer vision and robotics capabilities beyond just text conversation.",wrongExplanations:["Time duration is not the distinguishing factor","The number of interrogators isn't what differs","Accuracy thresholds aren't what makes it 'total'"]},
{chapter:100,difficulty:"medium",question:"Which capability is needed for the Turing Total Test but NOT for the standard Turing Test?",options:["Natural Language Processing","Machine Learning","Computer Vision","Automated reasoning"],answerIndex:2,explanation:"Computer Vision is required for the Total Turing Test (to interpret video signals and perceive objects) but not for the standard text-only Turing Test. Robotics is also added.",wrongExplanations:["NLP is needed for both tests ‚Äî it's fundamental to text conversation","Machine Learning is needed for both tests to adapt during conversation","Automated reasoning is needed for both tests to form logical responses"]},
{chapter:100,difficulty:"easy",question:"How many main disciplines compose the AI field?",options:["3 disciplines","5 disciplines","7 disciplines","10 disciplines"],answerIndex:1,explanation:"AI is composed of 5 main disciplines: (1) Natural Language Processing, (2) Knowledge Representation & Automated Reasoning, (3) Machine Learning, (4) Computer Vision, and (5) Robotics.",wrongExplanations:["3 is too few ‚Äî AI encompasses more disciplines","7 and 10 overcount ‚Äî the core disciplines recognized are 5","The standard framework identifies exactly 5 core disciplines"]},
{chapter:100,difficulty:"easy",question:"Which of the following is NOT one of the 5 AI disciplines?",options:["Natural Language Processing","Automated Reasoning","Quantum Computing","Machine Learning"],answerIndex:2,explanation:"Quantum Computing is a field of physics/CS, not one of the 5 AI disciplines. The 5 are: NLP, Automated Reasoning, Machine Learning, Computer Vision, and Robotics.",wrongExplanations:["NLP is one of the 5 core AI disciplines","Automated Reasoning is one of the 5 core AI disciplines","Machine Learning is one of the 5 core AI disciplines"]},
{chapter:100,difficulty:"easy",question:"Natural Language Processing (NLP) is primarily used for:",options:["Moving objects","Communicating successfully in English","Perceiving images","Making autonomous decisions"],answerIndex:1,explanation:"NLP enables machines to understand, interpret, and generate human language. It covers tasks like translation, sentiment analysis, text generation, and conversation ‚Äî communicating in natural language.",wrongExplanations:["Moving objects falls under Robotics","Perceiving images falls under Computer Vision","Autonomous decisions relate to Automated Reasoning and planning"]},
{chapter:100,difficulty:"easy",question:"Which discipline is used 'to adapt to new circumstances and detect patterns'?",options:["Computer Vision","Robotics","Machine Learning","Automated Reasoning"],answerIndex:2,explanation:"Machine Learning enables systems to learn from data, adapt to new situations, and detect patterns without being explicitly programmed. It's how AI improves through experience.",wrongExplanations:["Computer Vision is about interpreting visual information","Robotics deals with physical manipulation and movement","Automated Reasoning uses logic rules, not pattern learning from data"]},
{chapter:100,difficulty:"medium",question:"To pass the Turing Total Test, which additional capability beyond the Turing Test is needed?",options:["Only Natural Language Processing","Computer Vision and Robotics","Only Machine Learning","Knowledge representation"],answerIndex:1,explanation:"The Total Turing Test adds Computer Vision (to see and interpret the physical world) and Robotics (to manipulate objects), going beyond the text-only standard Turing Test.",wrongExplanations:["NLP is already required for the standard Turing Test","Machine Learning is already part of the standard test requirements","Knowledge representation is already part of the standard test requirements"]},
{chapter:100,difficulty:"easy",question:"In what year was there 'great expectations and high enthusiasm' for AI?",options:["1943","1950","1965","1980"],answerIndex:1,explanation:"Around 1950, following Turing's paper, there was enormous excitement about AI's potential. This early optimism eventually led to overblown predictions and later disappointment (AI Winters).",wrongExplanations:["1943 saw early neural network concepts but not the peak enthusiasm","1965 was already seeing some disillusionment with early AI limitations","1980 marked the expert systems boom, a different wave of enthusiasm"]},
{chapter:100,difficulty:"easy",question:"Which of the following disciplines has contributed to AI development?",options:["Philosophy and Mathematics","Psychology and Neuroscience","Control Theory and Computer Engineering","All of the above"],answerIndex:3,explanation:"AI is deeply interdisciplinary. Philosophy contributed logic and mind theories, Mathematics gave formal reasoning, Psychology informed cognitive models, Neuroscience inspired neural networks, and Engineering provided the hardware.",wrongExplanations:["Philosophy and Mathematics contributed, but they're not the only ones","Psychology and Neuroscience contributed, but other fields did too","Control Theory and Computer Engineering contributed, but AI draws from all listed fields"]},
{chapter:100,difficulty:"medium",question:"What does Neuroscience contribute to AI?",options:["Programming techniques","Understanding how brains process information","Mathematical algorithms","Military applications"],answerIndex:1,explanation:"Neuroscience reveals how biological neural networks process information, inspiring artificial neural networks and deep learning architectures that mimic brain structures.",wrongExplanations:["Programming techniques come from computer science, not neuroscience","Mathematical algorithms come from mathematics, not neuroscience","Military applications are use cases, not neuroscience contributions"]},
{chapter:100,difficulty:"medium",question:"The 'logicist' tradition in AI hoped to:",options:["Teach computers to think emotionally","Build systems using logical notation and reasoning","Create robots that move","Develop language translation"],answerIndex:1,explanation:"The logicist tradition aimed to encode all knowledge in formal logic and use deductive reasoning to solve problems. Programs like the Logic Theorist and General Problem Solver followed this approach.",wrongExplanations:["Emotional computing is a different field (affective computing)","Robot movement falls under Robotics, not the logicist tradition","Language translation is an NLP application, not the core logicist goal"]},
{chapter:100,difficulty:"hard",question:"What is the main obstacle to the logicist approach in AI?",options:["Computers are too slow","It's difficult to express informal knowledge in formal logical terms","People don't want it","It requires too much electricity"],answerIndex:1,explanation:"The biggest challenge is that real-world knowledge is often uncertain, ambiguous, and contextual ‚Äî extremely hard to capture in rigid formal logic. This is called the qualification problem.",wrongExplanations:["Computer speed has improved dramatically but the fundamental problem remains","Public desire is irrelevant to the technical limitation","Energy consumption is a practical concern, not the core theoretical obstacle"]},
{chapter:100,difficulty:"easy",question:"Which of the following is an example of AI application?",options:["Robotic vehicles","Game playing (like chess or Go)","Machine translation","All of the above"],answerIndex:3,explanation:"All three are real AI applications: self-driving cars use computer vision and planning, game AI uses search and learning, and machine translation uses NLP and deep learning.",wrongExplanations:["Robotic vehicles are one example but not the only one","Game playing is one example but not the only one","Machine translation is one example but not the only one"]},
{chapter:100,difficulty:"easy",question:"Online machine translation systems can now:",options:["Translate only between 5 languages","Translate documents in over 100 languages","Understand all human thoughts","Translate in real-time only"],answerIndex:1,explanation:"Modern systems like Google Translate support over 100 languages using neural machine translation. Quality varies by language pair, but coverage is extensive.",wrongExplanations:["Modern translation far exceeds 5 languages","AI cannot understand all human thoughts ‚Äî translation is a specific NLP task","Translation works both real-time and on documents, and that's not the key point"]},
{chapter:100,difficulty:"medium",question:"Deep learning in recommender systems analyzes:",options:["Only text content","Text, music, videos, history, and metadata","Only user behavior","Only purchase history"],answerIndex:1,explanation:"Deep learning recommenders analyze multiple data types: text reviews, audio/video content features, browsing history, purchase patterns, and metadata to make personalized suggestions.",wrongExplanations:["Modern recommenders go far beyond just text","They combine user behavior with content features, not just behavior alone","Purchase history is one signal among many that deep learning combines"]},
{chapter:100,difficulty:"easy",question:"Companies like Amazon, Netflix, and Walmart use AI for:",options:["Warehouse management only","Recommendations of products/content","Payroll processing","Customer complaints only"],answerIndex:1,explanation:"These companies heavily use AI recommendation systems to suggest products (Amazon), movies/shows (Netflix), and items (Walmart), driving significant portions of their revenue.",wrongExplanations:["AI is used for recommendations primarily, not just warehouse management","Payroll processing is not the primary AI application for these companies","Customer complaints are one area, but recommendations are the flagship AI use"]},
{chapter:100,difficulty:"medium",question:"Spam filtering can be considered as:",options:["A form of recommendation","A form of dis-recommendation","Both A and B","Only a security measure"],answerIndex:2,explanation:"Spam filtering is both recommendation (showing relevant emails) and dis-recommendation (filtering out unwanted ones). It uses ML classification to decide what you should and shouldn't see.",wrongExplanations:["It's partially recommendation but also dis-recommendation","It's partially dis-recommendation but also recommendation","While it has security aspects, its classification nature makes it a recommendation system too"]},
{chapter:100,difficulty:"easy",question:"Which of the following is a major risk of AI development?",options:["Biased decision making","Surveillance and persuasion","Lethal autonomous weapons","All of the above"],answerIndex:3,explanation:"AI poses multiple risks: biased algorithms can discriminate, surveillance tech threatens privacy, persuasion systems manipulate behavior, and autonomous weapons raise ethical and safety concerns.",wrongExplanations:["Biased decision making is one risk but not the only one","Surveillance and persuasion is one risk but not the only one","Lethal autonomous weapons is one risk but not the only one"]},
{chapter:100,difficulty:"medium",question:"Biased decision making in AI can affect which of the following?",options:["Loan applications and employment decisions","Credit card applications and payroll evaluation","Both A and B","Only financial institutions"],answerIndex:2,explanation:"AI bias can affect hiring (resume screening), lending (loan approvals), credit scoring, employee evaluations, and many other high-stakes decisions where biased training data leads to unfair outcomes.",wrongExplanations:["Loan/employment decisions are affected, but not the only ones","Credit/payroll decisions are affected, but not the only ones","Bias affects many sectors beyond just financial institutions"]},
{chapter:100,difficulty:"easy",question:"According to the course, AI risks can arise from:",options:["Misuse of machine learning algorithms","Bias in training data","Poor system design","All of the above"],answerIndex:3,explanation:"AI risks are multifaceted: algorithms can be misused intentionally, training data can embed historical biases, and poor design can create unintended harmful behaviors. All three sources contribute.",wrongExplanations:["Algorithm misuse is one source but not the only one","Training data bias is one source but not the only one","Poor design is one source but not the only one"]},
{chapter:100,difficulty:"easy",question:"Which sectors are mentioned as being affected by AI risks?",options:["Economic and social sectors","Military and financial sectors","Scientific sector","All of the above"],answerIndex:3,explanation:"AI risks span every sector: economic (job displacement), social (bias, privacy), military (autonomous weapons), financial (algorithmic trading), and scientific (reproducibility, ethics).",wrongExplanations:["Economic and social sectors are affected, but not exclusively","Military and financial sectors are affected, but not exclusively","Scientific sector is affected, but not exclusively"]},
{chapter:100,difficulty:"easy",question:"An intelligent agent is best defined as:",options:["A robot that can move","A system that perceives its environment and acts to achieve goals","A computer program only","A device that learns continuously"],answerIndex:1,explanation:"An intelligent agent perceives its environment through sensors and takes actions through actuators to maximize its performance measure. This is the core definition from AIMA.",wrongExplanations:["Not all agents are robots ‚Äî software agents also qualify","Agents are more than just programs ‚Äî they interact with environments","Continuous learning is one capability, but perception + action is the defining feature"]},
{chapter:100,difficulty:"medium",question:"What is the primary difference between an agent and a program?",options:["Agents are faster","Agents are more intelligent","Agents interact with their environment and can perceive and act autonomously","There is no difference"],answerIndex:2,explanation:"The key difference is environment interaction: agents perceive their environment and act autonomously to achieve goals. A regular program just executes instructions without environmental awareness.",wrongExplanations:["Speed is not the distinguishing factor","Intelligence level doesn't define the difference","There is a clear and important difference in environmental interaction"]},
{chapter:100,difficulty:"easy",question:"In agent theory, the 'environment' refers to:",options:["Only the physical surroundings","Everything external to the agent","Only the computer system","The internet only"],answerIndex:1,explanation:"The environment is everything external to the agent that it can perceive and potentially affect. For a software agent this might be a database; for a robot, the physical world.",wrongExplanations:["The environment can be virtual or digital, not just physical","It's broader than just the computer system ‚Äî it includes all external factors","The internet is one possible environment, but the concept is much broader"]},
{chapter:100,difficulty:"easy",question:"In search problems, what is the 'state space'?",options:["The physical space where the agent operates","The set of all possible states the system can be in","Only the starting state","Only the goal state"],answerIndex:1,explanation:"The state space is the complete set of all possible configurations the system can reach. Search algorithms explore this space to find a path from the initial state to a goal state.",wrongExplanations:["State space is abstract, not necessarily physical","It includes ALL possible states, not just the starting one","It includes ALL possible states, not just the goal"]},
{chapter:100,difficulty:"easy",question:"Which of the following is an uninformed search strategy?",options:["Best-first search","A* search","Breadth-first search (BFS)","Greedy search"],answerIndex:2,explanation:"BFS is uninformed (blind) ‚Äî it explores all nodes level by level without using any heuristic or domain knowledge. It only uses the problem structure (start, goal, actions).",wrongExplanations:["Best-first search uses a heuristic to evaluate nodes ‚Äî it's informed","A* uses both path cost and heuristic ‚Äî it's informed","Greedy search uses a heuristic to pick the closest-looking node ‚Äî it's informed"]},
{chapter:100,difficulty:"medium",question:"Depth-first search (DFS) explores:",options:["All nodes at the current depth before going deeper","The deepest path first","Only the shortest path","All paths simultaneously"],answerIndex:1,explanation:"DFS always expands the deepest unexplored node first, going as deep as possible before backtracking. It's memory-efficient but doesn't guarantee finding the shortest path.",wrongExplanations:["Expanding all nodes at current depth first is BFS, not DFS","DFS doesn't guarantee the shortest path ‚Äî it may find a long path first","DFS explores one path at a time, not all simultaneously"]},
{chapter:100,difficulty:"easy",question:"What is the main advantage of Breadth-first search?",options:["It's faster than DFS","It guarantees finding the shortest path","It uses less memory","It's easier to implement"],answerIndex:1,explanation:"BFS explores level by level, so the first solution it finds is guaranteed to have the fewest steps (shortest path for uniform cost). The tradeoff is higher memory usage.",wrongExplanations:["BFS is often slower than DFS due to exploring many nodes","BFS actually uses MORE memory than DFS ‚Äî it stores the entire frontier","Implementation complexity is similar between BFS and DFS"]},
{chapter:100,difficulty:"medium",question:"Uniform Cost Search (UCS) is appropriate when:",options:["All edge costs are equal","Edge costs vary, and we want the minimum cost path","We only care about the number of steps","Memory is unlimited"],answerIndex:1,explanation:"UCS expands the node with the lowest total path cost g(n). It's the right choice when edges have different costs and you want the cheapest path, not just the fewest steps.",wrongExplanations:["When costs are equal, BFS suffices ‚Äî UCS is unnecessary","If you only care about steps, BFS works fine","Memory availability doesn't determine when to use UCS"]},
{chapter:100,difficulty:"easy",question:"What is a heuristic in AI search?",options:["A guarantee to find the optimal solution","An estimate that guides search toward the goal","A random search method","A type of algorithm that learns"],answerIndex:1,explanation:"A heuristic is an educated estimate of how close a state is to the goal. It guides informed search algorithms (like A*) toward promising paths, trading optimality guarantees for speed.",wrongExplanations:["Heuristics don't guarantee optimal solutions ‚Äî they're estimates","Heuristics are the opposite of random ‚Äî they use domain knowledge","Heuristics are fixed functions, not learning algorithms"]},
{chapter:100,difficulty:"medium",question:"Greedy search always:",options:["Finds the optimal solution","Uses the least memory","Selects the path with the best heuristic value at each step","Explores all paths"],answerIndex:2,explanation:"Greedy best-first search always expands the node that appears closest to the goal according to the heuristic h(n). It's fast but not optimal ‚Äî it can get stuck in local minima.",wrongExplanations:["Greedy search does NOT guarantee optimal solutions","Memory usage depends on the problem, not a defining feature","Greedy search does not explore all paths ‚Äî it follows the most promising one"]},
{chapter:100,difficulty:"medium",question:"A* search combines:",options:["The actual path cost and the heuristic estimate","Depth and breadth","Two different heuristics","Random selection and deterministic search"],answerIndex:0,explanation:"A* uses f(n) = g(n) + h(n), combining the actual cost so far g(n) with the heuristic estimate to goal h(n). This balance makes it both complete and optimal with admissible heuristics.",wrongExplanations:["A* doesn't combine depth and breadth search strategies","A* uses one heuristic combined with actual cost, not two heuristics","A* is fully deterministic ‚Äî no random selection involved"]},
{chapter:100,difficulty:"hard",question:"What makes A* search optimal?",options:["It always finds the shortest path","It uses admissible heuristics (never overestimate) and combines actual cost with estimated cost","It's the fastest algorithm","It uses the least memory"],answerIndex:1,explanation:"A* is optimal when its heuristic is admissible (never overestimates). By using f(n) = g(n) + h(n) with this guarantee, it will always find the lowest-cost solution before any suboptimal one.",wrongExplanations:["Shortest path is the result, not what makes it optimal ‚Äî admissibility is the key","A* is not necessarily the fastest ‚Äî its optimality comes from admissible heuristics","A* can use significant memory ‚Äî that's not what makes it optimal"]},
{chapter:100,difficulty:"easy",question:"What is an 'admissible heuristic'?",options:["A heuristic that is easy to compute","A heuristic that never overestimates the actual cost to reach the goal","A heuristic that uses machine learning","A heuristic that is always correct"],answerIndex:1,explanation:"An admissible heuristic always gives an optimistic estimate ‚Äî it never overestimates the true cost to reach the goal. This property is what guarantees A* finds optimal solutions.",wrongExplanations:["Ease of computation doesn't define admissibility","ML-based heuristics exist but that's not what admissible means","Admissible heuristics can underestimate ‚Äî they just can't overestimate"]},
{chapter:100,difficulty:"medium",question:"Which search algorithm guarantees finding the shortest path with minimum cost?",options:["Greedy search","Depth-first search","A* search with an admissible heuristic","Breadth-first search"],answerIndex:2,explanation:"A* with an admissible heuristic guarantees finding the minimum cost path. BFS finds fewest steps but not minimum cost when edges have different weights. DFS and Greedy offer no optimality guarantees.",wrongExplanations:["Greedy search can get trapped by misleading heuristics ‚Äî no optimality guarantee","DFS may find a very long path first ‚Äî no guarantee of shortest","BFS finds fewest steps but not minimum cost when edge weights vary"]},
{chapter:100,difficulty:"easy",question:"The 'features of a heuristic function' include:",options:["It should be easy to compute","It should not overestimate the cost","It should guide the search efficiently","All of the above"],answerIndex:3,explanation:"A good heuristic should be: (1) easy to compute (not slow down search), (2) admissible (not overestimate), and (3) informative enough to guide the search efficiently toward the goal.",wrongExplanations:["Easy computation is one feature but not the only one","Not overestimating (admissibility) is one feature but not the only one","Efficient guidance is one feature but not the only one"]},
// QUIZ 2: MACHINE LEARNING - MODULE 5 (55 questions)
// Topic 5.1 ‚Äì Machine Learning Overview
{chapter:101,difficulty:"easy",question:"What is Machine Learning?",options:["Programming explicit rules for every situation","The ability of a machine to expand its knowledge without human intervention","A type of database management system","Manual data entry automation"],answerIndex:1,explanation:"Machine Learning is the ability of a machine to learn and expand its knowledge automatically from data, without being explicitly programmed for every scenario.",wrongExplanations:["Explicit rule programming is traditional programming, not ML","Database management stores data but doesn't learn from it","Data entry automation is robotic process automation, not ML"]},
{chapter:101,difficulty:"easy",question:"When can we say an agent is learning?",options:["When it runs faster over time","When it improves its performance after making observations about the world","When it uses more memory","When it is connected to the internet"],answerIndex:1,explanation:"An agent is learning when it improves its performance on future tasks after observing and processing data from its environment. Improvement through experience is the key indicator.",wrongExplanations:["Speed improvements can come from hardware upgrades, not learning","Using more memory is a resource issue, not evidence of learning","Internet connectivity enables data access but isn't learning itself"]},
{chapter:101,difficulty:"medium",question:"What are the three steps a computer does in machine learning?",options:["Input ‚Üí Process ‚Üí Output","Observe data ‚Üí Build a model ‚Üí Use model as hypothesis and problem-solving software","Download ‚Üí Install ‚Üí Run","Collect ‚Üí Store ‚Üí Delete"],answerIndex:1,explanation:"In ML, a computer: (1) Observes data, (2) Builds a model based on that data, (3) Uses the model as both a hypothesis about the world and software that solves problems.",wrongExplanations:["Input-Process-Output is general computing, not specifically ML","Download-Install-Run describes software installation","Collect-Store-Delete is data lifecycle management, not learning"]},
{chapter:101,difficulty:"easy",question:"Why is machine learning required?",options:["Because computers are bored","Because designers cannot anticipate all future situations and changes over time","Because it's trendy in the industry","Because manual programming is illegal"],answerIndex:1,explanation:"ML is needed because: (1) designers can't anticipate all possible future situations, and (2) designers can't predict all changes over time. ML lets systems adapt automatically.",wrongExplanations:["Computers don't experience boredom ‚Äî this isn't a valid reason","Trendiness isn't a technical requirement","Manual programming is perfectly legal ‚Äî ML solves practical limitations"]},
{chapter:101,difficulty:"easy",question:"What are the three main types of learning?",options:["Fast, medium, and slow learning","Supervised, unsupervised, and reinforcement learning","Visual, auditory, and kinesthetic learning","Online, offline, and hybrid learning"],answerIndex:1,explanation:"The three main ML paradigms are: Supervised (labeled data), Unsupervised (no labels, find patterns), and Reinforcement (learn from rewards/punishments).",wrongExplanations:["Speed categories aren't ML learning types","These are human learning styles, not ML paradigms","Online/offline refer to training modes, not the three main types"]},
{chapter:101,difficulty:"easy",question:"In supervised learning, what does the agent observe?",options:["Random noise from sensors","Input-output pairs where the environment acts as teacher","Only the output without any input","Unlabeled data clusters"],answerIndex:1,explanation:"In supervised learning, the agent sees input-output pairs (x, y) and learns a function mapping inputs to outputs. The labeled examples serve as the 'teacher' guiding the learning.",wrongExplanations:["Random noise would prevent learning ‚Äî supervised learning needs structured pairs","Without inputs, the agent can't learn the input-output mapping","Unlabeled data is used in unsupervised learning, not supervised"]},
{chapter:101,difficulty:"easy",question:"What is a 'label' in supervised learning?",options:["A physical sticker on the computer","The output associated with an input example","The name of the algorithm","The brand of the dataset"],answerIndex:1,explanation:"A label is the correct output/answer associated with an input. For example, an image of a bus has the label 'bus'. Labels are what the model learns to predict.",wrongExplanations:["Labels are data annotations, not physical stickers","Algorithm names are separate from data labels","Dataset brands don't exist ‚Äî labels are the target values"]},
{chapter:101,difficulty:"easy",question:"What is the most common unsupervised learning task?",options:["Classification","Regression","Clustering","Labeling"],answerIndex:2,explanation:"Clustering is the most common unsupervised task ‚Äî it groups similar data points together without predefined labels, discovering natural patterns in the data.",wrongExplanations:["Classification requires labels ‚Äî it's supervised learning","Regression predicts continuous values with labels ‚Äî it's supervised","Labeling is what humans do to create supervised datasets"]},
{chapter:101,difficulty:"easy",question:"In unsupervised learning, what kind of feedback is supplied?",options:["Constant positive feedback","No explicit feedback ‚Äî the agent learns patterns on its own","Negative feedback only","Feedback from a human teacher"],answerIndex:1,explanation:"Unsupervised learning has no explicit feedback or labels. The agent must discover patterns, structures, and relationships in the data entirely on its own.",wrongExplanations:["There is no feedback at all in unsupervised learning","Negative-only feedback would be a form of supervision","Human teacher feedback defines supervised learning"]},
{chapter:101,difficulty:"easy",question:"How does reinforcement learning work?",options:["By memorizing all possible answers","By learning from rewards and punishments through experience","By copying other agents exactly","By reading instruction manuals"],answerIndex:1,explanation:"In RL, an agent learns by interacting with its environment and receiving reinforcements ‚Äî rewards for good actions and punishments for bad ones. It learns to maximize cumulative reward.",wrongExplanations:["Memorizing answers is lookup, not learning from experience","Copying other agents is imitation learning, different from RL","Reading manuals would be knowledge-based systems, not RL"]},
{chapter:101,difficulty:"medium",question:"Which of the following are reinforcement learning algorithms?",options:["Linear regression and logistic regression","Q-learning, Deep Q-Network (DQN), and Model-based RL","K-means and hierarchical clustering","Decision trees and random forests"],answerIndex:1,explanation:"Q-learning learns action values, DQN uses deep neural networks for Q-learning, and Model-based RL builds an environment model. All three are core RL algorithms.",wrongExplanations:["Linear and logistic regression are supervised learning methods","K-means and hierarchical clustering are unsupervised methods","Decision trees and random forests are supervised classification/regression methods"]},
{chapter:101,difficulty:"easy",question:"What is the difference between classification and regression?",options:["Classification is faster than regression","Classification predicts discrete categories; regression predicts continuous values","Regression uses more data than classification","There is no difference"],answerIndex:1,explanation:"Classification predicts categorical/discrete outputs (e.g., sunny/cloudy/rainy). Regression predicts continuous numerical outputs (e.g., temperature = 33.5). The output type defines the difference.",wrongExplanations:["Speed doesn't define the difference ‚Äî it's about output type","Data quantity doesn't distinguish them","They are fundamentally different in their output types"]},
// Topic 5.2 ‚Äì Supervised Learning
{chapter:101,difficulty:"easy",question:"What is the main concept behind supervised learning?",options:["Hard-coding every possible rule manually","Letting the machine learn from experiences and examples","Running simulations without any data","Randomly guessing outputs"],answerIndex:1,explanation:"Supervised learning replaces manual rule-coding by letting machines learn patterns from labeled examples. The data teaches the model, not the programmer writing explicit rules.",wrongExplanations:["Hard-coding rules is traditional programming, the opposite of ML","You need actual training data ‚Äî simulations without data aren't supervised learning","Random guessing has no learning component"]},
{chapter:101,difficulty:"medium",question:"In the formal definition of supervised learning, what is the function h?",options:["The hardware specification","A hypothesis that approximates the true function f","The hashing function for data storage","The help function in the program"],answerIndex:1,explanation:"Given training pairs generated by unknown function y = f(x), supervised learning finds hypothesis h from hypothesis space H that best approximates f. The goal is h ‚âà f.",wrongExplanations:["h refers to the learned hypothesis, not hardware","Hashing is for data structures, not supervised learning","It's a mathematical function, not a program help utility"]},
{chapter:101,difficulty:"medium",question:"What are the 4 main steps of supervised learning?",options:["Download, install, configure, run","Split data ‚Üí Train model ‚Üí Test on unseen data ‚Üí Compare predictions with actual labels","Observe, hypothesize, experiment, conclude","Input, process, store, output"],answerIndex:1,explanation:"The 4 steps: (1) Split data into training/testing sets, (2) Train model parameters on training data, (3) Run model on test data (without labels), (4) Compare predictions to actual test labels.",wrongExplanations:["Software installation steps, not ML workflow","Scientific method steps ‚Äî related but not the specific ML procedure","General computing steps, not supervised learning process"]},
{chapter:101,difficulty:"medium",question:"What is Bias in model validation?",options:["Unfair treatment of data","A model's assumptions about data ‚Äî high bias means oversimplification and underfitting","A type of data corruption","The model's favorite features"],answerIndex:1,explanation:"Bias refers to the assumptions a model makes. High bias means the model is too simple and oversimplifies patterns, leading to underfitting ‚Äî it misses important relationships in the data.",wrongExplanations:["While algorithmic bias exists, 'bias' in model validation specifically means simplifying assumptions","Bias isn't data corruption ‚Äî it's a model property","Features aren't 'favorites' ‚Äî bias is about model complexity"]},
{chapter:101,difficulty:"medium",question:"What is Variance in model validation?",options:["The average of all predictions","The amount of change in the hypothesis due to fluctuation in training data","The total number of features","The speed of model training"],answerIndex:1,explanation:"Variance measures how much the learned model changes when trained on different subsets of data. High variance means the model is overly sensitive to training data specifics ‚Äî it overfits.",wrongExplanations:["Average predictions relate to bias, not variance","Number of features is dimensionality, not variance","Training speed is computational efficiency, not variance"]},
{chapter:101,difficulty:"medium",question:"What is overfitting?",options:["When the model is too simple to capture patterns","When the model fits training data too closely, including noise, resulting in high variance","When the model runs out of memory","When the model takes too long to train"],answerIndex:1,explanation:"Overfitting occurs when a model learns the noise and random fluctuations in training data rather than the underlying pattern. It performs well on training data but poorly on new data.",wrongExplanations:["A model that's too simple is underfitting, not overfitting","Memory issues are computational, not related to overfitting","Training time doesn't define overfitting"]},
{chapter:101,difficulty:"hard",question:"What is the Bias-Variance trade-off?",options:["Choosing between fast and slow models","Balancing between a model too simple (high bias) and too complex (high variance) to optimally fit future examples","Trading data quality for quantity","Exchanging accuracy for speed"],answerIndex:1,explanation:"The trade-off: simple models underfit (high bias, low variance), complex models overfit (low bias, high variance). The goal is the sweet spot that generalizes best to unseen data.",wrongExplanations:["Speed isn't what's being traded ‚Äî it's model complexity","Data quality vs quantity is a different concern","Accuracy vs speed is a computational trade-off, not bias-variance"]},
{chapter:101,difficulty:"medium",question:"What does the loss function measure in ML?",options:["How much money the model costs","The difference between predicted and actual values","The number of training epochs","The size of the dataset"],answerIndex:1,explanation:"The loss function L measures how far off predictions are from actual values. Minimizing the loss function is how models learn ‚Äî smaller loss means better predictions.",wrongExplanations:["Financial cost is unrelated to the mathematical loss function","Training epochs are iterations, not what loss measures","Dataset size doesn't define loss"]},
{chapter:101,difficulty:"hard",question:"Which of the following correctly describes the three types of loss functions?",options:["Small, medium, and large loss","L1 = |y - ≈∑| (absolute), L2 = (y - ≈∑)¬≤ (squared), L0/1 = 0 if correct else 1","Positive, negative, and neutral loss","Input loss, output loss, and hidden loss"],answerIndex:1,explanation:"L1 (absolute loss) = |y - ≈∑|, L2 (squared loss) = (y - ≈∑)¬≤, and L0/1 (zero-one loss) = 0 if prediction matches actual, 1 otherwise. Each is used for different scenarios.",wrongExplanations:["Loss functions aren't categorized by size","Loss values can be positive but aren't categorized this way","There's no concept of 'hidden loss' ‚Äî these are standard mathematical formulas"]},
// Topic 5.3 ‚Äì Feature Engineering
{chapter:101,difficulty:"easy",question:"What percentage of time in ML projects is typically spent on building the feature space?",options:["10-20%","30-40%","70-80%","95-100%"],answerIndex:2,explanation:"70-80% of ML project time is spent on feature engineering ‚Äî preparing, cleaning, transforming, and selecting features. Model building is actually the smaller portion of the work.",wrongExplanations:["10-20% would be ideal but feature engineering takes much more time","30-40% underestimates the effort ‚Äî data preparation dominates","95-100% would leave no time for actual model building"]},
{chapter:101,difficulty:"medium",question:"What does building the feature space involve?",options:["Only collecting raw data","Data transformation, standardization, handling missing values, correlation analysis, and feature selection","Just running the ML algorithm","Only visualizing the data"],answerIndex:1,explanation:"Feature engineering includes: (1) data transformation & modeling, (2) standardization/normalization, (3) handling missing values, (4) correlation analysis, and (5) feature selection.",wrongExplanations:["Raw data collection is just the start ‚Äî extensive processing follows","Running the algorithm comes after feature preparation","Visualization helps but doesn't cover the full feature engineering pipeline"]},
{chapter:101,difficulty:"medium",question:"What are entropy and information gain used for in ML?",options:["Measuring computer temperature","Selecting the most relevant features/attributes for the model","Calculating electricity usage","Measuring internet speed"],answerIndex:1,explanation:"Entropy measures disorder/uncertainty in data. Information gain measures how much a feature reduces that uncertainty. Together they help select the most informative features for splitting decisions.",wrongExplanations:["Entropy in ML is an information theory concept, not about temperature","These are mathematical measures, not related to electricity","Internet speed is unrelated to these information theory concepts"]},
// Topic 5.4 ‚Äì Statistics Review
{chapter:101,difficulty:"medium",question:"What three fundamental areas does AI formalization require?",options:["Art, music, and literature","Logic, computation, and probability","Speed, memory, and storage","Input, processing, and output"],answerIndex:1,explanation:"AI requires: Logic (formal reasoning), Computation (algorithmic processing), and Probability (handling uncertainty). These three pillars form the mathematical foundation of AI.",wrongExplanations:["Arts aren't part of AI's mathematical foundation","Hardware specs don't define AI formalization","Input-processing-output is computing architecture, not AI foundations"]},
{chapter:101,difficulty:"medium",question:"What is a Probability Density Function (PDF)?",options:["A file format for documents","A function giving the probability of a random variable attaining a particular value","A type of neural network","A programming language"],answerIndex:1,explanation:"A PDF describes the likelihood of a random variable taking on each possible value. It counts how often each value occurs, giving the probability distribution of the variable.",wrongExplanations:["PDF documents are unrelated to probability density functions","Neural networks are ML models, not statistical functions","PDFs are mathematical functions, not programming languages"]},
{chapter:101,difficulty:"medium",question:"What is a Cumulative Density Function (CDF)?",options:["The total number of data points","The probability that a random variable is less than or equal to a given value","The maximum value in a dataset","The average of all probabilities"],answerIndex:1,explanation:"The CDF gives P(X ‚â§ x) ‚Äî the total probability that the random variable takes a value less than or equal to x. It accumulates probability from left to right, always ending at 1.",wrongExplanations:["Data point count is sample size, not CDF","CDF gives cumulative probability, not the maximum value","It's cumulative probability up to a point, not an average"]},
{chapter:101,difficulty:"easy",question:"Which of the following is a type of probability distribution?",options:["Linear distribution","Normal (Bell Curve) distribution","Parallel distribution","Sequential distribution"],answerIndex:1,explanation:"The Normal (Gaussian/Bell Curve) distribution is one of the most important probability distributions. Others include Uniform, Poisson, Binomial, and Multinomial.",wrongExplanations:["'Linear distribution' is not a standard probability distribution","'Parallel distribution' isn't a probability distribution type","'Sequential distribution' isn't a recognized probability distribution"]},
{chapter:101,difficulty:"easy",question:"What is Correlation?",options:["When two computers are connected","Finding a relationship between two variables","A type of programming error","The speed of data transfer"],answerIndex:1,explanation:"Correlation measures the relationship between two variables. If they change together (one goes up, the other goes up or down), they are correlated. It suggests dependence between variables.",wrongExplanations:["Computer connections are networking, not statistical correlation","Programming errors are bugs, not correlation","Data transfer speed is bandwidth, unrelated to correlation"]},
{chapter:101,difficulty:"easy",question:"What is the range of the correlation coefficient?",options:["0 to 100","‚àí1 to 1","-10 to 10","0 to 1"],answerIndex:1,explanation:"The correlation coefficient ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation). Zero means no correlation. Values beyond ¬±0.5 indicate notable correlation.",wrongExplanations:["0 to 100 is a percentage range, not correlation","The range is exactly -1 to 1, not -10 to 10","Negative correlations exist, so 0 to 1 is incomplete"]},
{chapter:101,difficulty:"easy",question:"What does a positive correlation coefficient mean?",options:["The variables are unrelated","There is a direct relationship ‚Äî both increase or decrease together","One variable disappears","The data is corrupted"],answerIndex:1,explanation:"Positive correlation means a direct relationship: when one variable increases, the other also increases; when one decreases, the other also decreases. They move in the same direction.",wrongExplanations:["Unrelated variables have correlation near 0, not positive","Variables don't disappear ‚Äî they move in the same direction","Positive correlation is a valid statistical finding, not data corruption"]},
{chapter:101,difficulty:"easy",question:"What does a negative correlation coefficient mean?",options:["The correlation is invalid","There is an inverse relationship ‚Äî one increases while the other decreases","Both variables are negative numbers","The measurement is wrong"],answerIndex:1,explanation:"Negative correlation means an inverse relationship: when one variable increases, the other decreases, and vice versa. For example, more exercise often correlates with lower weight.",wrongExplanations:["Negative correlation is a valid, meaningful finding","The sign refers to direction of relationship, not the values themselves","Negative correlation is a real statistical pattern, not a measurement error"]},
{chapter:101,difficulty:"medium",question:"What is random sampling?",options:["Randomly deleting data","Taking groups of individuals, calculating means repeatedly to estimate population parameters","Choosing only the best data points","Sampling data at fixed time intervals"],answerIndex:1,explanation:"Random sampling involves taking random groups (e.g., 1000 people), calculating their mean, and repeating many times. This estimates the population parameter through the distribution of sample means.",wrongExplanations:["Random sampling collects data, it doesn't delete it","Cherry-picking best points is selection bias, not random sampling","Fixed-interval sampling is systematic, not random"]},
{chapter:101,difficulty:"easy",question:"What is the null hypothesis (H‚ÇÄ)?",options:["A hypothesis that nothing exists","The hypothesis that already exists or is assumed to be true","A hypothesis about zero values","The strongest hypothesis"],answerIndex:1,explanation:"H‚ÇÄ is the default assumption ‚Äî what we currently believe to be true. Statistical tests try to find evidence against it. We only reject H‚ÇÄ if the evidence (p-value) is strong enough.",wrongExplanations:["It's not about existence ‚Äî it's the default assumption to test against","'Null' doesn't mean zero values ‚Äî it means the status quo assumption","Strength isn't what defines it ‚Äî it's the existing/default hypothesis"]},
{chapter:101,difficulty:"easy",question:"What is the alternate hypothesis (H‚Çê)?",options:["A backup computer system","The hypothesis someone is proposing as different from the null","An alternative dataset","A second null hypothesis"],answerIndex:1,explanation:"H‚Çê is the new claim being tested ‚Äî what the researcher proposes is true instead of H‚ÇÄ. If evidence rejects H‚ÇÄ, we accept H‚Çê. It represents the 'something is different' claim.",wrongExplanations:["It's a statistical concept, not a computer system","It's a proposed explanation, not a dataset","There can only be one null hypothesis ‚Äî H‚Çê is fundamentally different from H‚ÇÄ"]},
{chapter:101,difficulty:"medium",question:"What is a P-value?",options:["The price of the analysis","The probability of finding the observed results when the null hypothesis is true","The percentage of data used","The power of the computer"],answerIndex:1,explanation:"The p-value is the probability of getting results as extreme as observed, assuming H‚ÇÄ is true. A small p-value (< 0.05) means the results are unlikely under H‚ÇÄ, so we reject it.",wrongExplanations:["P-value is a probability measure, not a financial cost","It's a probability, not a percentage of data used","Computing power is unrelated to p-values"]},
{chapter:101,difficulty:"medium",question:"When do we reject the null hypothesis?",options:["When we feel like it","When the p-value is less than the significance level (e.g., p < 0.05)","When the sample size is large","When the data looks interesting"],answerIndex:1,explanation:"We reject H‚ÇÄ when p-value < significance level (commonly 0.05). This means there's less than a 5% chance of seeing these results if H‚ÇÄ were true ‚Äî strong evidence against it.",wrongExplanations:["Hypothesis rejection must be based on statistical evidence, not feelings","Large sample size alone doesn't determine rejection","'Interesting' is subjective ‚Äî we need statistical thresholds"]},
{chapter:101,difficulty:"hard",question:"What is the difference between a T-test and a Z-test?",options:["T-test is faster, Z-test is slower","T-test: Student-t distribution, variance unknown, n < 30. Z-test: Normal distribution, variance known, n > 30","T-test uses text data, Z-test uses numbers","They are exactly the same"],answerIndex:1,explanation:"T-test uses Student-t distribution when population variance is unknown and sample size < 30. Z-test uses Normal distribution when population variance is known and sample size > 30.",wrongExplanations:["Speed doesn't differentiate them ‚Äî it's about distribution assumptions","Both tests use numerical data, not text","They have different assumptions about variance and sample size"]},
{chapter:101,difficulty:"hard",question:"The Z-test formula Z = (Am ‚àí A‚ÇÄ) / (œÉ / ‚àön) ‚Äî what does œÉ represent?",options:["Sample mean","Population standard deviation","Sample size","Significance level"],answerIndex:1,explanation:"In the Z-test formula, œÉ is the population standard deviation. Am = sample mean, A‚ÇÄ = population mean, n = sample size. The formula calculates how many standard errors the sample mean is from the population mean.",wrongExplanations:["Am represents the sample mean, not œÉ","n represents the sample size, not œÉ","The significance level is typically denoted Œ≤ or Œ±, not œÉ"]},
{chapter:101,difficulty:"medium",question:"What is the Chi-square test used for in ML?",options:["Testing computer speed","Showing causal relationships, checking data fairness, and detecting data that's too good to be true","Measuring file sizes","Testing internet bandwidth"],answerIndex:1,explanation:"Chi-square tests: (1) show independence/dependence between input and output variables, (2) check if data comes from a fair/unbiased source, (3) detect suspiciously perfect data.",wrongExplanations:["Chi-square is a statistical test, not a performance benchmark","File sizes are measured in bytes, not with chi-square","Bandwidth testing is networking, not statistics"]},
{chapter:101,difficulty:"hard",question:"The chi-square formula is œá¬≤ = Œ£(O ‚àí E)¬≤ / E. What do O and E represent?",options:["Old and Estimated values","Observed and Expected values","Original and Extra values","Output and Error values"],answerIndex:1,explanation:"O = Observed value (what actually happened in the data) and E = Expected value (what we'd expect under the null hypothesis). The formula measures how much observed data deviates from expected.",wrongExplanations:["O stands for Observed, not Old","O stands for Observed and E for Expected, not Original and Extra","While related to outputs and errors, the correct terms are Observed and Expected"]},
// Topic 5.5 ‚Äì Linear Regression
{chapter:101,difficulty:"easy",question:"What is the uni-variate linear model equation?",options:["y = mx¬≤ + c","y = w‚ÇÅx + w‚ÇÄ","y = e^x","y = log(x)"],answerIndex:1,explanation:"The uni-variate (single variable) linear model is y = w‚ÇÅx + w‚ÇÄ, where w‚ÇÅ is the slope (weight) and w‚ÇÄ is the y-intercept (bias). It models a straight-line relationship.",wrongExplanations:["y = mx¬≤ + c is a quadratic equation, not linear","y = e^x is exponential, not linear","y = log(x) is logarithmic, not linear"]},
{chapter:101,difficulty:"medium",question:"What loss function is used in linear regression?",options:["L0/1 (zero-one) loss","L2 (squared) loss: Œ£(y‚±º ‚àí (w‚ÇÅx‚±º + w‚ÇÄ))¬≤","Hinge loss","Cross-entropy loss"],answerIndex:1,explanation:"Linear regression uses L2 (squared) loss, summing the squared differences between actual values y‚±º and predictions (w‚ÇÅx‚±º + w‚ÇÄ). Squaring penalizes large errors more heavily.",wrongExplanations:["Zero-one loss is for classification, not regression","Hinge loss is used in SVMs, not standard linear regression","Cross-entropy is for classification probability, not regression"]},
{chapter:101,difficulty:"medium",question:"What is the Ordinary Least Squares (OLS) method?",options:["Finding the most expensive line","Finding the line that best fits data by minimizing the sum of squared residuals","Drawing a random line through data","Maximizing the distance between points and line"],answerIndex:1,explanation:"OLS finds the best-fit line by minimizing the sum of squared differences (residuals) between actual data points and the predicted line. This gives the optimal w‚ÇÄ and w‚ÇÅ.",wrongExplanations:["OLS minimizes error, it doesn't consider cost","Random lines don't optimize anything ‚Äî OLS is a precise optimization","OLS minimizes distance, not maximizes it"]},
{chapter:101,difficulty:"medium",question:"What shape does the loss function form when plotted in 3D with w‚ÇÄ and w‚ÇÅ?",options:["A flat plane","A convex (bowl) shape","A random scatter","A cube"],answerIndex:1,explanation:"The L2 loss function forms a convex bowl shape in 3D. The goal is to reach the bottom of this bowl ‚Äî the global minimum ‚Äî where the loss is lowest and w‚ÇÄ, w‚ÇÅ are optimal.",wrongExplanations:["A flat plane would mean all parameter values give equal loss","The shape is structured and smooth, not random","Loss functions create smooth surfaces, not geometric shapes like cubes"]},
{chapter:101,difficulty:"hard",question:"How do we find the optimal w‚ÇÄ and w‚ÇÅ in linear regression?",options:["By random guessing","By setting partial derivatives ‚àÇLoss/‚àÇw‚ÇÄ = 0 and ‚àÇLoss/‚àÇw‚ÇÅ = 0 and solving","By using the largest possible values","By copying from another model"],answerIndex:1,explanation:"We find the minimum of the convex loss function by setting its partial derivatives with respect to w‚ÇÄ and w‚ÇÅ to zero, then solving these equations using the chain rule.",wrongExplanations:["Random guessing is inefficient and doesn't guarantee optimal values","Large values would likely increase loss, not minimize it","Each dataset needs its own optimal parameters"]},
{chapter:101,difficulty:"medium",question:"What is SST (Total Sum of Squares)?",options:["The total number of squares in the dataset","SST = Œ£(y_actual ‚àí y_average)¬≤, representing total variability with no model","The sum of all predictions","The total storage size"],answerIndex:1,explanation:"SST measures the total variability in the data by summing squared differences between each actual value and the overall mean. It represents how spread out the data is with no model.",wrongExplanations:["SST is a statistical measure, not a count of squares","SST measures variability in actual values, not predictions","Storage size is a computing concept, not statistics"]},
{chapter:101,difficulty:"medium",question:"What are the two components of SST?",options:["Mean and median","SSR (Regression Sum of Squares) and SSE (Error Sum of Squares)","Input and output","Bias and variance"],answerIndex:1,explanation:"SST = SSR + SSE. SSR is the variability explained by the model (regression). SSE is the unexplained variability (residual error). A good model has high SSR and low SSE.",wrongExplanations:["Mean and median are measures of central tendency, not components of SST","Input and output describe data, not SST decomposition","Bias and variance are model properties, not SST components"]},
{chapter:101,difficulty:"medium",question:"What is R¬≤ (R-squared)?",options:["The square root of R","R¬≤ = 1 ‚àí (SSE/SST), measuring how well the model explains variability","The number of rows squared","The regression coefficient times 2"],answerIndex:1,explanation:"R¬≤ measures the proportion of total variability explained by the model. R¬≤ = 1 means perfect fit, R¬≤ = 0 means the model explains nothing. Higher R¬≤ = better model.",wrongExplanations:["R¬≤ is a ratio, not a square root operation","It's a statistical measure, not related to data dimensions","R¬≤ is a goodness-of-fit measure, not a simple multiplication"]},
{chapter:101,difficulty:"medium",question:"What is the null hypothesis in linear regression?",options:["The data doesn't exist","H‚ÇÄ: w‚ÇÅ = 0, meaning no relationship between x and y","The model is always correct","All predictions are zero"],answerIndex:1,explanation:"H‚ÇÄ: w‚ÇÅ = 0 means the slope is zero ‚Äî there's no linear relationship between x and y. If we reject H‚ÇÄ, it means there IS a significant relationship worth modeling.",wrongExplanations:["The null hypothesis is about the slope, not data existence","No model is always correct ‚Äî H‚ÇÄ tests whether a relationship exists","H‚ÇÄ is about the slope being zero, not predictions being zero"]},
{chapter:101,difficulty:"medium",question:"What is the alternate hypothesis in linear regression?",options:["H‚Çê: w‚ÇÅ = 0","H‚Çê: w‚ÇÅ ‚â† 0, meaning there IS a relationship between x and y","H‚Çê: the model is broken","H‚Çê: all data points are identical"],answerIndex:1,explanation:"H‚Çê: w‚ÇÅ ‚â† 0 proposes that the slope is not zero ‚Äî there IS a meaningful linear relationship between x and y. If evidence supports this, the model is statistically justified.",wrongExplanations:["w‚ÇÅ = 0 is the null hypothesis, not the alternate","The alternate hypothesis is about the relationship, not model functionality","Identical data points would mean zero variance, not the alternate hypothesis"]},
{chapter:101,difficulty:"medium",question:"What is multivariate linear regression?",options:["Running linear regression multiple times","Extending linear regression to multiple input variables: h(x) = w‚ÇÄ + w‚ÇÅx‚ÇÅ + ... + w‚Çôx‚Çô","Using multiple datasets simultaneously","Linear regression with multiple outputs"],answerIndex:1,explanation:"Multivariate linear regression extends the model to multiple input features: the prediction is a weighted sum of all input variables plus a bias term (w‚ÇÄ).",wrongExplanations:["It's about multiple features in one model, not running the same model repeatedly","It uses one dataset with multiple features, not multiple datasets","Standard multivariate regression still predicts one output variable"]},
{chapter:101,difficulty:"hard",question:"What is a major risk with multivariate linear regression?",options:["It runs too slowly","Overfitting ‚Äî irrelevant dimensions may appear useful by chance in high-dimensional spaces","It can only handle 2 variables","It always produces negative results"],answerIndex:1,explanation:"In high dimensions, some features may randomly correlate with the output, causing the model to overfit. The 'curse of dimensionality' means more features can paradoxically hurt performance.",wrongExplanations:["Speed can be an issue but overfitting is the primary statistical risk","Multivariate regression handles many variables ‚Äî that's its purpose","Results can be positive or negative ‚Äî the sign isn't the risk"]},
{chapter:101,difficulty:"easy",question:"What does 'hypothesis space H' refer to in supervised learning?",options:["The physical space where experiments happen","The set of all possible functions the model can learn","A type of computer memory","The space between data points"],answerIndex:1,explanation:"Hypothesis space H is the set of all possible functions/models that the learning algorithm can choose from. The algorithm searches H to find the hypothesis h that best approximates the true function f.",wrongExplanations:["It's a mathematical concept, not a physical space","Hypothesis space is about function possibilities, not computer memory","It's about possible models, not spatial distances between data"]},
{chapter:101,difficulty:"easy",question:"In supervised learning, what role does the training set play?",options:["It tests the final model","It provides labeled examples for the model to learn from","It stores the model's predictions","It evaluates the model's speed"],answerIndex:1,explanation:"The training set provides the labeled input-output pairs that the model learns from. The model adjusts its parameters to minimize error on these examples. Testing uses separate data.",wrongExplanations:["Testing is done with the test set, not the training set","The training set contains actual data, not predictions","Speed evaluation is a computational concern, not the training set's role"]},
{chapter:101,difficulty:"medium",question:"What happens when a model has high bias?",options:["It overfits the training data","It underfits ‚Äî it's too simple and misses important patterns","It runs out of memory","It produces random outputs"],answerIndex:1,explanation:"High bias means the model makes strong simplifying assumptions, causing it to underfit. It misses real patterns in the data because it can't capture the complexity of the true relationship.",wrongExplanations:["Overfitting comes from high variance, not high bias","Memory issues are computational, not related to bias","High bias produces consistently wrong (not random) predictions"]},
{chapter:101,difficulty:"medium",question:"What happens when a model has high variance?",options:["It underfits the data","It overfits ‚Äî it's too sensitive to training data fluctuations","It always predicts the same value","It ignores all features"],answerIndex:1,explanation:"High variance means the model changes dramatically with different training data. It memorizes noise and specific patterns in training data, performing poorly on new unseen data.",wrongExplanations:["Underfitting comes from high bias, not high variance","A model with high variance gives very different predictions across datasets","High variance models use features too aggressively, not ignore them"]},
{chapter:101,difficulty:"easy",question:"Which of the following is a supervised learning task?",options:["Grouping customers by purchase behavior (no labels)","Predicting house prices from features like size and location","Finding anomalies in network traffic without labeled examples","An agent learning to play a game through trial and error"],answerIndex:1,explanation:"Predicting house prices uses labeled data (features ‚Üí price), making it supervised regression. Clustering is unsupervised, anomaly detection can be unsupervised, and game-playing is reinforcement learning.",wrongExplanations:["Grouping without labels is unsupervised clustering","Anomaly detection without labels is unsupervised learning","Trial-and-error game learning is reinforcement learning"]},
{chapter:101,difficulty:"hard",question:"Why does a linear function generalize better than a high-degree polynomial that perfectly fits training data?",options:["Linear functions are faster to compute","Simpler models (linear) avoid fitting noise ‚Äî a perfect-fit polynomial likely captures noise and won't generalize to new data","Linear functions always have higher accuracy","Polynomials cannot be used in ML"],answerIndex:1,explanation:"A polynomial that perfectly fits all training points is likely memorizing noise (overfitting). A simpler linear model captures the true underlying trend and generalizes better to unseen data ‚Äî this is the bias-variance trade-off in action.",wrongExplanations:["Computation speed isn't why linear models generalize better","Linear functions don't always have higher accuracy ‚Äî they generalize better on noisy data","Polynomials are valid ML models but risk overfitting"]},
{chapter:101,difficulty:"hard",question:"Name five kinds of probability distributions:",options:["Normal, Uniform, Poisson, Binomial, and Multinomial","Mean, Median, Mode, Range, and Variance","Linear, Quadratic, Cubic, Quartic, and Quintic","Addition, Subtraction, Multiplication, Division, and Modulus"],answerIndex:0,explanation:"The five key probability distributions: Normal (bell curve), Uniform (equal probability), Poisson (rare events), Binomial (success/fail trials), and Multinomial (multi-category trials).",wrongExplanations:["These are measures of central tendency and spread, not distributions","These are polynomial degrees, not probability distributions","These are arithmetic operations, not probability distributions"]},

// CHAPTER 102: MIDTERM EXAM PREP ‚Äî Predicted & Confirmed Questions
// MODULE 1 MIDTERM
{chapter:102,difficulty:"easy",question:"Which of the following is NOT one of the four AI definition categories?",options:["Think humanly","Act humanly","Learn humanly","Think rationally"],answerIndex:2,explanation:"The four categories are: Think Humanly, Act Humanly, Think Rationally, Act Rationally. 'Learn humanly' is not one of them.",wrongExplanations:["Think Humanly is one of the four categories (cognitive modeling)","Act Humanly is one of the four categories (Turing Test approach)","Think Rationally is one of the four categories (laws of thought)"]},
{chapter:102,difficulty:"easy",question:"What are the two dimensions on which all AI definitions revolve?",options:["Speed and efficiency","Human versus Rational and Thought versus Behavior","Hardware and Software","Agents and systems"],answerIndex:1,explanation:"All AI definitions are organized along two dimensions: Human vs Rational (who to model) and Thought vs Behavior (internal process vs external action).",wrongExplanations:["Speed/efficiency are performance metrics, not definitional dimensions","Hardware/software is a computing distinction, not an AI definition framework","Agents/systems are AI components, not the definitional dimensions"]},
{chapter:102,difficulty:"easy",question:"What is the main question the Turing Test tries to answer?",options:["Can machines be faster than humans?","Can machines think?","Can machines see?","Can machines move?"],answerIndex:1,explanation:"Alan Turing proposed the test in 1950 to address the fundamental question: 'Can machines think?' ‚Äî evaluated through conversation indistinguishable from a human.",wrongExplanations:["Speed is a hardware concern, not what the Turing Test measures","Vision is tested by the Total Turing Test, not the standard one","Physical movement is tested by the Total Turing Test, not the standard one"]},
{chapter:102,difficulty:"medium",question:"For a machine to pass the standard Turing Test, it needs:",options:["All 4 capabilities: NLP, Knowledge Representation, Automated Reasoning, Machine Learning","Only NLP and Machine Learning","Computer Vision and Robotics","All 6 AI capabilities"],answerIndex:0,explanation:"The standard Turing Test requires: (1) NLP to communicate, (2) Knowledge Representation to store information, (3) Automated Reasoning to draw conclusions, (4) Machine Learning to adapt.",wrongExplanations:["NLP and ML alone are insufficient ‚Äî reasoning and knowledge storage are also needed","Computer Vision and Robotics are additional requirements for the Total Turing Test only","Only 4 capabilities are needed for the standard test, not all 6"]},
{chapter:102,difficulty:"medium",question:"The Turing Total Test differs from the standard Turing Test by requiring:",options:["More time for conversation","Interaction with physical objects ‚Äî requires Computer Vision and Robotics","Multiple interrogators","Higher accuracy levels"],answerIndex:1,explanation:"The Total Turing Test adds physical interaction via video signal and object passing, requiring Computer Vision (perceiving objects) and Robotics (manipulating objects).",wrongExplanations:["Time is not the distinguishing factor between the two tests","The number of interrogators doesn't change between the two versions","Accuracy thresholds are the same ‚Äî the difference is physical interaction"]},
{chapter:102,difficulty:"easy",question:"What does it mean if a machine 'passes' the Turing Test?",options:["It can solve all problems","It fools the interrogator 30% of the time or more","It is smarter than humans","It can learn new languages instantly"],answerIndex:1,explanation:"A machine passes the Turing Test if it fools the human interrogator into thinking it is human at least 30% of the time during text-based conversation.",wrongExplanations:["Passing the Turing Test doesn't mean solving all problems","Passing means indistinguishable behavior, not superior intelligence","Language learning ability isn't how the test is measured"]},
{chapter:102,difficulty:"easy",question:"Which of the following is NOT one of the 5 AI disciplines?",options:["Natural Language Processing","Automated Reasoning","Quantum Computing","Machine Learning"],answerIndex:2,explanation:"The 5 AI disciplines are: NLP, Automated Reasoning, Machine Learning, Computer Vision, and Robotics. Quantum Computing is a separate field.",wrongExplanations:["NLP is one of the 5 AI disciplines","Automated Reasoning is one of the 5 AI disciplines","Machine Learning is one of the 5 AI disciplines"]},
{chapter:102,difficulty:"easy",question:"Which of the following is a major risk of AI development?",options:["Biased decision making","Surveillance and persuasion","Lethal autonomous weapons","All of the above"],answerIndex:3,explanation:"All three are recognized AI risks: bias affects decisions (loans, jobs), surveillance threatens privacy, and autonomous weapons raise ethical concerns.",wrongExplanations:["Biased decision making is a risk, but not the only one listed","Surveillance is a risk, but not the only one listed","Autonomous weapons are a risk, but not the only one listed"]},

// MODULE 2 MIDTERM ‚Äî AGENTS
{chapter:102,difficulty:"easy",question:"An agent's behavior is described by the _____ that maps any given percept sequence to an action.",options:["Agent function","Agent program","Agent sensor","Agent actuator"],answerIndex:0,explanation:"The agent function is the abstract mathematical description that maps every possible percept sequence to an action. The agent program is the concrete implementation.",wrongExplanations:["The agent program is the implementation, not the abstract mapping","Sensors perceive the environment but don't define the behavior mapping","Actuators execute actions but don't define the mapping from percepts to actions"]},
{chapter:102,difficulty:"easy",question:"An intelligent agent is best defined as:",options:["A robot that can move","A system that perceives its environment through sensors and acts through actuators to achieve goals","A computer program only","A device that learns continuously"],answerIndex:1,explanation:"An intelligent agent perceives its environment via sensors and acts upon it via actuators to achieve goals ‚Äî this is the fundamental agent definition in AI.",wrongExplanations:["Not all agents are physical robots ‚Äî software agents exist too","An agent is more than just a program ‚Äî it interacts with an environment","Learning is one capability, not the defining characteristic of all agents"]},
{chapter:102,difficulty:"medium",question:"What is the primary difference between an agent and a program?",options:["Agents are faster","Agents are more intelligent","Agents interact with their environment and can perceive and act autonomously","There is no difference"],answerIndex:2,explanation:"The key distinction: agents perceive their environment through sensors and act autonomously through actuators. Programs simply execute instructions without environmental interaction.",wrongExplanations:["Speed is not what differentiates agents from programs","Intelligence level isn't the defining distinction","There is a fundamental difference ‚Äî environmental interaction and autonomy"]},
{chapter:102,difficulty:"easy",question:"How does an AI agent interact with its environment?",options:["Using sensors and actuators","Using only sensors","Using only actuators","None of the above"],answerIndex:0,explanation:"An AI agent uses sensors to perceive its environment and actuators to act upon it. Both are needed for the perception-action loop.",wrongExplanations:["Sensors alone allow perception but not action","Actuators alone allow action but not perception","Agents must interact ‚Äî using both sensors and actuators"]},
{chapter:102,difficulty:"medium",question:"Which of the following best describes a rational agent?",options:["Makes random decisions to explore the environment","Attempts to maximize expected performance","Always succeeds in achieving its goals","Requires no information about the environment"],answerIndex:1,explanation:"A rational agent selects actions that maximize its expected performance measure, given the percept sequence and built-in knowledge available to it.",wrongExplanations:["Random decisions are not rational ‚Äî rational means optimizing performance","Rationality doesn't guarantee success ‚Äî it means making the best possible choice","Rational agents use available information to make decisions"]},
{chapter:102,difficulty:"easy",question:"Which of the following is a type of AI agent?",options:["Learning AI Agent","Simple Reflex AI Agent","Goal-Based AI Agent","All of the above"],answerIndex:3,explanation:"There are 5 agent types: Simple Reflex, Model-Based Reflex, Goal-Based, Utility-Based, and Learning. All listed options are valid types.",wrongExplanations:["Learning agent is valid but not the only correct answer","Simple Reflex is valid but not the only correct answer","Goal-Based is valid but not the only correct answer"]},
{chapter:102,difficulty:"medium",question:"A chess game environment is best described as:",options:["Fully observable, deterministic, multi-agent, sequential","Partially observable, stochastic, single-agent, dynamic","Fully observable, stochastic, multi-agent, dynamic","Partially observable, deterministic, single-agent, static"],answerIndex:0,explanation:"Chess: fully observable (entire board visible), deterministic (move outcomes are certain), multi-agent (two players), sequential/static (environment doesn't change between turns).",wrongExplanations:["Chess is fully observable ‚Äî you can see the entire board","Chess has no randomness ‚Äî it's deterministic, and it has two players","Chess is fully observable and deterministic ‚Äî every piece and position is visible"]},

// MODULE 3 MIDTERM ‚Äî UNINFORMED SEARCH
{chapter:102,difficulty:"easy",question:"In search problems, what is the 'state space'?",options:["The physical space where the agent operates","The set of all possible states the system can be in","Only the starting state","Only the goal state"],answerIndex:1,explanation:"The state space is the complete set of all possible configurations the system can exist in. The search algorithm explores this space to find a path from initial to goal state.",wrongExplanations:["State space is abstract, not necessarily a physical space","The starting state is just one element of the state space","The goal state is just one element of the state space"]},
{chapter:102,difficulty:"easy",question:"Which of the following is an uninformed search strategy?",options:["Best-first search","A* search","Breadth-first search (BFS)","Greedy search"],answerIndex:2,explanation:"BFS is uninformed ‚Äî it uses no domain-specific knowledge. Uninformed strategies include BFS, DFS, UCS, DLS, and Iterative Deepening. Greedy and A* are informed (use heuristics).",wrongExplanations:["Best-first search uses heuristics ‚Äî it's informed","A* uses g(n)+h(n) ‚Äî it's informed","Greedy search uses h(n) ‚Äî it's informed"]},
{chapter:102,difficulty:"medium",question:"After checking if a dequeued cell is the goal in BFS (and it is NOT the goal), what comes next?",options:["Update visited","Dequeue again","Exit algorithm","Enqueue neighbors and update predecessors"],answerIndex:3,explanation:"After dequeuing a node and finding it's not the goal, BFS enqueues all unvisited neighbors and updates their predecessor pointers for path reconstruction.",wrongExplanations:["Updating visited happens when neighbors are first discovered, not at this step","You don't immediately dequeue again ‚Äî first process the current node's neighbors","Exiting would terminate the search prematurely"]},
{chapter:102,difficulty:"easy",question:"Depth-first search (DFS) explores:",options:["All nodes at the current depth before going deeper","The deepest path first","Only the shortest path","All paths simultaneously"],answerIndex:1,explanation:"DFS goes as deep as possible along one branch before backtracking. It uses a stack (LIFO) to always explore the most recently discovered node first.",wrongExplanations:["Exploring level by level describes BFS, not DFS","DFS doesn't guarantee finding the shortest path","DFS explores one path at a time, not all simultaneously"]},
{chapter:102,difficulty:"easy",question:"What is the main advantage of Breadth-first search?",options:["It's faster than DFS","It guarantees finding the shortest path in unweighted graphs","It uses less memory","It's easier to implement"],answerIndex:1,explanation:"BFS explores nodes level by level, so the first solution found is guaranteed to be at the shallowest depth ‚Äî making it optimal for unweighted graphs.",wrongExplanations:["BFS is not necessarily faster ‚Äî it can be slower due to memory overhead","BFS uses MORE memory than DFS (O(b^d) vs O(bm))","Implementation complexity isn't BFS's main advantage"]},
{chapter:102,difficulty:"easy",question:"What data structure is used in Depth-First Search?",options:["Queue (FIFO)","Stack (LIFO)","Priority Queue","Linked List"],answerIndex:1,explanation:"DFS uses a Stack (Last-In, First-Out) so the most recently discovered node is explored first, driving the search deeper before backtracking.",wrongExplanations:["Queue (FIFO) is used in BFS, not DFS","Priority Queue is used in UCS, Greedy, and A* ‚Äî not DFS","Linked List is a general data structure, not specifically used for DFS"]},
{chapter:102,difficulty:"easy",question:"What data structure is used in Breadth-First Search?",options:["Stack (LIFO)","Queue (FIFO)","Priority Queue","Array"],answerIndex:1,explanation:"BFS uses a Queue (First-In, First-Out) so nodes are explored in the order they were discovered ‚Äî level by level, breadth-first.",wrongExplanations:["Stack (LIFO) is used in DFS, not BFS","Priority Queue is used in UCS and A*, not standard BFS","A plain array isn't the data structure ‚Äî BFS specifically uses FIFO queue operations"]},
{chapter:102,difficulty:"medium",question:"Which search algorithm uses the LEAST memory?",options:["DFS","BFS","A*","UCS"],answerIndex:0,explanation:"DFS has space complexity O(b*m) ‚Äî only stores the current path. BFS, A*, and UCS store the entire frontier: O(b^d), which is exponentially more.",wrongExplanations:["BFS stores the entire frontier level ‚Äî O(b^d), much more than DFS","A* stores the entire open set ‚Äî O(b^d)","UCS stores all discovered nodes sorted by cost ‚Äî O(b^d)"]},
{chapter:102,difficulty:"medium",question:"Why do we maintain a 'visited' set in graph search algorithms?",options:["To speed up the search","To prevent revisiting nodes and creating infinite loops","To determine the goal state","To calculate the cost function"],answerIndex:1,explanation:"Without a visited set, algorithms can revisit already-explored nodes, potentially creating infinite loops in graphs with cycles. The visited set ensures each node is processed only once.",wrongExplanations:["Speed improvement is a side effect, not the primary purpose","The goal test is a separate function, not related to the visited set","Cost calculation uses path costs, not the visited set"]},
{chapter:102,difficulty:"easy",question:"Which search algorithm uses FIFO queue ordering?",options:["Depth-First Search","Breadth-First Search","Depth-Limited Search","Iterative Deepening"],answerIndex:1,explanation:"BFS uses a FIFO (First-In, First-Out) queue ‚Äî nodes discovered first are explored first, ensuring level-by-level exploration.",wrongExplanations:["DFS uses a stack (LIFO), not a FIFO queue","DLS uses a stack with a depth limit, not a FIFO queue","Iterative Deepening uses repeated DFS with increasing limits, not a FIFO queue"]},
{chapter:102,difficulty:"medium",question:"For BFS with branching factor b and depth d, the time and space complexity are:",options:["Time: O(b^d), Space: O(b^d)","Time: O(b*d), Space: O(b*d)","Time: O(b^d), Space: O(b*m)","Time: O(d^b), Space: O(d^b)"],answerIndex:0,explanation:"BFS explores all nodes at each level: b + b¬≤ + b¬≥ + ... + b^d = O(b^d) for both time (nodes visited) and space (nodes stored in queue).",wrongExplanations:["O(b*d) is too optimistic ‚Äî BFS explores exponentially many nodes","O(b*m) space is for DFS, not BFS","O(d^b) inverts the relationship ‚Äî it should be base^exponent = b^d"]},
{chapter:102,difficulty:"hard",question:"Explain the key difference: DFS space complexity is O(b*m) while BFS is O(b^d) because:",options:["DFS is faster","DFS only stores nodes on the current path from root to leaf; BFS stores the entire frontier level","BFS uses less memory overall","They have the same space complexity"],answerIndex:1,explanation:"DFS only keeps the current path in memory (depth m, branching b = b*m nodes). BFS must store ALL nodes at the current frontier level, which grows exponentially (b^d).",wrongExplanations:["Speed and memory are different concerns ‚Äî this is about space, not time","BFS uses MORE memory, not less ‚Äî that's exactly why DFS is preferred for memory-limited search","They have very different space complexities ‚Äî O(bm) vs O(b^d)"]},
{chapter:102,difficulty:"medium",question:"Uniform Cost Search (UCS) is appropriate when:",options:["All edge costs are equal","Edge costs vary and we want the minimum cost path","We only care about the number of steps","Memory is unlimited"],answerIndex:1,explanation:"UCS expands the node with the lowest total path cost using a priority queue. When costs vary, it guarantees finding the cheapest path ‚Äî unlike BFS which only finds the shallowest.",wrongExplanations:["When all costs are equal, UCS reduces to BFS ‚Äî it's not specifically needed","UCS optimizes for cost, not just step count","Memory isn't a reason to choose UCS ‚Äî it uses similar memory to BFS"]},
{chapter:102,difficulty:"medium",question:"Given a state space with 10 possible actions per state and a maximum depth of 5, the maximum size of the search tree is:",options:["10^5 = 100,000","5^10","10 √ó 5 = 50","10 + 5 = 15"],answerIndex:0,explanation:"With branching factor b=10 and depth d=5, the search tree has at most b^d = 10^5 = 100,000 nodes. Each level multiplies the number of nodes by b.",wrongExplanations:["5^10 inverts the base and exponent ‚Äî it should be b^d, not d^b","10 √ó 5 only counts one branch per level ‚Äî the tree branches at every node","Addition doesn't account for the branching structure of the tree"]},
{chapter:102,difficulty:"medium",question:"UCS reduces to BFS when:",options:["The heuristic is zero","All step costs are equal","The graph has no cycles","The depth is limited"],answerIndex:1,explanation:"When all step costs are the same, minimizing total cost is equivalent to minimizing the number of steps ‚Äî so UCS behaves identically to BFS.",wrongExplanations:["Heuristic relates to A* and Greedy, not UCS","Cycles are handled by the visited set, not related to UCS/BFS equivalence","Depth limits relate to DLS, not the UCS-BFS relationship"]},

// MODULE 4 MIDTERM ‚Äî INFORMED SEARCH
{chapter:102,difficulty:"easy",question:"What is a heuristic in AI search?",options:["A guarantee to find the optimal solution","An estimate that guides search toward the goal","A random search method","A type of algorithm that learns"],answerIndex:1,explanation:"A heuristic is an educated estimate of the remaining cost from a node to the goal. It guides the search by helping prioritize which nodes to explore first.",wrongExplanations:["Heuristics don't guarantee optimality ‚Äî they provide estimates that may be imperfect","Heuristics are the opposite of random ‚Äî they provide informed guidance","Heuristics are fixed estimates, not learning algorithms"]},
{chapter:102,difficulty:"medium",question:"Greedy search always:",options:["Finds the optimal solution","Uses the least memory","Selects the path with the best heuristic value at each step","Explores all paths"],answerIndex:2,explanation:"Greedy search uses f(n) = h(n) ‚Äî it always expands the node that appears closest to the goal. It's NOT complete and NOT optimal because it ignores path cost g(n).",wrongExplanations:["Greedy search is NOT optimal ‚Äî it can miss cheaper paths","Greedy doesn't necessarily use the least memory","Greedy search is selective, not exhaustive ‚Äî it follows the best-looking path"]},
{chapter:102,difficulty:"medium",question:"A* search combines:",options:["The actual path cost g(n) and the heuristic estimate h(n)","Depth and breadth","Two different heuristics","Random selection and deterministic search"],answerIndex:0,explanation:"A* uses f(n) = g(n) + h(n): g(n) = actual cost from start to n, h(n) = estimated cost from n to goal. This balances known cost with estimated remaining cost.",wrongExplanations:["A* doesn't combine depth and breadth ‚Äî it uses cost-based prioritization","A* uses one heuristic combined with actual cost, not two heuristics","A* is fully deterministic, not random"]},
{chapter:102,difficulty:"medium",question:"What makes A* search optimal?",options:["It always finds the shortest path","It uses admissible heuristics (never overestimate) and combines actual cost with estimated cost","It's the fastest algorithm","It uses the least memory"],answerIndex:1,explanation:"A* is optimal when h(n) is admissible ‚Äî it never overestimates the true cost to the goal. Combined with g(n) (actual cost), this guarantees the first solution found is optimal.",wrongExplanations:["Shortest path guarantee requires the admissibility condition ‚Äî not automatic","A* is not the fastest ‚Äî it can be slow with poor heuristics","A* uses significant memory ‚Äî O(b^d) ‚Äî this isn't its advantage"]},
{chapter:102,difficulty:"easy",question:"What is an 'admissible heuristic'?",options:["A heuristic that is easy to compute","A heuristic that never overestimates the actual cost to reach the goal","A heuristic that uses machine learning","A heuristic that is always correct"],answerIndex:1,explanation:"Admissible means h(n) ‚â§ true cost from n to goal, for ALL nodes n. This optimistic estimate ensures A* never misses a cheaper path.",wrongExplanations:["Ease of computation is desirable but doesn't define admissibility","Admissibility is a mathematical property, not related to ML","Admissible heuristics can be wrong (underestimate) ‚Äî they just can't overestimate"]},
{chapter:102,difficulty:"medium",question:"Which search algorithm guarantees finding the shortest path with minimum cost?",options:["Greedy search","Depth-first search","A* search with an admissible heuristic","Breadth-first search"],answerIndex:2,explanation:"A* with an admissible heuristic guarantees the optimal (minimum cost) solution. BFS finds shortest by steps but not by cost. Greedy and DFS have no optimality guarantee.",wrongExplanations:["Greedy search is not optimal ‚Äî it ignores actual path cost","DFS is not optimal ‚Äî it finds the first solution, not the best","BFS is optimal for equal costs only ‚Äî A* handles variable costs"]},
{chapter:102,difficulty:"medium",question:"The features of a good heuristic function include:",options:["It should be easy to compute","It should not overestimate the cost","It should guide the search efficiently","All of the above"],answerIndex:3,explanation:"A good heuristic is: (1) easy to compute (low overhead), (2) admissible (never overestimates), and (3) informative (guides search toward the goal efficiently).",wrongExplanations:["Easy to compute is one feature, but not the only one","Not overestimating is admissibility ‚Äî one feature, but not the only one","Efficient guidance is one feature, but not the only one"]},
{chapter:102,difficulty:"medium",question:"What is the key difference between informed and uninformed search?",options:["Informed search uses more memory","Informed search uses domain-specific knowledge (heuristics) to guide the search","Uninformed search is always faster","They are the same thing"],answerIndex:1,explanation:"Informed search uses heuristics ‚Äî domain-specific estimates of remaining cost. Uninformed search (BFS, DFS, UCS) has no such guidance and explores more blindly.",wrongExplanations:["Memory usage depends on the specific algorithm, not informed vs uninformed","Uninformed search is often slower because it lacks guidance","They are fundamentally different in their use of domain knowledge"]},
{chapter:102,difficulty:"hard",question:"What is the main difference between Greedy search and A* search?",options:["Greedy uses f(n) = h(n) only; A* uses f(n) = g(n) + h(n)","Greedy is always optimal; A* is not","A* uses no heuristic; Greedy does","There is no difference"],answerIndex:0,explanation:"Greedy considers only estimated remaining cost h(n). A* considers both actual cost so far g(n) AND estimated remaining cost h(n), making it optimal with admissible h.",wrongExplanations:["Greedy is NOT optimal ‚Äî A* is optimal (with admissible h)","A* DOES use a heuristic ‚Äî it uses h(n) combined with g(n)","They are fundamentally different in their evaluation functions"]},
{chapter:102,difficulty:"hard",question:"In A* search, f(n) = g(n) + h(n). What does g(n) represent?",options:["The heuristic estimate from n to goal","The actual path cost from start to n","The total estimated cost","The branching factor"],answerIndex:1,explanation:"g(n) is the actual, known cost of the path from the start node to node n. h(n) is the estimated cost from n to the goal. f(n) combines both.",wrongExplanations:["The heuristic estimate from n to goal is h(n), not g(n)","The total estimated cost is f(n) = g(n) + h(n), not just g(n)","The branching factor b is a property of the search tree, not related to g(n)"]},
{chapter:102,difficulty:"hard",question:"Given: Start S, Goal G. Path S‚ÜíA: g=2, h(A)=3. Path S‚ÜíB: g=4, h(B)=1. Which node does A* expand first?",options:["Node A because f(A)=2+3=5","Node B because f(B)=4+1=5","Both have equal f-value (5), either can be expanded","Neither ‚Äî the algorithm terminates"],answerIndex:2,explanation:"f(A) = g(A)+h(A) = 2+3 = 5. f(B) = g(B)+h(B) = 4+1 = 5. Since both have equal f-values, ties are broken arbitrarily ‚Äî either can be expanded first.",wrongExplanations:["f(A)=5 is correct, but f(B)=5 too, so A isn't necessarily first","f(B)=5 is correct, but f(A)=5 too, so B isn't necessarily first","The algorithm continues ‚Äî it only terminates when the goal is dequeued"]},
{chapter:102,difficulty:"hard",question:"A heuristic is consistent (monotone) if for every node n and successor n':",options:["h(n) ‚â§ cost(n,n') + h(n') ‚Äî the triangle inequality","h(n) ‚â• cost(n,n') + h(n')","h(n) = h(n')","h(n) = 0"],answerIndex:0,explanation:"Consistency means the heuristic estimate can't decrease by more than the actual step cost ‚Äî this is the triangle inequality. Every consistent heuristic is automatically admissible.",wrongExplanations:["The inequality goes the other way ‚Äî h(n) must be ‚â§, not ‚â•","Heuristic values change between nodes ‚Äî they're not equal","h(n)=0 is a valid but uninformative heuristic ‚Äî not the definition of consistency"]},

// MODULE 5 MIDTERM ‚Äî MACHINE LEARNING
{chapter:102,difficulty:"easy",question:"Machine learning is best described as:",options:["Programming a computer with explicit rules","The ability of a machine to expand its knowledge without human intervention","A type of database management","A hardware improvement technique"],answerIndex:1,explanation:"Machine learning enables machines to learn from data and improve performance without being explicitly programmed with every rule ‚Äî they learn patterns from observations.",wrongExplanations:["Explicit rules is the traditional approach, not ML","Database management stores data but doesn't learn from it","Hardware improvements are engineering, not ML"]},
{chapter:102,difficulty:"easy",question:"The main difference between the traditional approach and machine learning is:",options:["Traditional is faster","Traditional focuses on rules to build logic; ML focuses on previous data to identify rules/logic","ML requires more hardware","There is no difference"],answerIndex:1,explanation:"Traditional AI: humans write rules explicitly. ML: the system discovers rules/patterns from data. This is a fundamental paradigm shift from rule-based to data-driven AI.",wrongExplanations:["Speed isn't the defining difference between the approaches","Hardware requirements vary ‚Äî this isn't the core distinction","They are fundamentally different approaches to building AI systems"]},
{chapter:102,difficulty:"easy",question:"What are the three main types of machine learning?",options:["Supervised, Unsupervised, and Reinforcement","Classification, Regression, and Clustering","Linear, Non-linear, and Neural","Training, Testing, and Validation"],answerIndex:0,explanation:"The three main types: Supervised (labeled data), Unsupervised (no labels, finds patterns), and Reinforcement (learns from rewards/punishments).",wrongExplanations:["Classification/Regression/Clustering are subtypes, not the main categories","Linear/Non-linear/Neural are model types, not learning categories","Training/Testing/Validation are data splits, not learning types"]},
{chapter:102,difficulty:"medium",question:"In supervised learning, what does the agent observe?",options:["Random data without labels","Input-output pairs, learning a function that maps input to output","Only rewards and punishments","Patterns without guidance"],answerIndex:1,explanation:"Supervised learning uses labeled training data ‚Äî pairs of (input, correct output). The agent learns a function h that maps inputs to outputs, approximating the true function f.",wrongExplanations:["Random unlabeled data is used in unsupervised learning","Rewards/punishments are used in reinforcement learning","Finding patterns without guidance is unsupervised learning"]},
{chapter:102,difficulty:"easy",question:"The most common unsupervised learning task is:",options:["Classification","Regression","Clustering","Reinforcement"],answerIndex:2,explanation:"Clustering groups similar data points together without labeled examples. The algorithm discovers natural groupings in the data on its own ‚Äî no teacher needed.",wrongExplanations:["Classification requires labels ‚Äî it's supervised","Regression requires labeled outputs ‚Äî it's supervised","Reinforcement is a separate learning type, not unsupervised"]},
{chapter:102,difficulty:"easy",question:"In reinforcement learning, the agent learns from:",options:["Labeled examples","A series of rewards and punishments","Explicit rules","Unsupervised patterns"],answerIndex:1,explanation:"Reinforcement learning agents take actions in an environment and receive rewards (+) or punishments (-). Over time, they learn which actions maximize cumulative reward.",wrongExplanations:["Labeled examples are used in supervised learning","Explicit rules are used in traditional AI, not reinforcement learning","Unsupervised patterns is unsupervised learning, not reinforcement"]},
{chapter:102,difficulty:"medium",question:"If the predicted output is categorical (e.g., sunny, cloudy, rainy), this is a _____ problem. If continuous (e.g., 33.5¬∞), it's a _____ problem.",options:["Regression; Classification","Classification; Regression","Clustering; Regression","Classification; Clustering"],answerIndex:1,explanation:"Classification predicts discrete categories (spam/not spam, sunny/cloudy). Regression predicts continuous numerical values (temperature, price, distance).",wrongExplanations:["The order is reversed ‚Äî categorical=classification, continuous=regression","Clustering is unsupervised and doesn't predict specific outputs","Clustering is unsupervised, not a supervised prediction task"]},
{chapter:102,difficulty:"medium",question:"A model that performs well on training data but poorly on test data is experiencing:",options:["Underfitting","Overfitting","Good generalization","Random error"],answerIndex:1,explanation:"Overfitting = high variance. The model memorizes training data (including noise) and fails to generalize to new, unseen data. Good training + bad testing = overfitting.",wrongExplanations:["Underfitting means poor performance on BOTH training and test data","Good generalization means good performance on BOTH, not just training","Random error would affect both training and test equally"]},
{chapter:102,difficulty:"medium",question:"A model that has poor performance on BOTH training and test data is experiencing:",options:["Overfitting","Underfitting","Perfect fit","Bias-variance balance"],answerIndex:1,explanation:"Underfitting = high bias. The model is too simple to capture the underlying patterns. It can't even fit the training data, so it naturally fails on test data too.",wrongExplanations:["Overfitting means good training performance ‚Äî this model is poor on both","Perfect fit would mean excellent performance, not poor","Bias-variance balance is the goal ‚Äî this model has too much bias"]},
{chapter:102,difficulty:"medium",question:"The goal of the bias-variance tradeoff is to:",options:["Always minimize bias","Always minimize variance","Find the model that optimally fits future examples, balancing simplicity and complexity","Use the most complex model possible"],answerIndex:2,explanation:"The tradeoff: too simple = high bias (underfitting), too complex = high variance (overfitting). The goal is finding the sweet spot that generalizes best to unseen data.",wrongExplanations:["Minimizing only bias leads to overfitting (high variance)","Minimizing only variance leads to underfitting (high bias)","Maximum complexity causes overfitting ‚Äî the opposite of what we want"]},
{chapter:102,difficulty:"medium",question:"What percentage of time in an ML project is typically spent on building the feature space?",options:["10-20%","30-40%","50-60%","70-80%"],answerIndex:3,explanation:"Feature engineering is the most time-consuming part of ML ‚Äî 70-80% of project time goes to data transformation, normalization, handling missing values, correlation analysis, and feature selection.",wrongExplanations:["10-20% greatly underestimates the effort required","30-40% underestimates the feature engineering workload","50-60% is closer but still underestimates ‚Äî it's 70-80%"]},
{chapter:102,difficulty:"easy",question:"The L2 loss function is defined as:",options:["|y - ≈∑| (absolute difference)","(y - ≈∑)¬≤ (squared difference)","0 if y=≈∑, else 1 (zero-one loss)","log(y - ≈∑)"],answerIndex:1,explanation:"L2 (squared loss) = (y - ≈∑)¬≤. It penalizes larger errors more heavily than L1. L1 = |y-≈∑| (absolute), L0/1 = 0 if correct, 1 if wrong.",wrongExplanations:["Absolute difference is L1 loss, not L2","Zero-one loss is L0/1, used for classification, not L2","Logarithmic loss is cross-entropy, not L2"]},
{chapter:102,difficulty:"medium",question:"In supervised learning, the function h that approximates the true function f is called:",options:["The loss function","The hypothesis","The training set","The test set"],answerIndex:1,explanation:"The hypothesis h is the learned function drawn from hypothesis space H. The learning algorithm searches H to find the h that best approximates the unknown true function f.",wrongExplanations:["The loss function measures error, it's not the learned approximation","The training set is the data used to learn, not the learned function itself","The test set evaluates the learned function, it's not the function itself"]},
{chapter:102,difficulty:"medium",question:"The uni-variate linear regression equation is:",options:["y = w‚ÇÅx + w‚ÇÄ","y = x¬≤ + w‚ÇÄ","y = eÀ£ + w‚ÇÄ","y = log(x) + w‚ÇÄ"],answerIndex:0,explanation:"Linear regression: y = w‚ÇÅx + w‚ÇÄ, where w‚ÇÅ is the slope (rate of change) and w‚ÇÄ is the y-intercept. The model learns optimal w‚ÇÄ and w‚ÇÅ by minimizing squared loss.",wrongExplanations:["x¬≤ is quadratic, not linear regression","eÀ£ is exponential, not linear regression","log(x) is logarithmic, not linear regression"]},
{chapter:102,difficulty:"medium",question:"R-squared (R¬≤) measures:",options:["The total error","How well the regression model explains the variability in data","The number of features","The sample size"],answerIndex:1,explanation:"R¬≤ = 1 - (SSE/SST). It ranges from 0 to 1, measuring the proportion of variance in the data explained by the model. R¬≤=1 means perfect fit, R¬≤=0 means no explanation.",wrongExplanations:["R¬≤ measures explained variance, not total error (that's SST)","Number of features is a model parameter, not what R¬≤ measures","Sample size is n ‚Äî unrelated to R¬≤"]},
{chapter:102,difficulty:"medium",question:"A correlation coefficient of -0.85 indicates:",options:["No relationship","A strong positive relationship","A strong negative (inverse) relationship","A weak relationship"],answerIndex:2,explanation:"A coefficient of -0.85 is below -0.5, indicating a strong negative (inverse) correlation: as one variable increases, the other decreases significantly.",wrongExplanations:["0 means no relationship ‚Äî -0.85 is far from 0","Positive correlation has a positive coefficient ‚Äî this is negative","Values above 0.5 or below -0.5 are notable ‚Äî 0.85 is very strong"]},
{chapter:102,difficulty:"medium",question:"The null hypothesis in linear regression is:",options:["H‚ÇÄ: w‚ÇÅ = 1","H‚ÇÄ: w‚ÇÅ = 0 (no relationship between x and y)","H‚ÇÄ: w‚ÇÄ = 0","H‚ÇÄ: R¬≤ = 1"],answerIndex:1,explanation:"H‚ÇÄ: w‚ÇÅ = 0 means the slope is zero ‚Äî there is no linear relationship between x and y. If we reject H‚ÇÄ (p-value < 0.05), we conclude a significant relationship exists.",wrongExplanations:["w‚ÇÅ=1 would mean a specific slope, not 'no relationship'","w‚ÇÄ=0 would test the intercept, not the relationship itself","R¬≤=1 would mean perfect fit ‚Äî that's not the null hypothesis"]},
{chapter:102,difficulty:"medium",question:"When do we reject the null hypothesis?",options:["When p-value > 0.05","When p-value < significance level (e.g., < 0.05)","When p-value = 1","Never"],answerIndex:1,explanation:"We reject H‚ÇÄ when the p-value is less than the significance level (typically 0.05). A small p-value means the observed data is unlikely under H‚ÇÄ, so we reject it.",wrongExplanations:["p-value > 0.05 means we FAIL to reject H‚ÇÄ ‚Äî the evidence isn't strong enough","p-value = 1 means the data perfectly matches H‚ÇÄ ‚Äî we definitely don't reject","Hypothesis testing is designed specifically to sometimes reject H‚ÇÄ"]},
{chapter:102,difficulty:"easy",question:"Exploratory Learning is another name for:",options:["Supervised learning","Unsupervised learning","Reinforcement learning","None of the above"],answerIndex:1,explanation:"Unsupervised learning is also called exploratory learning because the agent explores and discovers patterns in data without any external teacher or labels guiding it.",wrongExplanations:["Supervised learning has a teacher providing labels ‚Äî it's not exploratory","Reinforcement learning uses rewards ‚Äî different from unsupervised exploration","Unsupervised learning IS exploratory learning"]},

// CODE-BASED MIDTERM QUESTIONS
{chapter:102,difficulty:"medium",question:"What is the difference between queue.pop(0) and stack.pop() in Python?",options:["No difference","pop(0) removes the first element (FIFO); pop() removes the last element (LIFO)","pop(0) removes the last element; pop() removes the first element","Both remove random elements"],answerIndex:1,explanation:"pop(0) removes index 0 (first element) = FIFO behavior for BFS queues. pop() removes the last element = LIFO behavior for DFS stacks. This is the key code difference between BFS and DFS.",wrongExplanations:["There is a crucial difference ‚Äî they remove from opposite ends","The description is reversed ‚Äî pop(0)=first, pop()=last","Python list operations are deterministic, not random"]},
{chapter:102,difficulty:"hard",question:"What is wrong with this BFS? def bfs(s,g): q=[s]\\n while q: c=q.pop(0)\\n  if c==g: return True\\n  for n in neighbors(c): q.append(n)\\n return False",options:["Nothing is wrong","Missing a visited set ‚Äî will create infinite loops in graphs with cycles","Should use stack instead of queue","Should pop from the end"],answerIndex:1,explanation:"Without a visited set, the algorithm will revisit previously explored nodes. In any graph with cycles, this creates an infinite loop as nodes are enqueued repeatedly.",wrongExplanations:["There IS a critical bug ‚Äî missing visited tracking","Using a stack would make it DFS ‚Äî still needs a visited set","Popping from the end would make it DFS ‚Äî still needs a visited set"]},
{chapter:102,difficulty:"hard",question:"Given graph={'A':['B','C'],'B':['D','E'],'C':['F'],'D':[],'E':[],'F':[]} ‚Äî Using DFS with pop(), starting from 'A', what is the visit order?",options:["A, B, C, D, E, F","A, C, F, B, E, D","A, B, D, E, C, F","F, E, D, C, B, A"],answerIndex:1,explanation:"DFS with pop(): Pop A, push [B,C]. Pop C (last), push [F]. Pop F (no children). Pop B, push [D,E]. Pop E (no children). Pop D. Visit order: A, C, F, B, E, D.",wrongExplanations:["Level-by-level would be BFS order, not DFS","This order would require pushing children in reverse","Reverse order isn't how any standard search works"]},
{chapter:102,difficulty:"hard",question:"Given the same graph ‚Äî Using BFS with pop(0), starting from 'A', what is the visit order?",options:["A, B, C, D, E, F","A, C, F, B, E, D","A, B, D, E, C, F","F, E, D, C, B, A"],answerIndex:0,explanation:"BFS with pop(0): Dequeue A, enqueue [B,C]. Dequeue B (first), enqueue [D,E]. Dequeue C, enqueue [F]. Dequeue D. Dequeue E. Dequeue F. Order: A, B, C, D, E, F ‚Äî level by level.",wrongExplanations:["This is the DFS order, not BFS","This order skips levels ‚Äî BFS goes level by level","Reverse order isn't how BFS works"]},
{chapter:102,difficulty:"hard",question:"In a BFS implementation: queue=['B','C']; current=queue.pop(0) ‚Äî What is the value of current?",options:["'C'","'B'","'A'","Error"],answerIndex:1,explanation:"pop(0) removes and returns the FIRST element. Queue ['B','C'] ‚Üí removes index 0 ‚Üí current='B'. Queue becomes ['C']. This is FIFO behavior.",wrongExplanations:["'C' would be the result of pop() (last element), not pop(0)","'A' was already removed from the queue ‚Äî it's not present","This is valid Python ‚Äî no error occurs"]},
{chapter:102,difficulty:"hard",question:"What change would convert A* search code into Greedy search?",options:["Change f_score = g + h(n) to f_score = h(n) only","Change f_score = g + h(n) to f_score = g only","Remove the visited set","Use a stack instead of priority queue"],answerIndex:0,explanation:"A* uses f=g+h. Greedy uses f=h only ‚Äî removing g(n) (actual cost) makes the algorithm consider only the heuristic estimate. This makes it greedy but not optimal.",wrongExplanations:["Removing h and keeping only g would make it UCS, not Greedy","Removing visited doesn't change the algorithm type ‚Äî just allows revisits","Using a stack would make it DFS-like, not Greedy"]},
{chapter:102,difficulty:"hard",question:"In A* code: f_score = new_g + h(neighbor) ‚Äî This computes:",options:["Only the heuristic estimate","Only the actual path cost","The total estimated cost: actual cost from start + heuristic estimate to goal","The distance between two neighbors"],answerIndex:2,explanation:"f(n) = g(n) + h(n) is the core of A*. new_g is the actual cost from start to neighbor (g), h(neighbor) is the estimated cost from neighbor to goal (h). Together = total estimated cost.",wrongExplanations:["h(neighbor) alone is only the heuristic ‚Äî this also includes g","new_g alone is only the actual cost ‚Äî this also includes h","This computes the total estimated cost from start through neighbor to goal, not just one step"]},
{chapter:102,difficulty:"hard",question:"Linear regression code: X=[1,2,3,4,5], Y=[2,4,6,8,10]. After computing OLS, w1 and w0 are:",options:["w1=2.0, w0=0.0","w1=1.0, w0=1.0","w1=0.5, w0=1.5","w1=3.0, w0=-1.0"],answerIndex:0,explanation:"The data is perfectly linear: Y = 2X + 0. The slope w1=2 (Y increases by 2 for each unit increase in X) and intercept w0=0 (Y=0 when X=0).",wrongExplanations:["w1=1 would mean Y=X+1, which doesn't match (1,2), (2,4), (3,6)...","w1=0.5 would mean Y=0.5X+1.5, which doesn't match the data","w1=3 would mean Y=3X-1, which doesn't match (1,2), (2,4)..."]},

// SEARCH ALGORITHM PROPERTIES
{chapter:102,difficulty:"medium",question:"A search algorithm is said to be 'complete' if:",options:["It finds the optimal solution","It terminates in finite time","It guarantees to find a solution if one exists","It uses minimum memory"],answerIndex:2,explanation:"Completeness means the algorithm will always find a solution if one exists, given enough time and memory. BFS is complete; DFS is not (can get stuck in infinite branches).",wrongExplanations:["Finding the optimal solution is 'optimality', not completeness","Terminating in finite time is related but not the exact definition","Minimum memory is space efficiency, not completeness"]},
{chapter:102,difficulty:"medium",question:"A search algorithm is 'optimal' if:",options:["It runs in the least time","It uses the least memory","It finds the solution with the lowest cost","It expands the fewest nodes"],answerIndex:2,explanation:"Optimality means the algorithm finds the best possible solution ‚Äî the one with the lowest total path cost among all possible solutions. A* is optimal (with admissible h); DFS is not.",wrongExplanations:["Running time is efficiency, not optimality","Memory usage is space efficiency, not optimality","Expanding fewest nodes is about efficiency, not solution quality"]},
{chapter:102,difficulty:"medium",question:"A search problem is defined by which components?",options:["Initial state, actions, transition model, goal test, path cost","Only initial state and goal state","Only actions and costs","Only the search algorithm"],answerIndex:0,explanation:"A complete search problem needs all 5 components: (1) initial state, (2) available actions, (3) transition model (what actions do), (4) goal test, and (5) path cost function.",wrongExplanations:["Initial and goal states alone don't define how to get between them","Actions and costs miss the states and goal definition","The algorithm solves the problem ‚Äî it doesn't define it"]},
{chapter:102,difficulty:"easy",question:"The branching factor in a search tree is:",options:["The depth of the tree","The maximum number of successors of any node","The total number of nodes","The number of goal states"],answerIndex:1,explanation:"Branching factor (b) = the maximum number of children any node can have. It determines how fast the search tree grows: b^d nodes at depth d.",wrongExplanations:["Depth is a different property ‚Äî how many levels deep the tree goes","Total nodes depends on both branching factor and depth","Number of goal states is unrelated to branching factor"]},

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// CHAPTER 103: FINAL EXAM PREP ‚Äî ML & Logistic Regression
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

// --- LOGISTIC REGRESSION ---
{chapter:103,difficulty:"easy",question:"What is Logistic Regression used for?",options:["Predicting continuous values like price","Binary classification problems","Clustering data into groups","Dimensionality reduction"],answerIndex:1,explanation:"Logistic Regression is used for binary classification. It predicts the probability that an observation belongs to a particular class (0 or 1) using the sigmoid function.",wrongExplanations:["Predicting continuous values is Linear Regression, not Logistic","Clustering is unsupervised learning (e.g., K-Means), not Logistic Regression","Dimensionality reduction is done by PCA, not Logistic Regression"]},
{chapter:103,difficulty:"easy",question:"The sigmoid function transforms any real value into a number between:",options:["‚àí1 and 1","0 and 100","0 and 1","‚àí‚àû and +‚àû"],answerIndex:2,explanation:"The sigmoid function œÉ(z) = 1/(1+e^(‚àíz)) maps any real number to a value between 0 and 1, making it perfect for probability estimation in classification.",wrongExplanations:["The range ‚àí1 to 1 describes the tanh function, not sigmoid","0 to 100 is not the range of any standard activation function","‚àí‚àû to +‚àû is the input range, not the output range of sigmoid"]},
{chapter:103,difficulty:"medium",question:"Why can't we use MSE (Mean Squared Error) as the loss function for Logistic Regression?",options:["MSE is only for unsupervised learning","MSE with sigmoid creates a non-convex optimization problem","MSE always equals zero for classification","MSE requires continuous output only"],answerIndex:1,explanation:"Because the sigmoid function is non-linear, using MSE results in a non-convex optimization landscape with many local minima. Log Loss (Cross-Entropy) is used instead because it's convex with sigmoid.",wrongExplanations:["MSE can be used in supervised learning (Linear Regression uses it)","MSE wouldn't be zero ‚Äî it just wouldn't optimize well","While MSE is common for continuous output, the real issue is non-convexity with sigmoid"]},
{chapter:103,difficulty:"medium",question:"What is the Log Loss (Cross-Entropy) formula?",options:["Loss = (y ‚àí p)¬≤","Loss = |y ‚àí p|","Loss = ‚àí[y¬∑log(p) + (1‚àíy)¬∑log(1‚àíp)]","Loss = y/p + (1‚àíy)/(1‚àíp)"],answerIndex:2,explanation:"Log Loss = ‚àí[y¬∑log(p) + (1‚àíy)¬∑log(1‚àíp)], where y is the actual label (0 or 1) and p is the predicted probability. This is convex and heavily penalizes confident wrong predictions.",wrongExplanations:["(y‚àíp)¬≤ is the squared loss (L2), used in Linear Regression","|y‚àíp| is the absolute loss (L1), not used in Logistic Regression","y/p + (1‚àíy)/(1‚àíp) is not a standard loss function"]},
{chapter:103,difficulty:"hard",question:"In Log Loss, if the actual label y=1 and the model predicts p=0.01, what happens?",options:["Loss is close to 0 (good prediction)","Loss is very large (heavy penalty)","Loss is exactly 1","Loss is undefined"],answerIndex:1,explanation:"When y=1, Loss = ‚àílog(p) = ‚àílog(0.01) ‚âà 4.6. The model was very confident it was class 0 but the actual was class 1, so the penalty is huge. Log Loss heavily penalizes confident wrong predictions.",wrongExplanations:["Loss close to 0 would happen if p were close to 1 (correct confident prediction)","Loss is not fixed at 1 ‚Äî it varies based on how wrong the prediction is","Loss is defined for all p between 0 and 1 (exclusive)"]},
{chapter:103,difficulty:"easy",question:"Despite its name, Logistic Regression is actually used for:",options:["Regression problems","Classification problems","Clustering problems","Feature selection"],answerIndex:1,explanation:"This is a common trick question! Despite having 'Regression' in its name, Logistic Regression is a classification algorithm. It outputs probabilities between 0 and 1 for class membership.",wrongExplanations:["The 'Regression' in the name is misleading ‚Äî it's actually for classification","Clustering is unsupervised learning, not what Logistic Regression does","Feature selection is a preprocessing step, not what Logistic Regression does"]},
{chapter:103,difficulty:"medium",question:"What is the decision boundary in Logistic Regression?",options:["The point where loss is minimized","The threshold (typically 0.5) where predictions switch between classes","The maximum value of the sigmoid function","The number of features in the model"],answerIndex:1,explanation:"The decision boundary is the threshold (usually 0.5) where the model switches from predicting class 0 to class 1. If sigmoid output ‚â• 0.5, predict class 1; otherwise class 0.",wrongExplanations:["Loss minimization is the training objective, not the decision boundary","The maximum of sigmoid is 1 (approached asymptotically), not the boundary","Number of features defines the input space, not the decision threshold"]},
{chapter:103,difficulty:"hard",question:"Which statement correctly compares Linear and Logistic Regression?",options:["Both use MSE as their loss function","Linear predicts continuous values with MSE; Logistic predicts probabilities with Log Loss","Both output values between 0 and 1","Logistic Regression cannot be used with multiple features"],answerIndex:1,explanation:"Linear Regression predicts continuous values using MSE (L2) loss. Logistic Regression predicts class probabilities (0 to 1) using Log Loss (Cross-Entropy). They serve fundamentally different purposes.",wrongExplanations:["Logistic Regression uses Log Loss, not MSE ‚Äî MSE would be non-convex with sigmoid","Linear Regression output can be any real number, not just 0 to 1","Logistic Regression can absolutely use multiple features (multivariate)"]},

// --- CONFUSION MATRIX ---
{chapter:103,difficulty:"easy",question:"In a confusion matrix, what does TP (True Positive) mean?",options:["The model predicted positive and it was actually positive","The model predicted positive but it was actually negative","The model predicted negative but it was actually positive","The model predicted negative and it was actually negative"],answerIndex:0,explanation:"True Positive = the model correctly predicted the positive class. The 'True' means the prediction was correct, and 'Positive' is the predicted class.",wrongExplanations:["Predicted positive but actually negative is a False Positive (FP)","Predicted negative but actually positive is a False Negative (FN)","Predicted negative and actually negative is a True Negative (TN)"]},
{chapter:103,difficulty:"easy",question:"What is a False Positive (FP) also known as?",options:["Type II Error","Type I Error","True alarm","Miss"],answerIndex:1,explanation:"False Positive = Type I Error. The model falsely raised an alarm (predicted positive when actually negative). Example: marking a legitimate email as spam.",wrongExplanations:["Type II Error is a False Negative (FN), not FP","A true alarm would be a True Positive, not FP","A miss is a False Negative ‚Äî we missed a positive case"]},
{chapter:103,difficulty:"easy",question:"What is a False Negative (FN) also known as?",options:["Type I Error","Type II Error","False alarm","Correct rejection"],answerIndex:1,explanation:"False Negative = Type II Error. The model missed a positive case (predicted negative when actually positive). Example: failing to detect cancer in a sick patient.",wrongExplanations:["Type I Error is a False Positive (FP), not FN","A false alarm is a False Positive ‚Äî predicted positive when it shouldn't","Correct rejection is a True Negative ‚Äî correctly predicted negative"]},
{chapter:103,difficulty:"medium",question:"Given: TP=40, TN=50, FP=10, FN=20. What is the Accuracy?",options:["75%","80%","85%","90%"],answerIndex:0,explanation:"Accuracy = (TP+TN)/(TP+TN+FP+FN) = (40+50)/(40+50+10+20) = 90/120 = 0.75 = 75%.",wrongExplanations:["80% would require (TP+TN)=96 out of 120, which is not the case","85% would require (TP+TN)=102 out of 120, which is not the case","90% would require (TP+TN)=108 out of 120, which is not the case"]},
{chapter:103,difficulty:"medium",question:"Given: TP=40, TN=50, FP=10, FN=20. What is the Precision?",options:["67%","75%","80%","85%"],answerIndex:2,explanation:"Precision = TP/(TP+FP) = 40/(40+10) = 40/50 = 0.80 = 80%. Of all positive predictions, 80% were actually positive.",wrongExplanations:["67% would be the Recall: TP/(TP+FN) = 40/60","75% is the Accuracy, not Precision","85% doesn't match any metric with these values"]},
{chapter:103,difficulty:"medium",question:"Given: TP=40, TN=50, FP=10, FN=20. What is the Recall?",options:["67%","75%","80%","85%"],answerIndex:0,explanation:"Recall = TP/(TP+FN) = 40/(40+20) = 40/60 = 0.667 = 67%. Of all actual positives, we caught 67%.",wrongExplanations:["75% is the Accuracy, not Recall","80% is the Precision: TP/(TP+FP) = 40/50","85% doesn't match any metric with these values"]},
{chapter:103,difficulty:"hard",question:"Given: TP=30, TN=50, FP=10, FN=10. Calculate the Precision.",options:["75%","80%","85%","60%"],answerIndex:0,explanation:"Precision = TP/(TP+FP) = 30/(30+10) = 30/40 = 0.75 = 75%. Of all positive predictions, 75% were correct.",wrongExplanations:["80% is the Accuracy: (30+50)/100","85% doesn't match ‚Äî check: 30/40 = 0.75","60% doesn't match any metric here"]},
{chapter:103,difficulty:"hard",question:"Given: TP=30, TN=50, FP=10, FN=10. Calculate the Recall.",options:["60%","75%","80%","85%"],answerIndex:1,explanation:"Recall = TP/(TP+FN) = 30/(30+10) = 30/40 = 0.75 = 75%. Of all actual positives, we caught 75%.",wrongExplanations:["60% doesn't match: 30/40 = 0.75, not 0.60","80% is the Accuracy: (30+50)/100","85% doesn't match any metric here"]},
{chapter:103,difficulty:"hard",question:"Given: TP=45, TN=30, FP=15, FN=10. What is the Accuracy?",options:["70%","75%","80%","85%"],answerIndex:1,explanation:"Accuracy = (TP+TN)/(Total) = (45+30)/(45+30+15+10) = 75/100 = 0.75 = 75%.",wrongExplanations:["70% would require 70 correct out of 100","80% would require 80 correct out of 100","85% would require 85 correct out of 100"]},
{chapter:103,difficulty:"hard",question:"Given: TP=45, TN=30, FP=15, FN=10. What is the Recall?",options:["75%","81.8%","85%","90%"],answerIndex:1,explanation:"Recall = TP/(TP+FN) = 45/(45+10) = 45/55 ‚âà 0.818 = 81.8%. Of all actual positive cases, we caught about 82%.",wrongExplanations:["75% is the Accuracy, not the Recall","85% doesn't match: 45/55 = 0.818","90% doesn't match: 45/55 = 0.818"]},
{chapter:103,difficulty:"hard",question:"A model has: TP=5, TN=900, FP=10, FN=85. Accuracy is 90.5%. Is this a good model?",options:["Yes, 90.5% accuracy is excellent","No ‚Äî Precision is only 33% and Recall is only 5.6%, the model misses almost all positives","Yes, TN is very high so it works well","Cannot determine without more information"],answerIndex:1,explanation:"Despite 90.5% accuracy, Precision = 5/15 = 33% and Recall = 5/90 = 5.6%. The model catches almost none of the positive cases! High accuracy is misleading with imbalanced data.",wrongExplanations:["High accuracy alone is misleading ‚Äî look at Precision and Recall too","High TN just means most negatives are correct, but we miss 94% of positives","We can determine the model is poor from the confusion matrix values"]},

// --- PRECISION VS RECALL ---
{chapter:103,difficulty:"medium",question:"When is Precision more important than Recall?",options:["When False Negatives are costly (e.g., cancer detection)","When False Positives are costly (e.g., spam filter)","When accuracy is above 90%","When the dataset is balanced"],answerIndex:1,explanation:"Precision matters most when False Positives are costly. Example: spam filter ‚Äî marking an important email as spam (FP) is worse than letting a spam through (FN).",wrongExplanations:["When FN is costly, Recall is more important (catch every positive case)","Accuracy above 90% doesn't determine which metric matters more","Dataset balance doesn't determine which metric matters ‚Äî the business cost does"]},
{chapter:103,difficulty:"medium",question:"When is Recall more important than Precision?",options:["When False Positives are costly","When False Negatives are costly (e.g., disease detection)","When we have a small dataset","When training time is limited"],answerIndex:1,explanation:"Recall matters most when False Negatives are costly. Example: cancer detection ‚Äî missing a sick patient (FN) is far worse than a false alarm (FP). We want to catch every positive.",wrongExplanations:["When FP is costly, Precision is more important","Dataset size doesn't determine which metric matters more","Training time is unrelated to which evaluation metric to prioritize"]},
{chapter:103,difficulty:"medium",question:"You're building a model to detect cancer. Which metric matters most?",options:["Accuracy","Precision","Recall","F1 Score"],answerIndex:2,explanation:"Recall is critical for cancer detection. Missing a sick patient (False Negative) could be fatal. We'd rather have false alarms (FP) than miss real cancer cases (FN).",wrongExplanations:["Accuracy can be misleading with imbalanced medical data","Precision focuses on avoiding false alarms, but missing cancer is worse","F1 balances both, but in cancer detection we specifically need to catch every case (Recall)"]},
{chapter:103,difficulty:"hard",question:"What is the F1 Score?",options:["The average of Accuracy and Recall","F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)","F1 = (TP + TN) / Total","F1 = Precision / Recall"],answerIndex:1,explanation:"F1 Score is the harmonic mean of Precision and Recall: F1 = 2√ó(P√óR)/(P+R). Use it when you need to balance both metrics, especially with imbalanced classes.",wrongExplanations:["The average of Accuracy and Recall has no standard name","(TP+TN)/Total is the Accuracy formula, not F1","Precision/Recall is just a ratio, not the F1 Score"]},
{chapter:103,difficulty:"hard",question:"Your model has 95% accuracy but only 20% recall. What does this mean?",options:["The model is excellent and should be deployed","The model misses 80% of actual positive cases ‚Äî likely due to class imbalance","The model has high precision so it's acceptable","The model needs more training epochs only"],answerIndex:1,explanation:"95% accuracy with 20% recall means the model catches only 20% of positive cases (misses 80%). This typically happens with imbalanced data ‚Äî the model just predicts the majority class.",wrongExplanations:["A model that misses 80% of positives should not be deployed","We can't conclude high precision from this information alone","The issue is likely class imbalance or model design, not just training epochs"]},

// --- ROC & AUC ---
{chapter:103,difficulty:"easy",question:"What does the ROC curve plot?",options:["Precision vs Recall","True Positive Rate vs False Positive Rate","Accuracy vs Loss","Training error vs Test error"],answerIndex:1,explanation:"The ROC (Receiver Operating Characteristic) curve plots True Positive Rate (Recall) on the Y-axis vs False Positive Rate on the X-axis at various classification thresholds.",wrongExplanations:["Precision vs Recall is a different curve (PR curve)","Accuracy vs Loss is not what ROC shows","Training vs Test error is a learning curve, not ROC"]},
{chapter:103,difficulty:"easy",question:"What does AUC = 1.0 mean?",options:["The model is random","The model is perfect","The model is worse than random","The model has overfit"],answerIndex:1,explanation:"AUC (Area Under the ROC Curve) = 1.0 means the model perfectly distinguishes between positive and negative classes with zero errors.",wrongExplanations:["AUC = 0.5 indicates random performance, not 1.0","AUC < 0.5 would be worse than random","AUC = 1.0 on test data means perfect performance, not necessarily overfitting"]},
{chapter:103,difficulty:"medium",question:"What does AUC = 0.5 mean?",options:["Perfect model","Good model","Model performs like random guessing","Model is worse than random"],answerIndex:2,explanation:"AUC = 0.5 means the model performs no better than random coin flipping. The ROC curve follows the diagonal line from (0,0) to (1,1).",wrongExplanations:["AUC = 1.0 is a perfect model","AUC = 0.5 is the baseline for random ‚Äî definitely not good","AUC < 0.5 is worse than random"]},
{chapter:103,difficulty:"medium",question:"If the ROC curve is close to the diagonal line, the model is:",options:["Excellent","Good","Poor ‚Äî no better than random guessing","Overfitting"],answerIndex:2,explanation:"The diagonal line represents random guessing (AUC = 0.5). A curve close to it means the model has poor discriminative ability.",wrongExplanations:["Excellent would be a curve hugging the top-left corner","Good would be a curve well above the diagonal","Closeness to diagonal doesn't specifically indicate overfitting ‚Äî just poor performance"]},
{chapter:103,difficulty:"medium",question:"The ideal ROC curve hugs which corner of the plot?",options:["Bottom-right","Top-right","Top-left","Bottom-left"],answerIndex:2,explanation:"The top-left corner represents high True Positive Rate (catching all positives) and low False Positive Rate (few false alarms). This is the ideal scenario.",wrongExplanations:["Bottom-right = low TPR, high FPR ‚Äî the worst scenario","Top-right = high TPR but also high FPR ‚Äî not ideal","Bottom-left = low TPR and low FPR ‚Äî model predicts negative for everything"]},
{chapter:103,difficulty:"hard",question:"True Positive Rate (TPR) is the same as:",options:["Precision","Accuracy","Recall (Sensitivity)","Specificity"],answerIndex:2,explanation:"TPR = TP/(TP+FN), which is exactly the Recall (Sensitivity) formula. It measures what proportion of actual positives the model correctly identifies.",wrongExplanations:["Precision = TP/(TP+FP), which is different from TPR","Accuracy = (TP+TN)/Total, includes all predictions","Specificity = TN/(TN+FP), which measures true negative rate"]},
{chapter:103,difficulty:"hard",question:"False Positive Rate (FPR) is calculated as:",options:["FP / (FP + FN)","FP / (FP + TN)","FP / (FP + TP)","FP / Total"],answerIndex:1,explanation:"FPR = FP/(FP+TN). It measures what proportion of actual negatives were incorrectly flagged as positive. FPR = 1 ‚àí Specificity.",wrongExplanations:["FP/(FP+FN) mixes different categories incorrectly","FP/(FP+TP) is actually (1 ‚àí Precision), not FPR","FP/Total doesn't isolate the negative class"]},

// --- TRAIN/TEST SPLIT & K-FOLD ---
{chapter:103,difficulty:"easy",question:"Why do we split data into training and testing sets?",options:["To make the model train faster","To evaluate performance on unseen data and detect overfitting","To reduce the size of the dataset","To balance the classes"],answerIndex:1,explanation:"We split data to test the model on data it has never seen during training. This reveals how well the model generalizes and helps detect overfitting.",wrongExplanations:["Splitting doesn't make training faster ‚Äî it reduces training data","Splitting doesn't reduce the total dataset ‚Äî it divides it","Class balancing is a separate preprocessing step, not the purpose of splitting"]},
{chapter:103,difficulty:"medium",question:"In K-Fold Cross Validation with K=5, how many times is each data point used for testing?",options:["0 times","1 time","4 times","5 times"],answerIndex:1,explanation:"In 5-Fold CV, the data is split into 5 parts. Each part is used exactly once as the test set and 4 times as part of the training set. Results from all 5 folds are averaged.",wrongExplanations:["Every point is used for testing ‚Äî that's the whole point of K-Fold","4 times is how often each fold is used for training, not testing","5 times would mean testing on all data every fold ‚Äî that defeats the purpose"]},
{chapter:103,difficulty:"medium",question:"What is the advantage of K-Fold Cross Validation over a single train-test split?",options:["It's faster to compute","Every data point is used for both training and testing, giving a more reliable estimate","It always prevents overfitting","It requires less data"],answerIndex:1,explanation:"K-Fold uses all data for both training and testing across folds, reducing dependence on how the data happens to be split. The averaged results are more reliable.",wrongExplanations:["K-Fold is actually slower ‚Äî it trains K separate models","K-Fold reduces evaluation bias but does NOT prevent overfitting in the model itself","K-Fold doesn't change the amount of data needed"]},
{chapter:103,difficulty:"easy",question:"What is overfitting?",options:["Model performs poorly on both training and test data","Model performs well on training data but poorly on new/unseen data","Model trains too slowly","Model has too few features"],answerIndex:1,explanation:"Overfitting = great training performance but poor test/new data performance. The model memorized training data (including noise) instead of learning general patterns.",wrongExplanations:["Poor on both train and test is underfitting (high bias), not overfitting","Training speed is unrelated to overfitting","Too few features would cause underfitting, not overfitting"]},
{chapter:103,difficulty:"medium",question:"What is underfitting?",options:["Model performs well on training but poorly on test data","Model performs poorly on BOTH training and test data","Model takes too long to train","Model has too many features"],answerIndex:1,explanation:"Underfitting means the model is too simple to capture the underlying patterns. It has high bias and performs poorly on both training and test data.",wrongExplanations:["Good on training but poor on test is overfitting, not underfitting","Training time is unrelated to underfitting","Too many features typically causes overfitting, not underfitting"]},
{chapter:103,difficulty:"medium",question:"High variance in a model means:",options:["The model is too simple (underfitting)","The model is too complex (overfitting)","The model has low accuracy","The model trains too slowly"],answerIndex:1,explanation:"High variance = model is too complex, fits training data too closely (including noise), and doesn't generalize well. This is overfitting. High variance = different training sets produce very different models.",wrongExplanations:["Too simple model has high bias, not high variance","High variance doesn't directly mean low accuracy on training data","Training speed is unrelated to bias-variance"]},
{chapter:103,difficulty:"medium",question:"High bias in a model means:",options:["The model is overfitting","The model is too simple (underfitting)","The dataset is biased","The model has too many features"],answerIndex:1,explanation:"High bias = the model makes strong assumptions and oversimplifies, failing to capture the true patterns. This leads to underfitting on both training and test data.",wrongExplanations:["Overfitting is high variance, not high bias","Dataset bias is a data quality issue, different from model bias","Too many features typically causes high variance (overfitting), not high bias"]},

// --- DATA PREPROCESSING ---
{chapter:103,difficulty:"easy",question:"Why is feature scaling important in machine learning?",options:["It makes the model more interpretable","It prevents features with large values from dominating the model","It reduces the number of features","It increases the dataset size"],answerIndex:1,explanation:"Feature scaling ensures all features contribute equally. Without scaling, features with large ranges (e.g., salary: 50000) dominate features with small ranges (e.g., age: 30). Critical for Logistic Regression and distance-based models.",wrongExplanations:["Scaling doesn't affect interpretability ‚Äî it affects training","Scaling doesn't reduce features ‚Äî that's feature selection","Scaling doesn't change the number of data points"]},
{chapter:103,difficulty:"medium",question:"What is the Normalization (Min-Max Scaling) formula?",options:["z = (x ‚àí Œº) / œÉ","x' = (x ‚àí min) / (max ‚àí min)","x' = x / max","x' = (x ‚àí mean) / range"],answerIndex:1,explanation:"Normalization: x' = (x ‚àí min) / (max ‚àí min). This scales all values to the range [0, 1]. min and max are from the feature column.",wrongExplanations:["(x‚àíŒº)/œÉ is Standardization (Z-score), not Normalization","x/max doesn't account for the minimum value","(x‚àímean)/range is not the standard normalization formula"]},
{chapter:103,difficulty:"medium",question:"What is the Standardization (Z-score) formula?",options:["x' = (x ‚àí min) / (max ‚àí min)","z = (x ‚àí Œº) / œÉ","z = x / œÉ","z = (x ‚àí median) / IQR"],answerIndex:1,explanation:"Standardization: z = (x ‚àí Œº) / œÉ, where Œº is the mean and œÉ is the standard deviation. This centers data around mean=0 with standard deviation=1.",wrongExplanations:["(x‚àímin)/(max‚àímin) is Normalization (Min-Max), not Standardization","x/œÉ doesn't center the data around zero","(x‚àímedian)/IQR is a robust scaling method, not standard Standardization"]},
{chapter:103,difficulty:"hard",question:"Should feature scaling be done BEFORE or AFTER the train-test split?",options:["Before splitting ‚Äî scale the entire dataset first","After splitting ‚Äî fit scaler on training data only","It doesn't matter","Before splitting ‚Äî but only for the features, not the labels"],answerIndex:1,explanation:"AFTER splitting! Fit the scaler on training data ONLY, then transform both train and test. Scaling before splitting causes data leakage ‚Äî test data statistics influence training.",wrongExplanations:["Scaling before splitting causes data leakage ‚Äî test set info leaks into training","It absolutely matters ‚Äî wrong order gives overly optimistic results","Even if only scaling features, doing it before split still causes leakage"]},
{chapter:103,difficulty:"hard",question:"What is data leakage?",options:["When data is lost during preprocessing","When information from outside the training set influences model training","When the model loses accuracy over time","When data is duplicated in the dataset"],answerIndex:1,explanation:"Data leakage occurs when information from the test set (or future data) is used during training. Example: scaling before splitting lets test data statistics affect training. This gives overly optimistic results that won't hold on truly new data.",wrongExplanations:["Data loss is a different problem ‚Äî leakage is about information contamination","Accuracy decay over time is concept drift, not data leakage","Data duplication is a data quality issue, not leakage"]},
{chapter:103,difficulty:"medium",question:"In sklearn, why do we use fit_transform() on training data but only transform() on test data?",options:["fit_transform is faster","fit_transform learns parameters from data; transform applies those same parameters","transform doesn't work on training data","There is no difference between them"],answerIndex:1,explanation:"fit_transform() learns the scaling parameters (mean, std) from training data AND applies them. transform() uses those same parameters on test data. This prevents data leakage.",wrongExplanations:["Speed is not the reason ‚Äî it's about preventing data leakage","transform() can work on any data ‚Äî it just applies learned parameters","There IS a crucial difference ‚Äî fit_transform learns + applies, transform only applies"]},
{chapter:103,difficulty:"easy",question:"How can missing values be handled in a dataset?",options:["Only by removing rows with missing values","Remove rows, replace with mean/median, or use predictive models","Missing values must always be kept as-is","Convert them to zero always"],answerIndex:1,explanation:"Three common approaches: (1) Remove rows/columns with missing data, (2) Replace with mean/median/mode, (3) Use predictive models to impute values. The choice depends on the situation.",wrongExplanations:["Removing rows is just one option ‚Äî you can also impute","Keeping missing values as-is will cause errors in most ML algorithms","Converting to zero is a bad practice ‚Äî zero might be a meaningful value"]},
{chapter:103,difficulty:"medium",question:"What percentage of time is typically spent on feature engineering in an ML project?",options:["10-20%","30-40%","50-60%","70-80%"],answerIndex:3,explanation:"70-80% of ML project time is spent on feature engineering: data transformation, scaling, handling missing values, correlation analysis, and feature selection.",wrongExplanations:["10-20% is far too low ‚Äî feature engineering dominates ML workflows","30-40% underestimates the effort","50-60% is still too low ‚Äî most estimates say 70-80%"]},

// --- STATISTICS ---
{chapter:103,difficulty:"medium",question:"What is the correlation coefficient range?",options:["0 to 1","0 to 100","-1 to 1","-100 to 100"],answerIndex:2,explanation:"Correlation coefficient ranges from -1 to +1. +1 = perfect positive correlation, -1 = perfect negative correlation, 0 = no linear correlation.",wrongExplanations:["0 to 1 misses negative correlations","0 to 100 is not the range ‚Äî it's -1 to 1","Correlation is bounded between -1 and 1"]},
{chapter:103,difficulty:"medium",question:"A correlation of -0.85 between two variables means:",options:["No relationship","Weak positive relationship","Strong negative (inverse) relationship","The variables are independent"],answerIndex:2,explanation:"‚àí0.85 is a strong negative correlation. When one variable increases, the other strongly tends to decrease. Values below ‚àí0.5 or above 0.5 indicate notable correlation.",wrongExplanations:["‚àí0.85 is far from 0, so there is a strong relationship","Negative correlation means inverse, not positive","High absolute correlation means strong dependence, not independence"]},
{chapter:103,difficulty:"medium",question:"When do we reject the null hypothesis?",options:["When p-value > 0.05","When p-value < 0.05 (significance level)","When the sample size is large","When variance is high"],answerIndex:1,explanation:"We reject H‚ÇÄ when the p-value is less than the significance level (typically 0.05). This means the observed result is unlikely to have occurred by chance if H‚ÇÄ were true.",wrongExplanations:["p-value > 0.05 means we fail to reject H‚ÇÄ (not enough evidence)","Large sample size alone doesn't determine whether to reject H‚ÇÄ","High variance is about data spread, not hypothesis testing"]},
{chapter:103,difficulty:"hard",question:"What is the difference between a T-test and a Z-test?",options:["T-test is for means, Z-test is for proportions only","T-test: variance unknown, n<30; Z-test: variance known, n>30","T-test is always more accurate","There is no difference"],answerIndex:1,explanation:"T-test: population variance is unknown, sample size < 30, uses Student-t distribution. Z-test: population variance is known, sample size > 30, uses Normal distribution.",wrongExplanations:["Both can test means ‚Äî the distinction is about variance knowledge and sample size","Neither is always more accurate ‚Äî they're used in different situations","There is a clear difference in when each is appropriate"]},
{chapter:103,difficulty:"medium",question:"The Chi-square test is used to:",options:["Predict continuous outcomes","Test independence between variables and check if data is from an unbiased source","Measure correlation between variables","Calculate regression coefficients"],answerIndex:1,explanation:"Chi-square (œá¬≤ = Œ£(O‚àíE)¬≤/E) tests: (1) independence between input and output variables, (2) whether observed data comes from a fair/unbiased source, (3) whether data is 'too good to be true'.",wrongExplanations:["Predicting continuous outcomes is regression","Correlation is measured by the correlation coefficient, not chi-square","Regression coefficients are found using OLS or gradient descent"]},

// --- LINEAR REGRESSION ---
{chapter:103,difficulty:"easy",question:"The univariate linear regression equation is:",options:["y = mx¬≤ + b","y = w‚ÇÅx + w‚ÇÄ","y = e^(w‚ÇÅx)","y = 1/(1+e^(-x))"],answerIndex:1,explanation:"y = w‚ÇÅx + w‚ÇÄ, where w‚ÇÅ is the slope and w‚ÇÄ is the y-intercept. This finds the best-fit straight line through the data points.",wrongExplanations:["y=mx¬≤+b is a quadratic equation, not linear","y=e^(w‚ÇÅx) is an exponential function","y=1/(1+e^(-x)) is the sigmoid function used in Logistic Regression"]},
{chapter:103,difficulty:"medium",question:"R¬≤ = 0.92 means:",options:["The model explains 92% of the variance in the data","The model is 92% accurate","The correlation is 0.92","There is a 92% chance the model is correct"],answerIndex:0,explanation:"R¬≤ (R-squared) = 1 ‚àí SSE/SST measures how much variance the model explains. R¬≤=0.92 means 92% of the data's variability is explained by the model ‚Äî a very good fit.",wrongExplanations:["R¬≤ is not the same as accuracy ‚Äî it measures explained variance","R¬≤ is not the correlation coefficient (r) ‚Äî r¬≤=R¬≤ though","R¬≤ is not a probability ‚Äî it's a measure of fit quality"]},
{chapter:103,difficulty:"medium",question:"In linear regression, the null hypothesis H‚ÇÄ: w‚ÇÅ=0 means:",options:["The model perfectly fits the data","There is no relationship between x and y","The intercept is zero","The model is overfitting"],answerIndex:1,explanation:"H‚ÇÄ: w‚ÇÅ=0 means the slope is zero ‚Äî there's no linear relationship between the input (x) and output (y). If we reject H‚ÇÄ, there IS a significant relationship.",wrongExplanations:["Perfect fit would mean R¬≤=1, not w‚ÇÅ=0","w‚ÇÄ=0 means the intercept is zero ‚Äî different from w‚ÇÅ=0","w‚ÇÅ=0 is about relationship, not overfitting"]},

// --- PYTHON CODING ---
{chapter:103,difficulty:"medium",question:"What is the correct order for an ML pipeline?",options:["Scale ‚Üí Split ‚Üí Train ‚Üí Predict ‚Üí Evaluate","Split ‚Üí Scale (fit on train only) ‚Üí Train ‚Üí Predict ‚Üí Evaluate","Train ‚Üí Split ‚Üí Scale ‚Üí Predict ‚Üí Evaluate","Split ‚Üí Train ‚Üí Scale ‚Üí Predict ‚Üí Evaluate"],answerIndex:1,explanation:"Correct order: Split data FIRST ‚Üí Scale (fit scaler on training data ONLY) ‚Üí Train model ‚Üí Predict on test ‚Üí Evaluate. Scaling before splitting causes data leakage!",wrongExplanations:["Scaling before splitting causes data leakage ‚Äî test info leaks into training","You must split before training ‚Äî can't train on all data then evaluate properly","Scaling after training makes no sense ‚Äî the model already saw unscaled data"]},
{chapter:103,difficulty:"hard",question:"In Python: scaler.fit_transform(X_train) vs scaler.transform(X_test). Why the difference?",options:["fit_transform is for large data, transform for small data","fit_transform learns parameters from training data; transform applies those same parameters to test data","transform is deprecated","There is no practical difference"],answerIndex:1,explanation:"fit_transform() = learn mean/std from X_train AND scale it. transform() = use the SAME mean/std (from training) to scale X_test. Never fit on test data ‚Äî that's data leakage!",wrongExplanations:["Data size is irrelevant ‚Äî it's about preventing data leakage","transform() is not deprecated ‚Äî it's essential for test data","The difference is critical ‚Äî using fit_transform on test data causes data leakage"]},
{chapter:103,difficulty:"medium",question:"In sklearn's confusion_matrix output [[a, b], [c, d]], what does each position represent?",options:["[[TP, FP], [FN, TN]]","[[TN, FP], [FN, TP]]","[[TP, FN], [FP, TN]]","[[FP, TP], [TN, FN]]"],answerIndex:1,explanation:"sklearn's confusion_matrix returns [[TN, FP], [FN, TP]]. Top row = actual negatives (TN and FP). Bottom row = actual positives (FN and TP).",wrongExplanations:["TP is in the bottom-right, not top-left in sklearn","This layout doesn't match sklearn's convention","This layout doesn't match sklearn's convention"]},
{chapter:103,difficulty:"hard",question:"To plot an ROC curve, we need probabilities, not class labels. Which method do we use?",options:["model.predict(X_test)","model.predict_proba(X_test)[:,1]","model.score(X_test)","model.fit(X_test)"],answerIndex:1,explanation:"predict_proba() returns probabilities for each class. We take [:,1] to get the probability of the positive class (class 1). predict() only gives class labels (0 or 1), which aren't enough for ROC.",wrongExplanations:["predict() returns class labels (0/1), not probabilities needed for ROC","score() returns accuracy, not probabilities","fit() trains the model ‚Äî it doesn't make predictions"]},
{chapter:103,difficulty:"medium",question:"What does cross_val_score(model, X, y, cv=10) do?",options:["Trains the model 10 times on the same data","Performs 10-Fold Cross Validation and returns accuracy for each fold","Splits data into 10% test and 90% train","Runs the model on 10 different datasets"],answerIndex:1,explanation:"cross_val_score with cv=10 performs 10-Fold Cross Validation: splits data into 10 folds, trains/tests 10 times, and returns the score (accuracy by default) for each fold.",wrongExplanations:["It trains on different subsets each time, not the same data repeatedly","cv=10 means 10 folds, not 10% test ‚Äî each fold is used once for testing","It uses the same dataset, just splits it differently each fold"]},
{chapter:103,difficulty:"easy",question:"What does train_test_split(X, y, test_size=0.2, random_state=42) do?",options:["Splits data: 20% training, 80% testing","Splits data: 80% training, 20% testing with reproducible results","Trains the model on 20% of data","Creates 42 different splits"],answerIndex:1,explanation:"test_size=0.2 means 20% for testing, 80% for training. random_state=42 sets the random seed so the same split is produced every time ‚Äî ensuring reproducibility.",wrongExplanations:["test_size=0.2 means 20% TEST (not training), so 80% is training","This function splits data ‚Äî it doesn't train any model","random_state=42 is a seed value, not the number of splits"]},

// --- TRICK QUESTIONS / TRUE-FALSE ---
{chapter:103,difficulty:"hard",question:"TRUE or FALSE: 'High accuracy always means a good model.'",options:["TRUE ‚Äî accuracy is the most reliable metric","FALSE ‚Äî with imbalanced data, a useless model can have high accuracy","TRUE ‚Äî if accuracy is above 90%, the model is good","FALSE ‚Äî accuracy is never useful"],answerIndex:1,explanation:"FALSE. With imbalanced data (e.g., 99% class 0), always predicting 0 gives 99% accuracy but catches 0% of class 1 cases. Precision and Recall reveal the true performance.",wrongExplanations:["Accuracy is misleading with imbalanced data ‚Äî it's not always reliable","90% accuracy means nothing if the positive class is being ignored","Accuracy IS useful ‚Äî but not always sufficient alone. The answer is that high accuracy doesn't ALWAYS mean good"]},
{chapter:103,difficulty:"hard",question:"TRUE or FALSE: 'K-Fold Cross Validation eliminates overfitting.'",options:["TRUE ‚Äî K-Fold always prevents overfitting","FALSE ‚Äî it reduces evaluation bias but doesn't eliminate overfitting","TRUE ‚Äî by averaging results, overfitting is impossible","FALSE ‚Äî K-Fold actually causes more overfitting"],answerIndex:1,explanation:"FALSE. K-Fold provides a more reliable EVALUATION of model performance, but the model itself can still overfit. It helps detect overfitting, not eliminate it.",wrongExplanations:["K-Fold helps evaluate, but overfitting can still occur in the model","Averaging results gives a better estimate, but the model can still overfit","K-Fold doesn't cause overfitting ‚Äî but it doesn't eliminate it either"]},
{chapter:103,difficulty:"hard",question:"TRUE or FALSE: 'ROC curve below the diagonal means a good model.'",options:["TRUE ‚Äî below the line means better","FALSE ‚Äî below the diagonal means worse than random guessing","TRUE ‚Äî the diagonal is the worst-case scenario","FALSE ‚Äî there is no diagonal on an ROC curve"],answerIndex:1,explanation:"FALSE. The diagonal (from bottom-left to top-right) represents random guessing (AUC=0.5). Below the diagonal means WORSE than random (AUC<0.5). A good model is ABOVE the diagonal.",wrongExplanations:["Below the diagonal = AUC < 0.5 = worse than flipping a coin","The diagonal IS the worst baseline ‚Äî below it is even worse","There IS a diagonal on the ROC curve ‚Äî it represents random guessing"]},
{chapter:103,difficulty:"hard",question:"TRUE or FALSE: 'Feature scaling should be done before the train-test split.'",options:["TRUE ‚Äî scale everything first for consistency","FALSE ‚Äî scale after splitting, fit scaler on training data only","TRUE ‚Äî it doesn't matter when you scale","FALSE ‚Äî you should never scale features"],answerIndex:1,explanation:"FALSE. Scale AFTER splitting. Fit the scaler on training data ONLY, then transform test data with the same scaler. Scaling before split causes data leakage (test info contaminates training).",wrongExplanations:["Scaling before splitting causes data leakage ‚Äî test statistics leak into training","It absolutely matters ‚Äî wrong order gives unrealistically good performance","Feature scaling is important and should be done ‚Äî just after splitting"]},
{chapter:103,difficulty:"hard",question:"TRUE or FALSE: 'A model with 100% training accuracy is ideal.'",options:["TRUE ‚Äî perfect training means perfect model","FALSE ‚Äî it likely indicates overfitting","TRUE ‚Äî the goal is to minimize training error","FALSE ‚Äî 100% accuracy is mathematically impossible"],answerIndex:1,explanation:"FALSE. 100% training accuracy usually means the model memorized the training data (including noise) rather than learning general patterns. It will likely perform poorly on new data = overfitting.",wrongExplanations:["Perfect training accuracy almost always means overfitting, not a good model","While we want low training error, 100% usually means memorization (overfitting)","100% training accuracy is possible ‚Äî but it's usually a bad sign"]},
{chapter:103,difficulty:"hard",question:"TRUE or FALSE: 'If correlation between two variables is 0, they are independent.'",options:["TRUE ‚Äî zero correlation means complete independence","FALSE ‚Äî zero correlation means no LINEAR relationship, but non-linear relationships may exist","TRUE ‚Äî correlation measures all types of relationships","FALSE ‚Äî correlation cannot be zero"],answerIndex:1,explanation:"FALSE. Correlation measures LINEAR relationships only. Two variables can have zero correlation but still have a strong non-linear relationship (e.g., y = x¬≤).",wrongExplanations:["Zero correlation only rules out linear relationship, not all relationships","Correlation specifically measures LINEAR association only","Correlation can definitely be zero ‚Äî it just means no linear relationship"]},
{chapter:103,difficulty:"hard",question:"TRUE or FALSE: 'Normalization and Standardization are the same thing.'",options:["TRUE ‚Äî both scale the data the same way","FALSE ‚Äî Normalization scales to [0,1], Standardization centers around mean=0 with std=1","TRUE ‚Äî the terms are interchangeable","FALSE ‚Äî Standardization is not a real technique"],answerIndex:1,explanation:"FALSE. Normalization (Min-Max) scales to [0,1] using (x-min)/(max-min). Standardization (Z-score) centers around mean=0, std=1 using (x-Œº)/œÉ. Different formulas, different results.",wrongExplanations:["They use different formulas and produce different results","They are NOT interchangeable ‚Äî each is appropriate for different situations","Both are real and commonly used techniques"]},

// --- EXTRA SCENARIO & CONCEPT QUESTIONS ---
{chapter:103,difficulty:"medium",question:"You trained a model: 99% training accuracy, 60% test accuracy. What's happening?",options:["The model is underfitting","The model is overfitting ‚Äî memorized training data but can't generalize","The test data is corrupted","The model needs more features"],answerIndex:1,explanation:"Large gap between training accuracy (99%) and test accuracy (60%) is the classic sign of overfitting. The model memorized training data but fails on new data.",wrongExplanations:["Underfitting would show poor accuracy on BOTH train and test","Test data corruption is unlikely ‚Äî this pattern is classic overfitting","More features would likely make overfitting worse, not better"]},
{chapter:103,difficulty:"medium",question:"Which is the correct way to handle the sklearn confusion matrix output [[40, 5], [8, 47]]?",options:["TP=40, FP=5, FN=8, TN=47","TN=40, FP=5, FN=8, TP=47","TP=47, FP=8, FN=5, TN=40","TN=47, FP=40, FN=5, TP=8"],answerIndex:1,explanation:"sklearn confusion_matrix: [[TN, FP], [FN, TP]]. So TN=40, FP=5, FN=8, TP=47. Top row = actual negatives, bottom row = actual positives.",wrongExplanations:["TP is in the bottom-right position (47), not top-left (40)","This swaps TN and TP positions ‚Äî doesn't match sklearn layout","This layout is completely incorrect for sklearn"]},
{chapter:103,difficulty:"easy",question:"What are the three main types of machine learning?",options:["Classification, Regression, Clustering","Supervised, Unsupervised, Reinforcement","Training, Testing, Validation","Linear, Logistic, Polynomial"],answerIndex:1,explanation:"The three main types: (1) Supervised (labeled data), (2) Unsupervised (no labels, finds patterns), (3) Reinforcement (learns from rewards/punishments).",wrongExplanations:["Classification, Regression, Clustering are subtypes, not the three main types","Training, Testing, Validation are data splits, not learning types","Linear, Logistic, Polynomial are model types, not learning paradigms"]},
{chapter:103,difficulty:"easy",question:"In supervised learning, the model learns from:",options:["Unlabeled data only","Rewards and punishments","Labeled input-output pairs","Random noise"],answerIndex:2,explanation:"Supervised learning uses labeled input-output pairs (x, y). The 'supervisor' provides the correct output for each input, and the model learns to map inputs to outputs.",wrongExplanations:["Unlabeled data is used in Unsupervised learning","Rewards/punishments are used in Reinforcement learning","Models should learn patterns from data, not random noise"]},
{chapter:103,difficulty:"medium",question:"What is the purpose of the random_state parameter in train_test_split?",options:["It determines the test set size","It sets a random seed for reproducible splits","It shuffles the data randomly each time","It controls the number of folds"],answerIndex:1,explanation:"random_state sets the random seed so the same train-test split is produced every time you run the code. This ensures reproducibility and consistent results for debugging.",wrongExplanations:["Test size is controlled by test_size parameter, not random_state","Setting random_state ensures the SAME split each time, not different shuffles","Number of folds is controlled by cv parameter in cross_val_score"]},
{chapter:103,difficulty:"hard",question:"What does the 'max_iter=10000' parameter do in LogisticRegression()?",options:["Limits the number of training samples","Sets the maximum number of iterations for the optimization algorithm to converge","Limits the number of features used","Sets the maximum number of predictions"],answerIndex:1,explanation:"max_iter sets the maximum iterations for the solver to find optimal weights. If the default (100) isn't enough for convergence, you'll get a warning. 10000 gives more room to converge.",wrongExplanations:["max_iter doesn't limit training samples ‚Äî that's controlled by the data itself","max_iter doesn't affect feature count","max_iter is for training convergence, not for making predictions"]},
{chapter:103,difficulty:"medium",question:"What type of problem is email spam detection?",options:["Regression","Binary classification","Clustering","Reinforcement learning"],answerIndex:1,explanation:"Spam detection is binary classification: each email is classified as either 'spam' (1) or 'not spam' (0). Logistic Regression is commonly used for this.",wrongExplanations:["Regression predicts continuous values ‚Äî spam/not-spam is categorical","Clustering groups data without labels ‚Äî spam detection uses labeled data","Reinforcement learning uses rewards/penalties ‚Äî spam detection uses labeled examples"]},
{chapter:103,difficulty:"medium",question:"Specificity (True Negative Rate) is calculated as:",options:["TP / (TP + FN)","TN / (TN + FP)","TN / (TN + FN)","TP / (TP + FP)"],answerIndex:1,explanation:"Specificity = TN/(TN+FP). It measures the proportion of actual negatives correctly identified. Specificity = 1 ‚àí FPR.",wrongExplanations:["TP/(TP+FN) is Recall (Sensitivity/TPR), not Specificity","TN/(TN+FN) is not a standard metric","TP/(TP+FP) is Precision, not Specificity"]},
{chapter:103,difficulty:"hard",question:"Given Precision=0.8 and Recall=0.6, what is the F1 Score?",options:["0.70","0.686","0.75","0.60"],answerIndex:1,explanation:"F1 = 2√ó(P√óR)/(P+R) = 2√ó(0.8√ó0.6)/(0.8+0.6) = 2√ó0.48/1.4 = 0.96/1.4 ‚âà 0.686. The F1 Score is the harmonic mean, always between the lower and higher value.",wrongExplanations:["0.70 is the arithmetic mean (0.8+0.6)/2, not the harmonic mean","0.75 doesn't match: 2√ó0.48/1.4 = 0.686","0.60 is the Recall value, not the F1 Score"]},
{chapter:103,difficulty:"hard",question:"What is the Ordinary Least Squares (OLS) method?",options:["A classification technique","Finding the line that minimizes the sum of squared differences between actual and predicted values","A clustering algorithm","A method to handle missing values"],answerIndex:1,explanation:"OLS finds the best-fit line by minimizing the sum of squared residuals: Œ£(y_actual ‚àí y_predicted)¬≤. It finds optimal w‚ÇÄ and w‚ÇÅ by setting partial derivatives to zero.",wrongExplanations:["OLS is for regression (finding the best line), not classification","OLS is a regression method, not clustering","OLS is a model fitting method, not a missing value technique"]},
{chapter:103,difficulty:"medium",question:"SST (Total Sum of Squares) equals:",options:["SSR + SSE","SSR ‚àí SSE","SSR √ó SSE","SSR / SSE"],answerIndex:0,explanation:"SST = SSR + SSE. Total variability = Explained variability (Regression) + Unexplained variability (Error). This decomposition is fundamental to understanding R¬≤.",wrongExplanations:["SSR‚àíSSE has no standard interpretation","SSR√óSSE has no standard interpretation","SSR/SSE has no standard interpretation ‚Äî R¬≤ = 1 ‚àí SSE/SST"]},
{chapter:103,difficulty:"hard",question:"A dataset has 10,000 samples: 9,900 class 0 and 100 class 1. A model predicts ALL samples as class 0. What is the accuracy?",options:["50%","99%","0%","100%"],answerIndex:1,explanation:"Accuracy = correct predictions / total = 9900/10000 = 99%. But this model is useless ‚Äî it catches ZERO class 1 cases (Recall=0%). This demonstrates why accuracy alone is misleading with imbalanced data.",wrongExplanations:["50% would require half correct, but predicting all as class 0 gets 9900 right","0% would mean all predictions wrong, but 9900 class 0 predictions are correct","100% would mean no errors, but all 100 class 1 samples are misclassified"]},

// --- DATA VISUALIZATION (from exam.md Part 6) ---
{chapter:103,difficulty:"easy",question:"Why is data visualization important in machine learning?",options:["It makes the report look professional","It helps understand data distribution, detect outliers, and identify relationships between variables","It is required by sklearn before training","It speeds up model training"],answerIndex:1,explanation:"Data visualization helps you: (1) Understand data distribution, (2) Detect outliers, (3) Identify relationships between variables. Visualization is the first step before building any model.",wrongExplanations:["Visualization has practical analytical value, not just cosmetic","sklearn does not require visualization before training","Visualization is for human understanding, it doesn't affect training speed"]},
{chapter:103,difficulty:"easy",question:"Which three plots are most commonly used for data visualization?",options:["Pie chart, bar chart, line chart","Histogram, Scatter Plot, Boxplot","ROC curve, PR curve, learning curve","Heatmap, treemap, sunburst"],answerIndex:1,explanation:"The three most common plots: (1) Histogram ‚Äî shows distribution of a single variable, (2) Scatter Plot ‚Äî shows relationship between two variables, (3) Boxplot ‚Äî shows distribution, median, quartiles, and outliers.",wrongExplanations:["Pie/bar/line charts are general-purpose, not the core ML trio","ROC/PR/learning curves are model evaluation plots, not data exploration","Heatmap/treemap/sunburst are specialized visualizations, not the basics"]},
{chapter:103,difficulty:"medium",question:"What does a histogram show?",options:["The relationship between two variables","The distribution (frequency) of a single variable","The correlation matrix","Model accuracy over time"],answerIndex:1,explanation:"A histogram shows the distribution of a single variable by dividing data into bins and showing the frequency (count) in each bin. It reveals shape, spread, and potential outliers.",wrongExplanations:["Relationship between two variables is shown by a scatter plot","Correlation matrix is shown by a heatmap, not histogram","Accuracy over time is a learning curve, not histogram"]},
{chapter:103,difficulty:"medium",question:"What does a scatter plot show?",options:["Distribution of one variable","The relationship between two variables","Model predictions vs actual","Frequency counts"],answerIndex:1,explanation:"A scatter plot shows the relationship between two variables by plotting data points on X and Y axes. You can see correlation, clusters, and outliers.",wrongExplanations:["Distribution of one variable is shown by a histogram","Predictions vs actual could be on a scatter plot, but the general purpose is showing relationships","Frequency counts are shown by histograms or bar charts"]},
{chapter:103,difficulty:"medium",question:"What does a boxplot show?",options:["Only the mean of the data","The median, quartiles, range, and potential outliers","The frequency of each category","The correlation between variables"],answerIndex:1,explanation:"A boxplot (box-and-whisker) shows: median (center line), Q1 and Q3 (box edges), range (whiskers), and outliers (individual points beyond whiskers). Great for spotting outliers.",wrongExplanations:["Boxplot shows median, not mean, as the central line","Frequency of categories is shown by bar charts","Correlation is shown by scatter plots or heatmaps"]},
{chapter:103,difficulty:"medium",question:"In Python, which line creates a histogram of the first feature?",options:["plt.scatter(X[:,0], y)","plt.hist(X[:,0])","plt.boxplot(X[:,0])","plt.bar(X[:,0])"],answerIndex:1,explanation:"plt.hist(X[:,0]) creates a histogram of the first feature (column 0) of the dataset X. X[:,0] selects all rows of the first column.",wrongExplanations:["plt.scatter creates a scatter plot with two variables, not a histogram","plt.boxplot creates a boxplot, not a histogram","plt.bar creates a bar chart, not a histogram"]},

// --- SKLEARN CODING (from exam.md coding questions) ---
{chapter:103,difficulty:"medium",question:"Which sklearn function loads the breast cancer dataset?",options:["load_iris()","load_breast_cancer()","load_digits()","load_wine()"],answerIndex:1,explanation:"from sklearn.datasets import load_breast_cancer. This is a built-in binary classification dataset (malignant vs benign) commonly used in ML courses for Logistic Regression examples.",wrongExplanations:["load_iris() loads the iris flower dataset (3 classes, not binary)","load_digits() loads handwritten digit images (10 classes)","load_wine() loads a wine classification dataset (3 classes)"]},
{chapter:103,difficulty:"medium",question:"After loading data with load_breast_cancer(), how do you access the features and labels?",options:["X = data[0], y = data[1]","X = data.data, y = data.target","X = data.features, y = data.labels","X = data.X, y = data.y"],answerIndex:1,explanation:"sklearn datasets return a Bunch object. Features are accessed via data.data and labels via data.target. This is standard across all sklearn built-in datasets.",wrongExplanations:["data[0] and data[1] would only work if it were a tuple/list, not a Bunch","data.features and data.labels are not valid attribute names","data.X and data.y are not valid attribute names in sklearn Bunch"]},
{chapter:103,difficulty:"hard",question:"What does this code do: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",options:["Splits data into 20% train and 80% test","Splits data into 80% train and 20% test with a fixed random seed","Creates 42 different random splits","Splits features and labels into 20 groups"],answerIndex:1,explanation:"test_size=0.2 means 20% goes to test, 80% to train. random_state=42 fixes the random seed for reproducibility. Returns 4 arrays: training features, test features, training labels, test labels.",wrongExplanations:["test_size=0.2 means 20% test (not train), so training is 80%","random_state=42 is a seed number, not the number of splits","The function creates one split into train/test, not 20 groups"]},
{chapter:103,difficulty:"medium",question:"What does model.fit(X_train, y_train) do?",options:["Makes predictions on training data","Trains the model by learning patterns from the training data","Evaluates the model on training data","Scales the training data"],answerIndex:1,explanation:"model.fit() is the training step. It feeds the training features (X_train) and labels (y_train) to the model so it can learn the weights/parameters that best map inputs to outputs.",wrongExplanations:["Making predictions is done with model.predict(), not fit()","Evaluation is done with metrics functions, not fit()","Scaling is done with StandardScaler, not model.fit()"]},
{chapter:103,difficulty:"medium",question:"What does model.predict(X_test) return?",options:["Probabilities between 0 and 1","Class labels (e.g., 0 or 1) for each test sample","The model's accuracy score","The confusion matrix"],answerIndex:1,explanation:"model.predict() returns the predicted class labels (0 or 1) for each sample in X_test. For probabilities, use model.predict_proba() instead.",wrongExplanations:["Probabilities are returned by predict_proba(), not predict()","Accuracy is calculated with accuracy_score(), not predict()","Confusion matrix is from confusion_matrix(), not predict()"]},
{chapter:103,difficulty:"hard",question:"What is the difference between model.predict() and model.predict_proba()?",options:["No difference ‚Äî they return the same thing","predict() returns class labels (0/1); predict_proba() returns probability for each class","predict() is for training; predict_proba() is for testing","predict() is faster; predict_proba() is more accurate"],answerIndex:1,explanation:"predict() returns discrete class labels (0 or 1). predict_proba() returns an array of probabilities for each class ‚Äî needed for ROC curves. predict_proba(X)[:,1] gives the probability of class 1.",wrongExplanations:["They return different things ‚Äî labels vs probabilities","Both are for prediction/testing, not training","Speed and accuracy are not the distinction ‚Äî it's output format"]},
{chapter:103,difficulty:"medium",question:"Which import is needed to train a Logistic Regression model?",options:["from sklearn.linear_model import LinearRegression","from sklearn.linear_model import LogisticRegression","from sklearn.tree import LogisticRegression","from sklearn.metrics import LogisticRegression"],answerIndex:1,explanation:"LogisticRegression lives in sklearn.linear_model (same module as LinearRegression). Import: from sklearn.linear_model import LogisticRegression.",wrongExplanations:["LinearRegression is a different model ‚Äî for regression, not classification","sklearn.tree contains DecisionTreeClassifier, not LogisticRegression","sklearn.metrics contains evaluation functions, not model classes"]},
{chapter:103,difficulty:"medium",question:"Which import is needed for confusion_matrix, accuracy_score, precision_score, recall_score?",options:["from sklearn.model_selection import *","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score","from sklearn.preprocessing import *","from sklearn.linear_model import *"],answerIndex:1,explanation:"All evaluation metrics live in sklearn.metrics. This module contains confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, and more.",wrongExplanations:["sklearn.model_selection has train_test_split and cross_val_score, not metrics","sklearn.preprocessing has StandardScaler, not evaluation metrics","sklearn.linear_model has models like LogisticRegression, not metrics"]},
{chapter:103,difficulty:"hard",question:"What does StandardScaler() do internally?",options:["Scales features to range [0, 1]","Subtracts the mean and divides by standard deviation (z-score)","Divides each value by the maximum","Removes outliers from the data"],answerIndex:1,explanation:"StandardScaler computes z = (x ‚àí mean) / std for each feature. It centers data around mean=0 with standard deviation=1. This is Standardization, not Normalization.",wrongExplanations:["Scaling to [0,1] is MinMaxScaler (Normalization), not StandardScaler","Dividing by max is MaxAbsScaler, not StandardScaler","StandardScaler transforms values, it doesn't remove outliers"]},
{chapter:103,difficulty:"hard",question:"What is the correct Python code to plot an ROC curve?",options:["plt.plot(y_test, y_pred)","y_prob = model.predict_proba(X_test)[:,1]; fpr, tpr, _ = roc_curve(y_test, y_prob); plt.plot(fpr, tpr)","plt.plot(precision, recall)","plt.scatter(y_test, y_pred)"],answerIndex:1,explanation:"Steps: (1) Get probabilities with predict_proba()[:,1], (2) Compute FPR and TPR with roc_curve(), (3) Plot FPR vs TPR. You must use probabilities, not class labels.",wrongExplanations:["Plotting y_test vs y_pred is a prediction comparison, not an ROC curve","Plotting precision vs recall is a PR curve, not ROC","A scatter plot of test vs pred is not an ROC curve"]},
{chapter:103,difficulty:"medium",question:"In roc_curve(y_test, y_prob), what does y_prob represent?",options:["The predicted class labels (0 or 1)","The predicted probabilities of the positive class","The actual test labels","The model's accuracy at each threshold"],answerIndex:1,explanation:"y_prob should be the predicted probabilities for the positive class (class 1), obtained via model.predict_proba(X_test)[:,1]. The roc_curve function needs probabilities to compute TPR/FPR at different thresholds.",wrongExplanations:["Class labels (0/1) are not enough ‚Äî ROC needs probabilities to vary the threshold","y_test is the actual labels, not y_prob","Accuracy at each threshold is what the ROC curve helps visualize, not the input"]},

// --- MORE CALCULATION PRACTICE (from exam.md) ---
{chapter:103,difficulty:"hard",question:"Given: TP=60, TN=20, FP=5, FN=15. What is the Precision?",options:["80%","85%","92.3%","75%"],answerIndex:2,explanation:"Precision = TP/(TP+FP) = 60/(60+5) = 60/65 = 0.923 = 92.3%. Of all positive predictions, 92.3% were actually positive ‚Äî very precise!",wrongExplanations:["80% is the Accuracy: (60+20)/100","85% doesn't match any metric here","75% doesn't match: 60/65 = 0.923"]},
{chapter:103,difficulty:"hard",question:"Given: TP=60, TN=20, FP=5, FN=15. What is the Recall?",options:["75%","80%","85%","92.3%"],answerIndex:1,explanation:"Recall = TP/(TP+FN) = 60/(60+15) = 60/75 = 0.80 = 80%. Of all actual positives, we caught 80%.",wrongExplanations:["75% doesn't match: 60/75 = 0.80","85% doesn't match any metric here","92.3% is the Precision, not Recall"]},
{chapter:103,difficulty:"hard",question:"Given: TP=25, TN=60, FP=5, FN=10. Calculate the Accuracy.",options:["75%","80%","85%","90%"],answerIndex:2,explanation:"Accuracy = (TP+TN)/(Total) = (25+60)/(25+60+5+10) = 85/100 = 0.85 = 85%.",wrongExplanations:["75% would require 75 correct out of 100","80% would require 80 correct out of 100","90% would require 90 correct out of 100"]},
{chapter:103,difficulty:"hard",question:"Given: TP=25, TN=60, FP=5, FN=10. What is the Precision and Recall?",options:["Precision=83.3%, Recall=71.4%","Precision=71.4%, Recall=83.3%","Precision=80%, Recall=75%","Precision=83.3%, Recall=62.5%"],answerIndex:0,explanation:"Precision = TP/(TP+FP) = 25/(25+5) = 25/30 = 83.3%. Recall = TP/(TP+FN) = 25/(25+10) = 25/35 = 71.4%.",wrongExplanations:["This reverses Precision and Recall ‚Äî Precision uses FP, Recall uses FN","80% and 75% don't match: 25/30=83.3%, 25/35=71.4%","62.5% doesn't match: 25/35=71.4%, not 25/40"]},

// --- SIGMOID & WHY QUESTIONS ---
{chapter:103,difficulty:"medium",question:"Why is the sigmoid function specifically chosen for Logistic Regression?",options:["It is the fastest function to compute","It converts any linear output into a probability between 0 and 1","It always produces integers","It minimizes MSE automatically"],answerIndex:1,explanation:"The sigmoid function maps any real-valued input to a value between 0 and 1, which can be interpreted as a probability. This is essential for classification ‚Äî we need probabilities to decide class membership.",wrongExplanations:["Speed is not the reason ‚Äî sigmoid is chosen for its mathematical properties","Sigmoid produces continuous values between 0 and 1, not integers","Sigmoid is used with Log Loss, not MSE ‚Äî that's exactly why we don't use MSE"]},
{chapter:103,difficulty:"hard",question:"If the sigmoid output is exactly 0.5, what does this mean?",options:["The model is certain it is class 1","The model is certain it is class 0","The model is maximally uncertain ‚Äî equally likely to be either class","The model has an error"],answerIndex:2,explanation:"sigmoid(z)=0.5 when z=0. The model assigns equal probability to both classes ‚Äî maximum uncertainty. At the default threshold of 0.5, this is exactly on the decision boundary.",wrongExplanations:["0.5 probability means 50/50 ‚Äî not certain of class 1","0.5 probability means 50/50 ‚Äî not certain of class 0","This is normal behavior, not an error ‚Äî it just means the model can't decide"]},

// --- PROFESSOR EXAM STRUCTURE PREDICTION ---
{chapter:103,difficulty:"easy",question:"According to exam predictions, which topic is MOST likely to appear on the exam?",options:["Data Visualization","Confusion Matrix calculations","Reinforcement Learning details","Natural Language Processing"],answerIndex:1,explanation:"Confusion Matrix (TP, TN, FP, FN) and calculating Accuracy, Precision, Recall is almost GUARANTEED on any ML exam. Professors love calculation questions that test if you know the formulas.",wrongExplanations:["Data visualization is important but less likely to be a major question","RL details are a separate topic, not the focus of a Logistic Regression exam","NLP is a different module, not the core of this exam"]},
{chapter:103,difficulty:"medium",question:"A professor asks: 'What is the CORRECT order for building an ML model?' Which answer is correct?",options:["Train model, then split data, then scale features","Scale all data, then split, then train model","Split data, then scale (fit on train only), then train model","Split data, then train model, then scale features"],answerIndex:2,explanation:"Correct order: (1) SPLIT data into train/test, (2) SCALE features (fit scaler on training data only, transform both), (3) TRAIN model on scaled training data. This prevents data leakage.",wrongExplanations:["Training before splitting means you evaluate on data the model already saw","Scaling before splitting causes data leakage ‚Äî test statistics leak into training","Scaling after training means the model trained on unscaled data, then you scale for nothing"]},
{chapter:103,difficulty:"medium",question:"Which sklearn module contains train_test_split and cross_val_score?",options:["sklearn.metrics","sklearn.preprocessing","sklearn.model_selection","sklearn.linear_model"],answerIndex:2,explanation:"sklearn.model_selection contains data splitting and validation tools: train_test_split, cross_val_score, KFold, GridSearchCV, etc.",wrongExplanations:["sklearn.metrics has evaluation functions (accuracy_score, precision_score, etc.)","sklearn.preprocessing has scaling tools (StandardScaler, MinMaxScaler, etc.)","sklearn.linear_model has model classes (LogisticRegression, LinearRegression, etc.)"]},
{chapter:103,difficulty:"medium",question:"What does cross_val_score return when cv=10?",options:["A single accuracy value","An array of 10 accuracy scores (one per fold)","The confusion matrix for each fold","The best model from 10 folds"],answerIndex:1,explanation:"cross_val_score returns an array of scores (accuracy by default), one for each fold. With cv=10, you get 10 scores. Use scores.mean() to get the average performance.",wrongExplanations:["It returns 10 scores (one per fold), not a single value","It returns scores, not confusion matrices","It returns scores, not a trained model"]},
{chapter:103,difficulty:"hard",question:"In the breast cancer dataset, what does the model classify?",options:["Whether a patient has any disease","Whether a tumor is malignant (cancerous) or benign (not cancerous)","The stage of cancer (1-4)","The probability of developing cancer"],answerIndex:1,explanation:"load_breast_cancer() is a binary classification dataset: class 0 = malignant (cancerous), class 1 = benign (not cancerous). It has 30 features computed from cell nuclei images.",wrongExplanations:["It specifically classifies breast tumors, not any disease","It's binary (malignant/benign), not multi-class staging","It classifies existing tumors, not future probability"]},
{chapter:103,difficulty:"medium",question:"What does plt.xlabel(), plt.ylabel(), plt.title() do when plotting?",options:["They train the model with labeled data","They add labels to the X-axis, Y-axis, and chart title for readability","They filter the data being plotted","They change the scale of the axes"],answerIndex:1,explanation:"These matplotlib functions add text labels to your plot: xlabel labels the X-axis, ylabel labels the Y-axis, and title adds a title at the top. Essential for readable visualizations.",wrongExplanations:["These are plot formatting functions, not model training","They add text labels, they don't filter data","They add labels, they don't change the axis scale"]},

// --- ADDITIONAL CONCEPT QUESTIONS ---
{chapter:103,difficulty:"medium",question:"What is a convex function and why does it matter for loss functions?",options:["A function with multiple local minima ‚Äî harder to optimize","A function with a single global minimum ‚Äî guarantees finding the best solution","A function that only increases","A function that is always positive"],answerIndex:1,explanation:"A convex function has a single global minimum (bowl shape). This guarantees optimization algorithms will find the best solution. Log Loss is convex with sigmoid; MSE with sigmoid is NOT convex ‚Äî that's why we use Log Loss.",wrongExplanations:["Multiple local minima is a NON-convex function ‚Äî the opposite","A convex function can decrease then increase (bowl shape), not only increase","Convex functions can have negative values ‚Äî convexity is about shape, not sign"]},
{chapter:103,difficulty:"hard",question:"What is regularization in machine learning?",options:["Making the data more regular (evenly distributed)","Adding a penalty for model complexity to prevent overfitting","Removing outliers from the dataset","Normalizing the features"],answerIndex:1,explanation:"Regularization adds a penalty term to the loss function that discourages overly complex models (large weights). This helps prevent overfitting by keeping the model simpler.",wrongExplanations:["Regularization is about model constraints, not data distribution","Removing outliers is a data preprocessing step, not regularization","Normalizing features is feature scaling, not regularization"]},
{chapter:103,difficulty:"medium",question:"If a professor asks you to 'evaluate your model', what metrics should you compute?",options:["Only accuracy","Accuracy, Precision, Recall, and optionally F1 Score and ROC/AUC","Only the loss function value","Only the training time"],answerIndex:1,explanation:"Full model evaluation includes: Accuracy (overall correctness), Precision (positive prediction quality), Recall (positive detection rate), F1 (balance of P and R), and ROC/AUC (overall discriminative ability).",wrongExplanations:["Only accuracy is insufficient ‚Äî especially with imbalanced data","Loss value is used during training, not typically for final evaluation","Training time measures efficiency, not model quality"]},
{chapter:103,difficulty:"medium",question:"What happens if you forget to set max_iter in LogisticRegression and the default (100) isn't enough?",options:["The model returns random predictions","sklearn raises a ConvergenceWarning ‚Äî the model may not have found optimal weights","The program crashes with an error","Nothing ‚Äî it works fine"],answerIndex:1,explanation:"sklearn issues a ConvergenceWarning: 'Maximum number of iterations reached.' The model still returns predictions, but they may not be optimal since the solver didn't finish finding the best weights.",wrongExplanations:["The model still uses whatever weights it found so far, not random","It's a warning, not a crash ‚Äî the code continues running","The warning indicates a real issue ‚Äî the model may underperform"]}
];

const studyNotes = {
    1: {title:"Chapter 1: Introduction", notes:["AI has four approaches: thinking humanly, acting humanly, thinking rationally, acting rationally","The Turing Test evaluates machine intelligence through natural language conversation","Strong AI claims genuine understanding; Weak AI simulates intelligent behavior","John McCarthy coined 'Artificial Intelligence' at the 1956 Dartmouth Conference","AI Winters: periods of reduced funding (1974-1980, 1987-1993)","Key figures: Turing, McCarthy, Minsky, Simon, Newell, Shannon"]},
    2: {title:"Chapter 2: Intelligent Agents", notes:["Agent = perceives via sensors, acts via actuators","PEAS framework: Performance, Environment, Actuators, Sensors","Agent types: simple reflex, model-based reflex, goal-based, utility-based, learning","Environment properties: observable, deterministic, episodic, static, discrete, single/multi-agent","Rational agent maximizes expected performance given available information","Learning agent components: performance element, critic, learning element, problem generator"]},
    3: {title:"Chapter 3: Problem Solving by Searching", notes:["Problem = initial state + actions + transition model + goal test + path cost","Uninformed: BFS (complete, optimal for unit costs), DFS (space-efficient), UCS, IDS","Informed: Greedy best-first, A* with f(n)=g(n)+h(n)","Admissible heuristic: never overestimates actual cost to goal","Consistent heuristic: h(n) ‚â§ c(n,a,n') + h(n'), implies admissibility","IDS combines BFS completeness with DFS space efficiency"]},
    19: {title:"Chapter 19: Learning from Examples", notes:["Supervised: learns from labeled input-output pairs","Classification (discrete) vs Regression (continuous) outputs","Overfitting: model fits training noise, poor generalization","Decision trees: split on attributes maximizing information gain","Regularization: penalize complexity to prevent overfitting","Cross-validation: estimate generalization by train/test on subsets"]},
    21: {title:"Chapter 21: Reinforcement Learning", notes:["RL: learn from rewards/penalties through environment interaction","Policy œÄ(s): mapping states to actions","Q(s,a): expected cumulative reward for action a in state s","Bellman equation: Q(s,a) = R + Œ≥¬∑max Q(s',a')","Exploration vs exploitation: try new actions vs use best known","TD learning: update estimates from successive predictions without waiting for episode end"]},
    23: {title:"Chapter 23: Natural Language Processing", notes:["NLP pipeline: tokenization ‚Üí POS tagging ‚Üí parsing ‚Üí semantics ‚Üí pragmatics","Language models: P(word sequence) or P(next word | context)","Word embeddings: dense vectors capturing semantic similarity (Word2Vec, GloVe)","NER: identify named entities (persons, organizations, locations)","Attention: dynamically focus on relevant input parts","Transformers: self-attention enables parallel sequence processing"]},
    25: {title:"Chapter 25: Robotics", notes:["Robot: physical agent with sensors and actuators in real world","Sensors: cameras, LIDAR, sonar, encoders, IMU, GPS","Configuration space (C-space): all possible robot configurations","SLAM: Simultaneous Localization and Mapping","Forward kinematics: joint angles ‚Üí end-effector pose","Inverse kinematics: desired pose ‚Üí required joint angles (often harder)"]},
    27: {title:"Chapter 27: Philosophy, Ethics & Safety", notes:["AI ethics: fairness, accountability, transparency, privacy, safety","Algorithmic bias: unfair outcomes from biased data/design","Alignment problem: ensuring AI pursues intended human goals","Explainable AI (XAI): make decisions interpretable","Control problem: maintaining human control over powerful AI","Value alignment: AI systems should understand and optimize for human values"]},
    100: {title:"Quiz 1 Intro: Modules 1-4 Review", notes:["4 AI definition categories: Think Humanly, Act Humanly, Think Rationally, Act Rationally","Two dimensions: Human vs Rational, Thought vs Behavior","Turing Test needs 4 capabilities: NLP, Knowledge Representation, Automated Reasoning, Machine Learning","Total Turing Test adds: Computer Vision and Robotics","5 AI disciplines: NLP, Automated Reasoning, ML, Computer Vision, Robotics","ELIZA (1966) fooled humans using simple pattern matching","Uninformed search: BFS (shortest path), DFS (deepest first), UCS (minimum cost)","Informed search: Greedy (best heuristic), A* = g(n) + h(n)","Admissible heuristic: never overestimates actual cost ‚Äî guarantees A* optimality","AI risks: bias, surveillance, autonomous weapons, misuse across all sectors","Agent = perceives environment + acts autonomously to achieve goals","State space = set of all possible states the system can be in"]},
    101: {title:"Quiz 2: Machine Learning (Module 5)", notes:["ML = machine learns from data without explicit programming","3 types: Supervised (labeled), Unsupervised (no labels), Reinforcement (rewards/punishments)","Supervised: learns function h ‚âà f from input-output pairs (x, y)","Classification = discrete output, Regression = continuous output","Bias-Variance trade-off: simple model underfits (high bias), complex overfits (high variance)","Loss functions: L1 = |y‚àí≈∑|, L2 = (y‚àí≈∑)¬≤, L0/1 = 0 if correct else 1","Feature engineering takes 70-80% of ML project time","Correlation coefficient range: -1 to 1 (positive = direct, negative = inverse)","Reject H‚ÇÄ when p-value < significance level (typically 0.05)","T-test: variance unknown, n<30 | Z-test: variance known, n>30","Chi-square: œá¬≤ = Œ£(O‚àíE)¬≤/E tests independence and data fairness","Linear regression: y = w‚ÇÅx + w‚ÇÄ, minimize L2 loss (OLS method)","R¬≤ = 1 ‚àí SSE/SST measures model fit (1 = perfect, 0 = no explanation)","H‚ÇÄ: w‚ÇÅ=0 (no relationship) vs H‚Çê: w‚ÇÅ‚â†0 (relationship exists)","Multivariate regression risk: overfitting in high-dimensional spaces"]},
    102: {title:"MIDTERM Exam Prep ‚Äî Complete Study Notes (Modules 1-5)", notes:[
        "‚ïê‚ïê‚ïê MODULE 1: INTRODUCTION TO AI ‚ïê‚ïê‚ïê",
        "4 AI definition categories: Think Humanly, Act Humanly, Think Rationally, Act Rationally",
        "2 Dimensions: Human vs Rational AND Thought vs Behavior",
        "Turing Test (1950, Alan Turing): Can machines think? Pass = fool interrogator 30%+ of the time",
        "Standard Turing Test needs 4 capabilities: NLP, Knowledge Representation, Automated Reasoning, Machine Learning",
        "Total Turing Test adds 2 more: Computer Vision + Robotics (physical interaction with objects)",
        "5 AI Disciplines: NLP, Automated Reasoning, Machine Learning, Computer Vision, Robotics",
        "AI Risks: biased decisions, surveillance, autonomous weapons, economic disruption, data misuse",
        "ELIZA (1966) = early chatbot that fooled humans using simple pattern matching",
        "Contributing fields: Philosophy, Math, Psychology, Neuroscience, Control Theory, Computer Engineering",
        "‚ïê‚ïê‚ïê MODULE 2: INTELLIGENT AGENTS ‚ïê‚ïê‚ïê",
        "Agent = perceives environment via SENSORS, acts via ACTUATORS",
        "Agent function = maps percept sequences ‚Üí actions (the abstract behavior description)",
        "Rational agent = maximizes expected performance measure",
        "5 Agent Types: Simple Reflex | Model-Based | Goal-Based | Utility-Based | Learning",
        "Simple Reflex: 'If condition then action' ‚Äî no memory, no model",
        "Model-Based: maintains internal state of how the world evolves ‚Äî handles partial observability",
        "Goal-Based: works toward specific goals ‚Äî plans actions to achieve objectives",
        "Utility-Based: evaluates quality ‚Äî sequences may be 'quicker, safer, more reliable, cheaper'",
        "Learning: improves from experience ‚Äî has 'problem generator' element ‚Äî ANY agent can be this type",
        "PEAS = Performance, Environment, Actuators, Sensors",
        "Environment properties: Observable, Deterministic, Agents, Static/Dynamic, Discrete/Continuous",
        "Soccer PEAS: P=goals/wins | E=field/ball/players | A=legs/head | S=eyes/ears | Partially obs, stochastic, multi, dynamic",
        "Chess PEAS: P=win/loss | E=8x8 board/pieces | A=move pieces | S=see board | Fully obs, deterministic, multi, sequential",
        "Auto Vehicle PEAS: P=safe arrival/time | E=roads/traffic | A=steering/brakes | S=cameras/LIDAR/GPS | Partial, stochastic, multi, dynamic",
        "‚ïê‚ïê‚ïê MODULE 3: UNINFORMED SEARCH ‚ïê‚ïê‚ïê",
        "State space = set of ALL possible configurations the system can be in",
        "Search problem = initial state + actions + transition model + goal test + path cost",
        "BFS: Queue (FIFO), pop(0) | Complete: Yes | Optimal: Yes (unweighted) | Time: O(b^d) | Space: O(b^d)",
        "DFS: Stack (LIFO), pop() | Complete: No | Optimal: No | Time: O(b^m) | Space: O(b¬∑m) ‚Äî LINEAR!",
        "UCS: Priority Queue | Complete: Yes | Optimal: Yes | Expands lowest-cost node first",
        "UCS reduces to BFS when all step costs are equal",
        "BFS after dequeuing non-goal node ‚Üí enqueue neighbors and update predecessors",
        "VISITED SET prevents infinite loops ‚Äî without it, cycles cause the algorithm to run forever",
        "DFS neighbor push order matters: stack is LIFO, so last pushed = first explored (check lab notes!)",
        "KEY CODE: BFS uses queue.pop(0) [first element] | DFS uses stack.pop() [last element]",
        "BFS output for graph A‚Üí[B,C], B‚Üí[D,E], C‚Üí[F]: A, B, C, D, E, F (level by level)",
        "DFS output for same graph: A, C, F, B, E, D (pop() takes last = C before B)",
        "‚ïê‚ïê‚ïê MODULE 4: INFORMED SEARCH ‚ïê‚ïê‚ïê",
        "Informed search uses HEURISTICS (domain-specific estimates) to guide search",
        "Heuristic h(n) = estimated cost from node n to goal",
        "Admissible heuristic: NEVER overestimates actual cost ‚Üí h(n) ‚â§ true cost (guarantees A* optimality)",
        "Consistent heuristic: h(n) ‚â§ cost(n,n') + h(n') ‚Äî triangle inequality (implies admissible)",
        "Greedy Search: f(n) = h(n) ONLY | Not complete | Not optimal | Ignores actual path cost",
        "A* Search: f(n) = g(n) + h(n) | Complete: Yes | Optimal: Yes (with admissible h)",
        "g(n) = actual path cost from START to n | h(n) = estimated cost from n to GOAL",
        "To convert A* ‚Üí Greedy: remove g(n), use only f(n) = h(n)",
        "To convert A* ‚Üí UCS: remove h(n), use only f(n) = g(n)",
        "A* example: node A (g=2, h=3 ‚Üí f=5), node B (g=4, h=1 ‚Üí f=5) ‚Üí tie, expand either",
        "Good heuristic: easy to compute + admissible (no overestimate) + informative (guides efficiently)",
        "‚ïê‚ïê‚ïê MODULE 5: MACHINE LEARNING ‚ïê‚ïê‚ïê",
        "ML = ability of machine to expand knowledge WITHOUT human intervention",
        "Traditional approach: rules ‚Üí logic | ML approach: data ‚Üí discovers rules/logic",
        "3 Types: Supervised (labeled pairs), Unsupervised (no labels/clustering), Reinforcement (rewards)",
        "Supervised: learns h ‚âà f from (x,y) pairs | 4 steps: split ‚Üí train ‚Üí test ‚Üí compare",
        "Classification = categorical output (spam/not spam) | Regression = continuous output (temperature)",
        "Overfitting: good on train, BAD on test ‚Äî high variance, memorizes noise",
        "Underfitting: BAD on BOTH train and test ‚Äî high bias, too simple",
        "Bias-Variance tradeoff: balance simplicity vs complexity for best generalization",
        "ML Challenges: insufficient data ‚úì, poor quality ‚úì, overfitting ‚úì, underfitting ‚úì | NOT: scarcity of algorithms ‚úó, scarcity of measures ‚úó",
        "Feature engineering takes 70-80% of ML project time",
        "Loss functions: L1=|y‚àí≈∑|, L2=(y‚àí≈∑)¬≤, L0/1=0 if correct else 1",
        "Linear regression: y = w‚ÇÅx + w‚ÇÄ | H‚ÇÄ: w‚ÇÅ=0 (no relationship) | Reject when p < 0.05",
        "R¬≤ = 1 ‚àí SSE/SST | Closer to 1 = better fit | Measures explained variance",
        "Correlation: -1 to +1 | Above 0.5 or below -0.5 = notable | Negative = inverse relationship",
        "‚ïê‚ïê‚ïê KEY FORMULAS ‚ïê‚ïê‚ïê",
        "A* formula: f(n) = g(n) + h(n) | Greedy: f(n) = h(n) | UCS: f(n) = g(n)",
        "Linear regression: y = w‚ÇÅx + w‚ÇÄ | R¬≤ = 1 ‚àí (SSE/SST)",
        "L2 loss: (y ‚àí ≈∑)¬≤ | Z-test: Z = (Am ‚àí A‚ÇÄ)/(œÉ/‚àön) | Chi-square: œá¬≤ = Œ£(O‚àíE)¬≤/E",
        "BFS complexity: Time O(b^d), Space O(b^d) | DFS complexity: Time O(b^m), Space O(b¬∑m)",
        "‚ïê‚ïê‚ïê HIGH-CONFIDENCE MIDTERM PREDICTIONS ‚ïê‚ïê‚ïê",
        "‚úÖ Agent function fill-in-blank | ‚úÖ Agent type matching (5 types)",
        "‚úÖ PEAS description for an activity (Soccer/Vehicle/Chess/Medical)",
        "‚úÖ BFS/DFS operation steps and neighbor ordering",
        "‚úÖ Code question: pop(0) vs pop(), BFS/DFS pseudocode, output prediction",
        "‚úÖ A* formula f(n)=g(n)+h(n) and admissible heuristic definition",
        "‚úÖ Traditional vs ML comparison | ‚úÖ ML challenges checklist",
        "‚úÖ Overfitting vs Underfitting | ‚úÖ 3 types of ML",
        "‚úÖ Algorithm comparison: BFS vs DFS vs Greedy vs A*",
        "‚úÖ Linear regression equation and R¬≤ | ‚úÖ Visited set purpose"
    ]},
    103: {title:"FINAL EXAM Prep ‚Äî ML & Logistic Regression", notes:[
        "‚ïê‚ïê‚ïê LOGISTIC REGRESSION ‚ïê‚ïê‚ïê",
        "Logistic Regression = CLASSIFICATION (not regression despite the name!)",
        "Sigmoid: œÉ(z) = 1/(1+e^(‚àíz)) ‚Üí maps any real number to range [0, 1]",
        "Decision boundary: threshold (usually 0.5) where prediction switches class",
        "Log Loss = ‚àí[y¬∑log(p) + (1‚àíy)¬∑log(1‚àíp)] ‚Äî convex, penalizes confident wrong predictions",
        "Why not MSE? Sigmoid is non-linear ‚Üí MSE creates non-convex problem ‚Üí use Log Loss instead",
        "Linear Reg: continuous output, MSE loss | Logistic Reg: probability output [0,1], Log Loss",
        "‚ïê‚ïê‚ïê CONFUSION MATRIX (MUST KNOW) ‚ïê‚ïê‚ïê",
        "TP = predicted +, actual + | TN = predicted ‚àí, actual ‚àí | FP = predicted +, actual ‚àí | FN = predicted ‚àí, actual +",
        "FP = Type I Error (false alarm) | FN = Type II Error (miss)",
        "sklearn confusion_matrix: [[TN, FP], [FN, TP]] ‚Äî memorize this layout!",
        "‚ïê‚ïê‚ïê KEY FORMULAS ‚ïê‚ïê‚ïê",
        "Accuracy = (TP+TN) / (TP+TN+FP+FN) ‚Äî 'of ALL predictions, how many correct?'",
        "Precision = TP / (TP+FP) ‚Äî 'of POSITIVE predictions, how many actually positive?'",
        "Recall = TP / (TP+FN) ‚Äî 'of ACTUAL positives, how many did we catch?'",
        "F1 = 2√ó(P√óR)/(P+R) ‚Äî harmonic mean, balances Precision and Recall",
        "Specificity = TN / (TN+FP) | FPR = FP / (FP+TN) = 1‚àíSpecificity",
        "TPR = Recall = Sensitivity = TP / (TP+FN)",
        "‚ïê‚ïê‚ïê PRECISION vs RECALL ‚ïê‚ïê‚ïê",
        "Precision matters when FP is costly: spam filter (don't mark good emails as spam)",
        "Recall matters when FN is costly: cancer detection (don't miss sick patients)",
        "High accuracy ‚â† good model! With imbalanced data, predicting majority class gives high accuracy but 0% recall",
        "‚ïê‚ïê‚ïê ROC & AUC ‚ïê‚ïê‚ïê",
        "ROC curve: plots TPR (Y-axis) vs FPR (X-axis) at different thresholds",
        "Ideal ROC: hugs TOP-LEFT corner (high TPR, low FPR)",
        "AUC=1.0 ‚Üí perfect | AUC=0.5 ‚Üí random guessing (diagonal) | AUC<0.5 ‚Üí worse than random",
        "ROC near diagonal = poor model | ROC below diagonal = worse than random",
        "‚ïê‚ïê‚ïê TRAIN/TEST & K-FOLD ‚ïê‚ïê‚ïê",
        "Split data to evaluate on UNSEEN data and detect overfitting",
        "Typical split: 80% train / 20% test | random_state for reproducibility",
        "K-Fold: divide into K parts, each used once for testing, results averaged",
        "K-Fold gives more reliable estimate but does NOT eliminate overfitting",
        "‚ïê‚ïê‚ïê OVERFITTING vs UNDERFITTING ‚ïê‚ïê‚ïê",
        "Overfitting: great on train, poor on test ‚Äî HIGH VARIANCE, too complex",
        "Underfitting: poor on BOTH ‚Äî HIGH BIAS, too simple",
        "100% training accuracy = likely overfitting (memorizing noise)",
        "Bias-Variance tradeoff: balance simplicity vs complexity",
        "‚ïê‚ïê‚ïê DATA PREPROCESSING ‚ïê‚ïê‚ïê",
        "Feature scaling: prevents large-value features from dominating",
        "Normalization: (x‚àímin)/(max‚àímin) ‚Üí scales to [0,1]",
        "Standardization: (x‚àíŒº)/œÉ ‚Üí centers around mean=0, std=1",
        "CRITICAL: Scale AFTER splitting! fit_transform on train, transform only on test",
        "Data leakage: test info contaminates training ‚Üí overly optimistic results",
        "fit_transform() = learn params + apply | transform() = apply same params",
        "Feature engineering takes 70-80% of ML project time",
        "‚ïê‚ïê‚ïê STATISTICS REVIEW ‚ïê‚ïê‚ïê",
        "Correlation: ‚àí1 to +1 | >0.5 or <‚àí0.5 = notable | 0 = no LINEAR relationship (may have non-linear!)",
        "H‚ÇÄ = null hypothesis (no effect) | Reject when p-value < 0.05",
        "T-test: variance unknown, n<30 | Z-test: variance known, n>30",
        "Chi-square: œá¬≤=Œ£(O‚àíE)¬≤/E ‚Äî tests independence and fairness",
        "‚ïê‚ïê‚ïê LINEAR REGRESSION ‚ïê‚ïê‚ïê",
        "y = w‚ÇÅx + w‚ÇÄ | OLS minimizes Œ£(y‚àí≈∑)¬≤",
        "R¬≤ = 1‚àíSSE/SST | SST = SSR+SSE | R¬≤=1 perfect, R¬≤=0 no explanation",
        "H‚ÇÄ: w‚ÇÅ=0 (no relationship) | H‚Çê: w‚ÇÅ‚â†0 (relationship exists)",
        "‚ïê‚ïê‚ïê PYTHON PIPELINE (correct order!) ‚ïê‚ïê‚ïê",
        "1. Load data ‚Üí 2. Split (train_test_split) ‚Üí 3. Scale (fit_transform train, transform test)",
        "4. Train (model.fit) ‚Üí 5. Predict (model.predict) ‚Üí 6. Evaluate (confusion_matrix, accuracy, precision, recall)",
        "ROC: use predict_proba()[:,1] NOT predict() | K-Fold: cross_val_score(model, X, y, cv=10)",
        "LogisticRegression(max_iter=10000) to avoid convergence warnings",
        "‚ïê‚ïê‚ïê TRICK QUESTION ALERTS ‚ïê‚ïê‚ïê",
        "‚ùå 'High accuracy = good model' ‚Üí FALSE (imbalanced data!)",
        "‚ùå 'K-Fold eliminates overfitting' ‚Üí FALSE (reduces eval bias only)",
        "‚ùå 'ROC below diagonal = good' ‚Üí FALSE (worse than random)",
        "‚ùå 'Scale before split' ‚Üí FALSE (causes data leakage)",
        "‚ùå '100% train accuracy = ideal' ‚Üí FALSE (likely overfitting)",
        "‚ùå 'Correlation=0 means independent' ‚Üí FALSE (no LINEAR relation, maybe non-linear)",
        "‚ùå 'Logistic Regression is for regression' ‚Üí FALSE (it's classification!)"
    ]}
};

let currentQuestions=[], currentIndex=0, userAnswers=[], quizMode='practice', timerInterval=null, timeRemaining=0;

function toggleTheme() {
    const html=document.documentElement, btn=document.querySelector('.theme-toggle');
    if(html.getAttribute('data-theme')==='dark') { html.removeAttribute('data-theme'); btn.textContent='üåô'; }
    else { html.setAttribute('data-theme','dark'); btn.textContent='‚òÄÔ∏è'; }
}

function toggleNotes() {
    const panel=document.getElementById('notesPanel'), content=document.getElementById('notesContent');
    if(panel.style.display==='block') { panel.style.display='none'; return; }
    content.innerHTML='';
    Object.values(studyNotes).forEach(ch => {
        content.innerHTML+=`<div class="notes-chapter"><h4>${ch.title}</h4><ul>${ch.notes.map(n=>`<li>${n}</li>`).join('')}</ul></div>`;
    });
    panel.style.display='block';
}

document.getElementById('searchBox').addEventListener('input', function(e) {
    const q=e.target.value.toLowerCase().trim(), res=document.getElementById('searchResults');
    if(q.length<3) { res.innerHTML=''; return; }
    const matches=questions.filter(x=>x.question.toLowerCase().includes(q)||x.options.some(o=>o.toLowerCase().includes(q))).slice(0,5);
    res.innerHTML=matches.map(x=>`<div style="padding:12px;background:var(--bg-primary);border-radius:10px;margin-bottom:10px;border-left:3px solid var(--accent);"><strong style="color:var(--accent);">Ch ${x.chapter}</strong> <span class="badge difficulty-${x.difficulty}" style="margin-left:8px;font-size:0.75rem;">${x.difficulty}</span><p style="margin-top:8px;color:var(--text-secondary);">${x.question}</p></div>`).join('')||'<p style="color:var(--text-muted);padding:10px;">No matches found</p>';
});

function shuffleArray(arr) { const a=[...arr]; for(let i=a.length-1;i>0;i--){const j=Math.floor(Math.random()*(i+1));[a[i],a[j]]=[a[j],a[i]];} return a; }
function shuffleWithAnswer(opts,idx) { const ans=opts[idx], sh=shuffleArray(opts); return {options:sh,newIndex:sh.indexOf(ans)}; }

function startQuiz() {
    const selChapters=[...document.querySelectorAll('#chapterSelect input:checked')].map(c=>parseInt(c.value));
    const selDiff=[...document.querySelectorAll('input[name="difficulty"]:checked')].map(c=>c.value);
    quizMode=document.querySelector('input[name="mode"]:checked').value;
    const timerMin=parseInt(document.querySelector('input[name="timer"]:checked').value);
    if(!selChapters.length){alert('Please select at least one chapter.');return;}
    currentQuestions=questions.filter(q=>selChapters.includes(q.chapter)&&selDiff.includes(q.difficulty));
    if(!currentQuestions.length){alert('No questions match. Adjust filters.');return;}
    currentQuestions=shuffleArray(currentQuestions).map(q=>{const sh=shuffleWithAnswer(q.options,q.answerIndex);return{...q,options:sh.options,answerIndex:sh.newIndex};});
    currentIndex=0; userAnswers=new Array(currentQuestions.length).fill(null);
    if(timerMin>0){timeRemaining=timerMin*60;startTimer();}
    document.getElementById('setupSection').style.display='none';
    document.getElementById('quizSection').style.display='block';
    document.getElementById('homeBtn').style.display='flex';
    displayQuestion();
}

function startTimer() {
    updateTimerDisplay();
    timerInterval=setInterval(()=>{timeRemaining--;updateTimerDisplay();if(timeRemaining<=0){clearInterval(timerInterval);finishQuiz();}},1000);
}
function updateTimerDisplay() {
    const m=Math.floor(timeRemaining/60), s=timeRemaining%60;
    document.getElementById('timerDisplay').textContent=`${m}:${s.toString().padStart(2,'0')}`;
}

function displayQuestion() {
    const q=currentQuestions[currentIndex], pct=((currentIndex+1)/currentQuestions.length)*100;
    document.getElementById('progressFill').style.width=`${pct}%`;
    document.getElementById('progressText').textContent=`Question ${currentIndex+1} of ${currentQuestions.length}`;
    document.getElementById('chapterBadge').textContent=`Chapter ${q.chapter}`;
    const db=document.getElementById('difficultyBadge');
    db.textContent=q.difficulty.charAt(0).toUpperCase()+q.difficulty.slice(1);
    db.className=`badge difficulty-${q.difficulty}`;
    document.getElementById('questionText').textContent=q.question;
    document.getElementById('optionsList').innerHTML=q.options.map((o,i)=>`<button class="option-btn ${userAnswers[currentIndex]===i?'selected':''}" onclick="selectOption(${i})" aria-label="Option ${String.fromCharCode(65+i)}"><span class="option-letter">${String.fromCharCode(65+i)}</span><span>${o}</span></button>`).join('');
    document.getElementById('explanationBox').classList.remove('show');
    document.getElementById('explanationBox').innerHTML='';
    document.getElementById('prevBtn').disabled=currentIndex===0;
    const isLast=currentIndex===currentQuestions.length-1;
    document.getElementById('nextBtn').style.display=isLast?'none':'inline-flex';
    document.getElementById('finishBtn').style.display=isLast?'inline-flex':'none';
    if(quizMode==='practice'&&userAnswers[currentIndex]!==null) showExplanation();
}

function selectOption(idx) {
    if(quizMode==='practice'&&userAnswers[currentIndex]!==null) return;
    userAnswers[currentIndex]=idx;
    const btns=document.querySelectorAll('.option-btn'), q=currentQuestions[currentIndex];
    btns.forEach((b,i)=>{
        b.classList.remove('selected','correct','incorrect');
        if(quizMode==='practice'){
            if(i===q.answerIndex) b.classList.add('correct');
            else if(i===idx) b.classList.add('incorrect');
        } else { if(i===idx) b.classList.add('selected'); }
    });
    if(quizMode==='practice') showExplanation();
}

function showExplanation() {
    const q=currentQuestions[currentIndex], box=document.getElementById('explanationBox');
    let wrongHtml='';
    if(q.wrongExplanations&&q.wrongExplanations.length){
        wrongHtml='<div class="explanation-detail"><strong>Why other options are incorrect:</strong><div class="wrong-options">';
        q.options.forEach((o,i)=>{if(i!==q.answerIndex&&q.wrongExplanations[i<q.answerIndex?i:i-1]){wrongHtml+=`<div class="wrong-option"><span>${o.substring(0,50)}${o.length>50?'...':''}: ${q.wrongExplanations[i<q.answerIndex?i:i-1]}</span></div>`;}});
        wrongHtml+='</div></div>';
    }
    box.innerHTML=`<div class="explanation-title">‚úì Correct Answer: ${q.options[q.answerIndex]}</div><p class="explanation-text">${q.explanation}</p>${wrongHtml}`;
    box.classList.add('show');
}

function prevQuestion() { if(currentIndex>0){currentIndex--;displayQuestion();} }
function nextQuestion() { if(currentIndex<currentQuestions.length-1){currentIndex++;displayQuestion();} }
function finishQuiz() { const un=userAnswers.filter(a=>a===null).length; if(un>0){document.getElementById('confirmModal').classList.add('show');}else{confirmFinish();} }
function closeModal() { document.getElementById('confirmModal').classList.remove('show'); }

function confirmFinish() {
    closeModal(); if(timerInterval)clearInterval(timerInterval);
    let correct=0; currentQuestions.forEach((q,i)=>{if(userAnswers[i]===q.answerIndex)correct++;});
    const pct=Math.round((correct/currentQuestions.length)*100);
    document.getElementById('scoreCircle').style.setProperty('--score-percent',pct);
    document.getElementById('scoreValue').textContent=`${pct}%`;
    document.getElementById('correctCount').textContent=correct;
    document.getElementById('incorrectCount').textContent=currentQuestions.length-correct;
    document.getElementById('totalQuestions').textContent=currentQuestions.length;
    document.getElementById('reviewList').innerHTML=currentQuestions.map((q,i)=>{
        const ok=userAnswers[i]===q.answerIndex, ua=userAnswers[i]!==null?q.options[userAnswers[i]]:'Not answered';
        return`<div class="review-item ${ok?'correct':''}"><div class="review-question"><strong>Q${i+1}</strong> (Ch ${q.chapter}, ${q.difficulty}): ${q.question}</div><div class="review-answer"><strong>Your answer:</strong> ${ua}<br><strong>Correct:</strong> ${q.options[q.answerIndex]}<br><em style="color:var(--text-muted);display:block;margin-top:8px;">${q.explanation}</em></div></div>`;
    }).join('');
    document.getElementById('quizSection').style.display='none';
    document.getElementById('resultsSection').style.display='block';
    document.getElementById('homeBtn').style.display='flex';
}

function downloadResults() {
    const results={date:new Date().toISOString(),total:currentQuestions.length,correct:currentQuestions.filter((q,i)=>userAnswers[i]===q.answerIndex).length,
        questions:currentQuestions.map((q,i)=>({chapter:q.chapter,difficulty:q.difficulty,question:q.question,yourAnswer:userAnswers[i]!==null?q.options[userAnswers[i]]:null,correctAnswer:q.options[q.answerIndex],isCorrect:userAnswers[i]===q.answerIndex,explanation:q.explanation}))};
    const blob=new Blob([JSON.stringify(results,null,2)],{type:'application/json'});
    const a=document.createElement('a');a.href=URL.createObjectURL(blob);a.download=`ai-quiz-${new Date().toISOString().split('T')[0]}.json`;a.click();
}

function restartQuiz() {
    document.getElementById('resultsSection').style.display='none';
    document.getElementById('setupSection').style.display='block';
    document.getElementById('homeBtn').style.display='none';
    currentQuestions=[];currentIndex=0;userAnswers=[];
    document.getElementById('timerDisplay').textContent='';
}

function goHome() {
    if(confirm('Return to homepage? Your progress will be lost.')){
        if(timerInterval)clearInterval(timerInterval);
        document.getElementById('quizSection').style.display='none';
        document.getElementById('resultsSection').style.display='none';
        document.getElementById('setupSection').style.display='block';
        document.getElementById('homeBtn').style.display='none';
        document.getElementById('timerDisplay').textContent='';
        currentQuestions=[];currentIndex=0;userAnswers=[];
    }
}

document.addEventListener('keydown',e=>{
    if(document.getElementById('quizSection').style.display!=='block')return;
    if(e.key>='1'&&e.key<='4')selectOption(parseInt(e.key)-1);
    else if(e.key==='ArrowRight'||e.key==='n')nextQuestion();
    else if(e.key==='ArrowLeft'||e.key==='p')prevQuestion();
    else if(e.key==='Enter'&&currentIndex===currentQuestions.length-1)finishQuiz();
});
</script>
</body>
</html>
